{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jyRIr6JFLa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c3cdb34-fe2c-4420-d21b-bbd99438311e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/axolotl\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wavedrom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.6/147.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!git clone -q https://github.com/OpenAccess-AI-Collective/axolotl\n",
        "%cd axolotl\n",
        "!pip install -qqq packaging huggingface_hub torch==2.1.1 --progress-bar off\n",
        "!pip install -qqq -e '.[flash-attn,deepspeed]' --progress-bar off\n",
        "!pip install -qqq mlflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import (\n",
        "#     AutoModelForCausalLM,\n",
        "#     AutoTokenizer,\n",
        "# )\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"yanolja/KoSOLAR-10.7B-v0.2\", eos_token='<|im_end|>', trust_remote_code=True)\n",
        "# print(tokenizer.__class__.__name__)"
      ],
      "metadata": {
        "id": "1ygeSVU6WPIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "new_model = \"yanolja-axolotl-aug-noise\"\n",
        "yaml_string = \"\"\"\n",
        "base_model: yanolja/KoSOLAR-10.7B-v0.2\n",
        "model_type: LlamaForCausalLM\n",
        "tokenizer_type: LlamaTokenizerFast\n",
        "is_llama_derived_model: true\n",
        "\n",
        "load_in_8bit: false\n",
        "load_in_4bit: true\n",
        "strict: false\n",
        "\n",
        "datasets:\n",
        "  - path: /content/drive/MyDrive/DACON/Hansol_QA/datas/aug_noise_train.jsonl\n",
        "    ds_type: json\n",
        "    type:\n",
        "        system_prompt: \"-친절하게 답변하는 도배하자 처리 전문가입니다.\n",
        "                        -설명은 이해하기 쉽고 명료하게 합니다.\n",
        "                        -건축구조, 마감재, 마감하자, 시공, 인테리어,타 마감하자,기타 측면에서 대답합니다.\n",
        "                        -그 외의 것들에 대한 질문에는 '잘 모르겠습니다'라고 답변합니다.\n",
        "                        \"\n",
        "        system_format: \"{system}\"\n",
        "        field_system: system\n",
        "        field_instruction: question\n",
        "        field_input: \"\"\n",
        "        field_output: answer\n",
        "\n",
        "    format: |-\n",
        "        User: {instruction} {question}\n",
        "        Assistant: {answer}\n",
        "    no_input_format: \"{instruction} {question} \"\n",
        "\n",
        "dataset_prepared_path:\n",
        "val_set_size: 0.05\n",
        "output_dir: /content/drive/MyDrive/DACON/Hansol_QA/models/yanolja-axolotl-aug-noise-sysprompt\n",
        "\n",
        "adapter: qlora\n",
        "\n",
        "lora_model_dir:\n",
        "\n",
        "\n",
        "\n",
        "sequence_len: 1096\n",
        "sample_packing: true\n",
        "pad_to_sequence_len: true\n",
        "\n",
        "lora_r: 32\n",
        "lora_alpha: 16\n",
        "lora_dropout: 0.05\n",
        "lora_target_modules:\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "lora_on_cpu: true\n",
        "\n",
        "special_tokens:\n",
        "  # bos_token: \"<s>\"\n",
        "  # eos_token: \"<|im-end|>\"\n",
        "  # unk_token: \"<unk>\"\n",
        "\n",
        "wandb_mode: offline\n",
        "\n",
        "gradient_accumulation_steps: 2\n",
        "micro_batch_size: 4\n",
        "num_epochs: 3\n",
        "# max_steps: 20\n",
        "optimizer: paged_adamw_32bit\n",
        "lr_scheduler: cosine\n",
        "learning_rate: 2e-4\n",
        "\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: false\n",
        "fp16: true\n",
        "tf32: false\n",
        "\n",
        "gradient_checkpointing: true\n",
        "early_stopping_patience:\n",
        "resume_from_checkpoint:\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention:\n",
        "flash_attention: false\n",
        "\n",
        "warmup_steps: 10\n",
        "evals_per_epoch:\n",
        "saves_per_epoch: 1\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.001\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "\n",
        "\"\"\"\n",
        "#/content/drive/MyDrive/DACON/Hansol_QA/model/yanolja-axolotl\n",
        "# Convert the YAML string to a Python dictionary\n",
        "yaml_dict = yaml.safe_load(yaml_string)\n",
        "\n",
        "# Specify your file path\n",
        "yaml_file = 'config.yaml'\n",
        "\n",
        "# Write the YAML file\n",
        "with open(yaml_file, 'w') as file:\n",
        "    yaml.dump(yaml_dict, file)"
      ],
      "metadata": {
        "id": "8p94FZcLFoPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch -m axolotl.cli.train config.yaml"
      ],
      "metadata": {
        "id": "RHywbnV_F9b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "497cb1f9-b7bd-4d9e-95e8-06ae8a590f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "[2024-03-07 19:08:47,405] [INFO] [datasets.<module>:58] [PID:48199] PyTorch version 2.1.1 available.\n",
            "[2024-03-07 19:08:47,406] [INFO] [datasets.<module>:95] [PID:48199] TensorFlow version 2.15.0 available.\n",
            "[2024-03-07 19:08:47,407] [INFO] [datasets.<module>:108] [PID:48199] JAX version 0.4.23 available.\n",
            "2024-03-07 19:08:48.992343: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-07 19:08:48.992391: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-07 19:08:48.993860: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-07 19:08:50.250288: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2024-03-07 19:08:51,863] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-03-07 19:08:54,556] [INFO] [axolotl.normalize_config:178] [PID:48199] [RANK:0] GPU memory usage baseline: 0.000GB (+0.255GB misc)\u001b[39m\n",
            "                                 dP            dP   dP \n",
            "                                 88            88   88 \n",
            "      .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n",
            "      88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n",
            "      88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n",
            "      `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP \n",
            "                                                       \n",
            "                                                       \n",
            "\n",
            "\u001b[33m[2024-03-07 19:08:54,560] [WARNING] [axolotl.scripts.check_user_token:449] [PID:48199] [RANK:0] Error verifying HuggingFace token. Remember to log in using `huggingface-cli login` and get your access token from https://huggingface.co/settings/tokens if you want to use gated models or datasets.\u001b[39m\n",
            "[2024-03-07 19:08:54,869] [DEBUG] [axolotl.load_tokenizer:245] [PID:48199] [RANK:0] EOS: 32000 / <|im_end|>\u001b[39m\n",
            "[2024-03-07 19:08:54,869] [DEBUG] [axolotl.load_tokenizer:246] [PID:48199] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2024-03-07 19:08:54,869] [DEBUG] [axolotl.load_tokenizer:247] [PID:48199] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
            "[2024-03-07 19:08:54,869] [DEBUG] [axolotl.load_tokenizer:248] [PID:48199] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2024-03-07 19:08:54,869] [INFO] [axolotl.load_tokenizer:259] [PID:48199] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2024-03-07 19:08:54,870] [INFO] [axolotl.load_tokenized_prepared_datasets:191] [PID:48199] [RANK:0] Unable to find prepared dataset in last_run_prepared/695002a3685f823afa4e56a676a773ed\u001b[39m\n",
            "[2024-03-07 19:08:54,870] [INFO] [axolotl.load_tokenized_prepared_datasets:192] [PID:48199] [RANK:0] Loading raw datasets...\u001b[39m\n",
            "\u001b[33m[2024-03-07 19:08:54,870] [WARNING] [axolotl.load_tokenized_prepared_datasets:194] [PID:48199] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n",
            "[2024-03-07 19:08:54,870] [INFO] [axolotl.load_tokenized_prepared_datasets:201] [PID:48199] [RANK:0] No seed provided, using default seed of 42\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2): 100% 6836/6836 [00:04<00:00, 1520.13 examples/s]\n",
            "[2024-03-07 19:08:59,895] [INFO] [axolotl.load_tokenized_prepared_datasets:414] [PID:48199] [RANK:0] merging datasets\u001b[39m\n",
            "Dropping Long Sequences (num_proc=2): 100% 6836/6836 [00:01<00:00, 3777.75 examples/s]\n",
            "Add position_id column (Sample Packing) (num_proc=2): 100% 6836/6836 [00:02<00:00, 3172.79 examples/s]\n",
            "[2024-03-07 19:09:04,042] [INFO] [axolotl.load_tokenized_prepared_datasets:424] [PID:48199] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/695002a3685f823afa4e56a676a773ed\u001b[39m\n",
            "Saving the dataset (1/1 shards): 100% 6836/6836 [00:00<00:00, 101535.71 examples/s]\n",
            "[2024-03-07 19:09:04,126] [DEBUG] [axolotl.log:61] [PID:48199] [RANK:0] total_num_tokens: 96733\u001b[39m\n",
            "[2024-03-07 19:09:04,133] [DEBUG] [axolotl.log:61] [PID:48199] [RANK:0] `total_supervised_tokens: 63097`\u001b[39m\n",
            "[2024-03-07 19:09:10,595] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 96733\u001b[39m\n",
            "[2024-03-07 19:09:10,595] [DEBUG] [axolotl.log:61] [PID:48199] [RANK:0] data_loader_len: 2\u001b[39m\n",
            "[2024-03-07 19:09:10,595] [INFO] [axolotl.log:61] [PID:48199] [RANK:0] sample_packing_eff_est across ranks: [0.8652944754544153]\u001b[39m\n",
            "[2024-03-07 19:09:10,595] [DEBUG] [axolotl.log:61] [PID:48199] [RANK:0] sample_packing_eff_est: None\u001b[39m\n",
            "[2024-03-07 19:09:10,595] [DEBUG] [axolotl.log:61] [PID:48199] [RANK:0] total_num_steps: 6\u001b[39m\n",
            "[2024-03-07 19:09:10,604] [DEBUG] [axolotl.log:61] [PID:48199] [RANK:0] total_num_tokens: 1249980\u001b[39m\n",
            "[2024-03-07 19:09:10,662] [DEBUG] [axolotl.log:61] [PID:48199] [RANK:0] `total_supervised_tokens: 682152`\u001b[39m\n",
            "[2024-03-07 19:09:10,671] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 1249980\u001b[39m\n",
            "[2024-03-07 19:09:10,671] [DEBUG] [axolotl.log:61] [PID:48199] [RANK:0] data_loader_len: 35\u001b[39m\n",
            "[2024-03-07 19:09:10,672] [INFO] [axolotl.log:61] [PID:48199] [RANK:0] sample_packing_eff_est across ranks: [0.9030029301107894]\u001b[39m\n",
            "[2024-03-07 19:09:10,672] [DEBUG] [axolotl.log:61] [PID:48199] [RANK:0] sample_packing_eff_est: 0.91\u001b[39m\n",
            "[2024-03-07 19:09:10,672] [DEBUG] [axolotl.log:61] [PID:48199] [RANK:0] total_num_steps: 105\u001b[39m\n",
            "[2024-03-07 19:09:10,672] [DEBUG] [axolotl.train.log:61] [PID:48199] [RANK:0] loading tokenizer... yanolja/KoSOLAR-10.7B-v0.2\u001b[39m\n",
            "[2024-03-07 19:09:10,977] [DEBUG] [axolotl.load_tokenizer:245] [PID:48199] [RANK:0] EOS: 32000 / <|im_end|>\u001b[39m\n",
            "[2024-03-07 19:09:10,977] [DEBUG] [axolotl.load_tokenizer:246] [PID:48199] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2024-03-07 19:09:10,977] [DEBUG] [axolotl.load_tokenizer:247] [PID:48199] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
            "[2024-03-07 19:09:10,978] [DEBUG] [axolotl.load_tokenizer:248] [PID:48199] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2024-03-07 19:09:10,978] [INFO] [axolotl.load_tokenizer:259] [PID:48199] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2024-03-07 19:09:10,978] [DEBUG] [axolotl.train.log:61] [PID:48199] [RANK:0] loading model and peft_config...\u001b[39m\n",
            "[2024-03-07 19:09:11,085] [INFO] [axolotl.load_model:348] [PID:48199] [RANK:0] patching llama _prepare_4d_causal_attention_mask*\u001b[39m\n",
            "[2024-03-07 19:09:11,093] [INFO] [axolotl.load_model:371] [PID:48199] [RANK:0] patching _expand_mask\u001b[39m\n",
            "Loading checkpoint shards: 100% 5/5 [01:48<00:00, 21.78s/it]\n",
            "[2024-03-07 19:11:01,439] [INFO] [axolotl.load_model:660] [PID:48199] [RANK:0] GPU memory usage after model load: 5.766GB (+0.023GB cache, +0.368GB misc)\u001b[39m\n",
            "[2024-03-07 19:11:01,467] [INFO] [axolotl.load_model:701] [PID:48199] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n",
            "[2024-03-07 19:11:01,473] [INFO] [axolotl.load_model:710] [PID:48199] [RANK:0] converting modules to torch.float16 for flash attention\u001b[39m\n",
            "[2024-03-07 19:11:01,478] [INFO] [axolotl.load_lora:825] [PID:48199] [RANK:0] found linear modules: ['o_proj', 'up_proj', 'down_proj', 'gate_proj', 'q_proj', 'k_proj', 'v_proj']\u001b[39m\n",
            "trainable params: 125,829,120 || all params: 10,930,753,536 || trainable%: 1.1511477190075399\n",
            "[2024-03-07 19:11:03,123] [INFO] [axolotl.load_model:750] [PID:48199] [RANK:0] GPU memory usage after adapters: 6.236GB (+1.026GB cache, +0.368GB misc)\u001b[39m\n",
            "[2024-03-07 19:11:03,147] [INFO] [axolotl.train.log:61] [PID:48199] [RANK:0] Pre-saving adapter config to /content/drive/MyDrive/DACON/Hansol_QA/models/yanolja-axolotl-aug-noise-sysprompt\u001b[39m\n",
            "[2024-03-07 19:11:03,226] [INFO] [axolotl.train.log:61] [PID:48199] [RANK:0] Starting trainer...\u001b[39m\n",
            "[2024-03-07 19:11:03,659] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 1249980\u001b[39m\n",
            "[2024-03-07 19:11:03,666] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 1249980\u001b[39m\n",
            "  0% 0/462 [00:00<?, ?it/s][2024-03-07 19:11:04,189] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 1249980\u001b[39m\n",
            "{'loss': 3.5529, 'grad_norm': 2.38434100151062, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
            "  0% 1/462 [00:40<5:13:29, 40.80s/it][2024-03-07 19:12:27,126] [INFO] [axolotl.callbacks.on_step_end:123] [PID:48199] [RANK:0] GPU memory usage while training: 6.275GB (+6.553GB cache, +1.388GB misc)\u001b[39m\n",
            "{'loss': 3.6523, 'grad_norm': 2.3163557052612305, 'learning_rate': 4e-05, 'epoch': 0.01}\n",
            "{'loss': 3.5262, 'grad_norm': 2.890540599822998, 'learning_rate': 6e-05, 'epoch': 0.02}\n",
            "{'loss': 3.3712, 'grad_norm': 2.710336923599243, 'learning_rate': 8e-05, 'epoch': 0.03}\n",
            "{'loss': 3.3412, 'grad_norm': 2.3541464805603027, 'learning_rate': 0.0001, 'epoch': 0.03}\n",
            "{'loss': 2.9399, 'grad_norm': 2.3649580478668213, 'learning_rate': 0.00012, 'epoch': 0.04}\n",
            "{'loss': 2.8108, 'grad_norm': 3.840221643447876, 'learning_rate': 0.00014, 'epoch': 0.05}\n",
            "{'loss': 2.7675, 'grad_norm': 2.01668381690979, 'learning_rate': 0.00016, 'epoch': 0.05}\n",
            "{'loss': 2.6843, 'grad_norm': 2.287607192993164, 'learning_rate': 0.00018, 'epoch': 0.06}\n",
            "{'loss': 2.4757, 'grad_norm': 1.7315024137496948, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
            "{'loss': 2.253, 'grad_norm': 1.3387274742126465, 'learning_rate': 0.00019999758458848847, 'epoch': 0.07}\n",
            "{'loss': 2.3345, 'grad_norm': 1.1398574113845825, 'learning_rate': 0.00019999033847063811, 'epoch': 0.08}\n",
            "{'loss': 2.1919, 'grad_norm': 1.219656229019165, 'learning_rate': 0.00019997826199649605, 'epoch': 0.08}\n",
            "{'loss': 2.0973, 'grad_norm': 1.294191598892212, 'learning_rate': 0.00019996135574945544, 'epoch': 0.09}\n",
            "{'loss': 2.1606, 'grad_norm': 1.4889154434204102, 'learning_rate': 0.00019993962054622703, 'epoch': 0.1}\n",
            "{'loss': 2.1165, 'grad_norm': 1.6797398328781128, 'learning_rate': 0.00019991305743680013, 'epoch': 0.1}\n",
            "{'loss': 1.9468, 'grad_norm': 1.5924674272537231, 'learning_rate': 0.00019988166770439154, 'epoch': 0.11}\n",
            "{'loss': 1.8787, 'grad_norm': 1.4336662292480469, 'learning_rate': 0.0001998454528653836, 'epoch': 0.12}\n",
            "{'loss': 1.9354, 'grad_norm': 1.9726920127868652, 'learning_rate': 0.00019980441466925118, 'epoch': 0.12}\n",
            "{'loss': 1.944, 'grad_norm': 1.5870640277862549, 'learning_rate': 0.00019975855509847686, 'epoch': 0.13}\n",
            "{'loss': 2.045, 'grad_norm': 1.2412813901901245, 'learning_rate': 0.00019970787636845535, 'epoch': 0.14}\n",
            "{'loss': 1.9507, 'grad_norm': 0.902342677116394, 'learning_rate': 0.00019965238092738643, 'epoch': 0.14}\n",
            "{'loss': 1.8988, 'grad_norm': 0.8546000123023987, 'learning_rate': 0.00019959207145615665, 'epoch': 0.15}\n",
            "{'loss': 1.9328, 'grad_norm': 0.9697080850601196, 'learning_rate': 0.00019952695086820975, 'epoch': 0.16}\n",
            "{'loss': 1.8664, 'grad_norm': 0.8079473972320557, 'learning_rate': 0.00019945702230940614, 'epoch': 0.16}\n",
            "{'loss': 1.8793, 'grad_norm': 0.9859282970428467, 'learning_rate': 0.0001993822891578708, 'epoch': 0.17}\n",
            "{'loss': 1.9879, 'grad_norm': 0.9892823696136475, 'learning_rate': 0.0001993027550238299, 'epoch': 0.17}\n",
            "{'loss': 1.7977, 'grad_norm': 0.7854679822921753, 'learning_rate': 0.0001992184237494368, 'epoch': 0.18}\n",
            "{'loss': 1.7755, 'grad_norm': 0.7333126664161682, 'learning_rate': 0.00019912929940858607, 'epoch': 0.19}\n",
            "{'loss': 1.7721, 'grad_norm': 0.9684453010559082, 'learning_rate': 0.0001990353863067169, 'epoch': 0.19}\n",
            "{'loss': 1.8467, 'grad_norm': 0.7627235054969788, 'learning_rate': 0.00019893668898060502, 'epoch': 0.2}\n",
            "{'loss': 1.7315, 'grad_norm': 0.771747350692749, 'learning_rate': 0.0001988332121981436, 'epoch': 0.21}\n",
            "{'loss': 1.7515, 'grad_norm': 1.1475181579589844, 'learning_rate': 0.00019872496095811286, 'epoch': 0.21}\n",
            "{'loss': 1.6515, 'grad_norm': 0.7216413021087646, 'learning_rate': 0.00019861194048993863, 'epoch': 0.22}\n",
            "{'loss': 1.7094, 'grad_norm': 0.729178786277771, 'learning_rate': 0.0001984941562534397, 'epoch': 0.23}\n",
            "{'loss': 1.7904, 'grad_norm': 0.7055296301841736, 'learning_rate': 0.0001983716139385641, 'epoch': 0.23}\n",
            "{'loss': 1.6706, 'grad_norm': 1.2702982425689697, 'learning_rate': 0.0001982443194651142, 'epoch': 0.24}\n",
            "{'loss': 1.634, 'grad_norm': 0.9470176100730896, 'learning_rate': 0.0001981122789824607, 'epoch': 0.25}\n",
            "{'loss': 1.5976, 'grad_norm': 0.6601965427398682, 'learning_rate': 0.00019797549886924566, 'epoch': 0.25}\n",
            "{'loss': 1.7116, 'grad_norm': 0.676037609577179, 'learning_rate': 0.00019783398573307428, 'epoch': 0.26}\n",
            "{'loss': 1.6545, 'grad_norm': 0.6391857862472534, 'learning_rate': 0.0001976877464101957, 'epoch': 0.27}\n",
            "{'loss': 1.7314, 'grad_norm': 0.7915656566619873, 'learning_rate': 0.00019753678796517282, 'epoch': 0.27}\n",
            "{'loss': 1.7035, 'grad_norm': 0.7677041292190552, 'learning_rate': 0.00019738111769054093, 'epoch': 0.28}\n",
            "{'loss': 1.7224, 'grad_norm': 0.6811192035675049, 'learning_rate': 0.00019722074310645553, 'epoch': 0.28}\n",
            "{'loss': 1.7343, 'grad_norm': 0.7327229976654053, 'learning_rate': 0.00019705567196032892, 'epoch': 0.29}\n",
            "{'loss': 1.6366, 'grad_norm': 0.6583616733551025, 'learning_rate': 0.00019688591222645607, 'epoch': 0.3}\n",
            "{'loss': 1.5991, 'grad_norm': 0.7236470580101013, 'learning_rate': 0.00019671147210562927, 'epoch': 0.3}\n",
            "{'loss': 1.5721, 'grad_norm': 0.7297188639640808, 'learning_rate': 0.000196532360024742, 'epoch': 0.31}\n",
            "{'loss': 1.6688, 'grad_norm': 0.7269312739372253, 'learning_rate': 0.000196348584636382, 'epoch': 0.32}\n",
            "{'loss': 1.4998, 'grad_norm': 0.7000634074211121, 'learning_rate': 0.0001961601548184129, 'epoch': 0.32}\n",
            "{'loss': 1.7461, 'grad_norm': 0.7606585621833801, 'learning_rate': 0.00019596707967354585, 'epoch': 0.33}\n",
            "{'loss': 1.5886, 'grad_norm': 0.70643150806427, 'learning_rate': 0.00019576936852889936, 'epoch': 0.34}\n",
            "{'loss': 1.6133, 'grad_norm': 0.7295480966567993, 'learning_rate': 0.0001955670309355489, 'epoch': 0.34}\n",
            "{'loss': 1.527, 'grad_norm': 0.6899920701980591, 'learning_rate': 0.00019536007666806556, 'epoch': 0.35}\n",
            "{'loss': 1.4296, 'grad_norm': 0.9864940047264099, 'learning_rate': 0.00019514851572404368, 'epoch': 0.36}\n",
            "{'loss': 1.5497, 'grad_norm': 0.8610536456108093, 'learning_rate': 0.0001949323583236181, 'epoch': 0.36}\n",
            "{'loss': 1.5272, 'grad_norm': 0.6585819721221924, 'learning_rate': 0.00019471161490897029, 'epoch': 0.37}\n",
            "{'loss': 1.5067, 'grad_norm': 0.6669905781745911, 'learning_rate': 0.0001944862961438239, 'epoch': 0.38}\n",
            "{'loss': 1.6516, 'grad_norm': 0.8224841356277466, 'learning_rate': 0.00019425641291292978, 'epoch': 0.38}\n",
            "{'loss': 1.5507, 'grad_norm': 0.7209178805351257, 'learning_rate': 0.00019402197632153992, 'epoch': 0.39}\n",
            "{'loss': 1.5748, 'grad_norm': 0.7399146556854248, 'learning_rate': 0.00019378299769487117, 'epoch': 0.39}\n",
            "{'loss': 1.5766, 'grad_norm': 0.7071189284324646, 'learning_rate': 0.00019353948857755803, 'epoch': 0.4}\n",
            "{'loss': 1.6016, 'grad_norm': 0.715503990650177, 'learning_rate': 0.00019329146073309504, 'epoch': 0.41}\n",
            "{'loss': 1.4533, 'grad_norm': 0.7209118008613586, 'learning_rate': 0.00019303892614326836, 'epoch': 0.41}\n",
            "{'loss': 1.6167, 'grad_norm': 0.7074165940284729, 'learning_rate': 0.00019278189700757715, 'epoch': 0.42}\n",
            "{'loss': 1.3961, 'grad_norm': 0.7131977677345276, 'learning_rate': 0.00019252038574264405, 'epoch': 0.43}\n",
            "{'loss': 1.5424, 'grad_norm': 0.7831164598464966, 'learning_rate': 0.00019225440498161546, 'epoch': 0.43}\n",
            "{'loss': 1.4735, 'grad_norm': 0.8366077542304993, 'learning_rate': 0.00019198396757355118, 'epoch': 0.44}\n",
            "{'loss': 1.5186, 'grad_norm': 0.7501664161682129, 'learning_rate': 0.00019170908658280386, 'epoch': 0.45}\n",
            "{'loss': 1.4377, 'grad_norm': 0.7280973792076111, 'learning_rate': 0.00019142977528838762, 'epoch': 0.45}\n",
            "{'loss': 1.4809, 'grad_norm': 0.7320964336395264, 'learning_rate': 0.0001911460471833368, 'epoch': 0.46}\n",
            "{'loss': 1.4815, 'grad_norm': 0.7213183641433716, 'learning_rate': 0.00019085791597405404, 'epoch': 0.47}\n",
            "{'loss': 1.4337, 'grad_norm': 0.677836537361145, 'learning_rate': 0.00019056539557964813, 'epoch': 0.47}\n",
            "{'loss': 1.4695, 'grad_norm': 0.7358472943305969, 'learning_rate': 0.00019026850013126157, 'epoch': 0.48}\n",
            "{'loss': 1.3792, 'grad_norm': 0.7168346047401428, 'learning_rate': 0.00018996724397138813, 'epoch': 0.49}\n",
            "{'loss': 1.4387, 'grad_norm': 0.7004320025444031, 'learning_rate': 0.00018966164165317966, 'epoch': 0.49}\n",
            "{'loss': 1.3824, 'grad_norm': 0.702434778213501, 'learning_rate': 0.00018935170793974335, 'epoch': 0.5}\n",
            "{'loss': 1.4379, 'grad_norm': 0.7395328879356384, 'learning_rate': 0.00018903745780342839, 'epoch': 0.5}\n",
            "{'loss': 1.4485, 'grad_norm': 0.9146981835365295, 'learning_rate': 0.0001887189064251027, 'epoch': 0.51}\n",
            "{'loss': 1.3116, 'grad_norm': 0.7730915546417236, 'learning_rate': 0.0001883960691934196, 'epoch': 0.52}\n",
            "{'loss': 1.4021, 'grad_norm': 0.7870408296585083, 'learning_rate': 0.00018806896170407437, 'epoch': 0.52}\n",
            "{'loss': 1.3391, 'grad_norm': 0.766998827457428, 'learning_rate': 0.00018773759975905098, 'epoch': 0.53}\n",
            "{'loss': 1.552, 'grad_norm': 0.9341504573822021, 'learning_rate': 0.00018740199936585853, 'epoch': 0.54}\n",
            "{'loss': 1.4143, 'grad_norm': 0.732893705368042, 'learning_rate': 0.00018706217673675811, 'epoch': 0.54}\n",
            "{'loss': 1.3852, 'grad_norm': 0.7972571849822998, 'learning_rate': 0.0001867181482879795, 'epoch': 0.55}\n",
            "{'loss': 1.4292, 'grad_norm': 0.7838420271873474, 'learning_rate': 0.0001863699306389282, 'epoch': 0.56}\n",
            "{'loss': 1.4234, 'grad_norm': 0.7831794619560242, 'learning_rate': 0.00018601754061138256, 'epoch': 0.56}\n",
            "{'loss': 1.3225, 'grad_norm': 0.713044285774231, 'learning_rate': 0.00018566099522868119, 'epoch': 0.57}\n",
            "{'loss': 1.3424, 'grad_norm': 0.7864285111427307, 'learning_rate': 0.00018530031171490053, 'epoch': 0.58}\n",
            "{'loss': 1.4825, 'grad_norm': 0.8047345280647278, 'learning_rate': 0.00018493550749402278, 'epoch': 0.58}\n",
            "{'loss': 1.3858, 'grad_norm': 0.7115502953529358, 'learning_rate': 0.00018456660018909425, 'epoch': 0.59}\n",
            "{'loss': 1.2932, 'grad_norm': 0.7089678049087524, 'learning_rate': 0.00018419360762137395, 'epoch': 0.6}\n",
            "{'loss': 1.2826, 'grad_norm': 0.7036625146865845, 'learning_rate': 0.0001838165478094727, 'epoch': 0.6}\n",
            "{'loss': 1.4261, 'grad_norm': 0.73196941614151, 'learning_rate': 0.00018343543896848273, 'epoch': 0.61}\n",
            "{'loss': 1.2957, 'grad_norm': 0.7938100099563599, 'learning_rate': 0.00018305029950909768, 'epoch': 0.61}\n",
            "{'loss': 1.3031, 'grad_norm': 0.7349918484687805, 'learning_rate': 0.00018266114803672318, 'epoch': 0.62}\n",
            "{'loss': 1.3417, 'grad_norm': 0.7171084880828857, 'learning_rate': 0.00018226800335057822, 'epoch': 0.63}\n",
            "{'loss': 1.2893, 'grad_norm': 0.7708770036697388, 'learning_rate': 0.00018187088444278674, 'epoch': 0.63}\n",
            "{'loss': 1.3557, 'grad_norm': 0.7281236052513123, 'learning_rate': 0.00018146981049746043, 'epoch': 0.64}\n",
            "{'loss': 1.2801, 'grad_norm': 0.7060905694961548, 'learning_rate': 0.00018106480088977172, 'epoch': 0.65}\n",
            "{'loss': 1.3419, 'grad_norm': 0.7644048929214478, 'learning_rate': 0.00018065587518501804, 'epoch': 0.65}\n",
            "{'loss': 1.2855, 'grad_norm': 0.7362120151519775, 'learning_rate': 0.00018024305313767646, 'epoch': 0.66}\n",
            "{'loss': 1.2035, 'grad_norm': 0.835269570350647, 'learning_rate': 0.0001798263546904495, 'epoch': 0.67}\n",
            "{'loss': 1.3286, 'grad_norm': 0.7226265072822571, 'learning_rate': 0.00017940579997330165, 'epoch': 0.67}\n",
            "{'loss': 1.3189, 'grad_norm': 0.8325413465499878, 'learning_rate': 0.00017898140930248704, 'epoch': 0.68}\n",
            "{'loss': 1.203, 'grad_norm': 0.7484042644500732, 'learning_rate': 0.00017855320317956784, 'epoch': 0.69}\n",
            "{'loss': 1.2365, 'grad_norm': 0.7938231229782104, 'learning_rate': 0.00017812120229042416, 'epoch': 0.69}\n",
            "{'loss': 1.2258, 'grad_norm': 0.7953669428825378, 'learning_rate': 0.00017768542750425426, 'epoch': 0.7}\n",
            "{'loss': 1.2849, 'grad_norm': 0.7673153281211853, 'learning_rate': 0.00017724589987256698, 'epoch': 0.71}\n",
            "{'loss': 1.1993, 'grad_norm': 0.735249936580658, 'learning_rate': 0.0001768026406281642, 'epoch': 0.71}\n",
            "{'loss': 1.337, 'grad_norm': 0.7869948744773865, 'learning_rate': 0.0001763556711841157, 'epoch': 0.72}\n",
            "{'loss': 1.2057, 'grad_norm': 0.7563179731369019, 'learning_rate': 0.00017590501313272415, 'epoch': 0.72}\n",
            "{'loss': 1.1893, 'grad_norm': 0.7778875231742859, 'learning_rate': 0.00017545068824448255, 'epoch': 0.73}\n",
            "{'loss': 1.1556, 'grad_norm': 0.74296635389328, 'learning_rate': 0.00017499271846702213, 'epoch': 0.74}\n",
            "{'loss': 1.3383, 'grad_norm': 0.7902251482009888, 'learning_rate': 0.00017453112592405242, 'epoch': 0.74}\n",
            "{'loss': 1.3302, 'grad_norm': 0.8369236588478088, 'learning_rate': 0.00017406593291429217, 'epoch': 0.75}\n",
            "{'loss': 1.2638, 'grad_norm': 0.7482987642288208, 'learning_rate': 0.00017359716191039248, 'epoch': 0.76}\n",
            "{'loss': 1.0912, 'grad_norm': 0.7460401058197021, 'learning_rate': 0.00017312483555785086, 'epoch': 0.76}\n",
            "{'loss': 1.3216, 'grad_norm': 0.8428291082382202, 'learning_rate': 0.00017264897667391754, 'epoch': 0.77}\n",
            "{'loss': 1.2692, 'grad_norm': 0.7920532822608948, 'learning_rate': 0.00017216960824649303, 'epoch': 0.78}\n",
            "{'loss': 1.1916, 'grad_norm': 0.8362180590629578, 'learning_rate': 0.00017168675343301769, 'epoch': 0.78}\n",
            "{'loss': 1.2724, 'grad_norm': 0.8155562281608582, 'learning_rate': 0.00017120043555935298, 'epoch': 0.79}\n",
            "{'loss': 1.2074, 'grad_norm': 0.7642799615859985, 'learning_rate': 0.00017071067811865476, 'epoch': 0.8}\n",
            "{'loss': 1.1623, 'grad_norm': 0.7991020083427429, 'learning_rate': 0.0001702175047702382, 'epoch': 0.8}\n",
            "{'loss': 1.0862, 'grad_norm': 0.7759974598884583, 'learning_rate': 0.000169720939338435, 'epoch': 0.81}\n",
            "{'loss': 1.1474, 'grad_norm': 0.8128775954246521, 'learning_rate': 0.00016922100581144228, 'epoch': 0.82}\n",
            "{'loss': 1.179, 'grad_norm': 0.7936181426048279, 'learning_rate': 0.00016871772834016406, 'epoch': 0.82}\n",
            "{'loss': 1.1303, 'grad_norm': 0.7430436015129089, 'learning_rate': 0.00016821113123704424, 'epoch': 0.83}\n",
            "{'loss': 1.0825, 'grad_norm': 0.7373173236846924, 'learning_rate': 0.00016770123897489228, 'epoch': 0.83}\n",
            "{'loss': 1.155, 'grad_norm': 0.7511632442474365, 'learning_rate': 0.00016718807618570106, 'epoch': 0.84}\n",
            "{'loss': 1.091, 'grad_norm': 0.7703762054443359, 'learning_rate': 0.00016667166765945668, 'epoch': 0.85}\n",
            "{'loss': 1.2271, 'grad_norm': 0.9496337175369263, 'learning_rate': 0.00016615203834294119, 'epoch': 0.85}\n",
            "{'loss': 1.0961, 'grad_norm': 0.8255730867385864, 'learning_rate': 0.00016562921333852714, 'epoch': 0.86}\n",
            "{'loss': 1.1393, 'grad_norm': 0.7770088315010071, 'learning_rate': 0.00016510321790296525, 'epoch': 0.87}\n",
            "{'loss': 1.1227, 'grad_norm': 0.7877254486083984, 'learning_rate': 0.0001645740774461642, 'epoch': 0.87}\n",
            "{'loss': 1.088, 'grad_norm': 0.7871045470237732, 'learning_rate': 0.00016404181752996289, 'epoch': 0.88}\n",
            "{'loss': 1.0723, 'grad_norm': 0.8090150356292725, 'learning_rate': 0.00016350646386689593, 'epoch': 0.89}\n",
            "{'loss': 1.0802, 'grad_norm': 0.7290539741516113, 'learning_rate': 0.00016296804231895142, 'epoch': 0.89}\n",
            "{'loss': 1.0162, 'grad_norm': 0.7687435150146484, 'learning_rate': 0.00016242657889632133, 'epoch': 0.9}\n",
            "{'loss': 1.1633, 'grad_norm': 0.8182690143585205, 'learning_rate': 0.00016188209975614542, 'epoch': 0.91}\n",
            "{'loss': 1.0353, 'grad_norm': 0.7411192059516907, 'learning_rate': 0.00016133463120124731, 'epoch': 0.91}\n",
            "{'loss': 1.1701, 'grad_norm': 0.7712575197219849, 'learning_rate': 0.00016078419967886402, 'epoch': 0.92}\n",
            "{'loss': 1.0978, 'grad_norm': 0.7494246363639832, 'learning_rate': 0.00016023083177936823, 'epoch': 0.93}\n",
            "{'loss': 1.1081, 'grad_norm': 0.8056508302688599, 'learning_rate': 0.00015967455423498387, 'epoch': 0.93}\n",
            "{'loss': 1.0409, 'grad_norm': 0.7950778603553772, 'learning_rate': 0.00015911539391849462, 'epoch': 0.94}\n",
            "{'loss': 1.0422, 'grad_norm': 0.789923369884491, 'learning_rate': 0.00015855337784194577, 'epoch': 0.94}\n",
            "{'loss': 1.0757, 'grad_norm': 0.956673800945282, 'learning_rate': 0.00015798853315533931, 'epoch': 0.95}\n",
            "{'loss': 1.0514, 'grad_norm': 0.8610673546791077, 'learning_rate': 0.00015742088714532247, 'epoch': 0.96}\n",
            "{'loss': 1.1232, 'grad_norm': 0.8218308687210083, 'learning_rate': 0.00015685046723386937, 'epoch': 0.96}\n",
            "{'loss': 1.0927, 'grad_norm': 0.823711097240448, 'learning_rate': 0.00015627730097695638, 'epoch': 0.97}\n",
            "{'loss': 1.1812, 'grad_norm': 0.783659815788269, 'learning_rate': 0.00015570141606323105, 'epoch': 0.98}\n",
            "{'loss': 1.1112, 'grad_norm': 0.8338517546653748, 'learning_rate': 0.00015512284031267437, 'epoch': 0.98}\n",
            "{'loss': 1.1131, 'grad_norm': 0.7768589854240417, 'learning_rate': 0.00015454160167525685, 'epoch': 0.99}\n",
            "{'loss': 1.0531, 'grad_norm': 0.7569327354431152, 'learning_rate': 0.00015395772822958845, 'epoch': 1.0}\n",
            "{'loss': 0.9941, 'grad_norm': 0.8386690020561218, 'learning_rate': 0.00015337124818156205, 'epoch': 1.0}\n",
            "{'loss': 1.0897, 'grad_norm': 0.8047767281532288, 'learning_rate': 0.00015278218986299074, 'epoch': 1.01}\n",
            "{'loss': 1.0684, 'grad_norm': 0.8147164583206177, 'learning_rate': 0.0001521905817302395, 'epoch': 1.02}\n",
            "{'loss': 0.996, 'grad_norm': 0.9777876734733582, 'learning_rate': 0.0001515964523628501, 'epoch': 1.02}\n",
            " 34% 158/462 [1:57:37<3:35:04, 42.45s/it][2024-03-07 21:08:42,026] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "[2024-03-07 21:08:47,321] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "[2024-03-07 21:08:47,322] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            "  0% 0/23 [00:00<?, ?it/s]\u001b[A[2024-03-07 21:08:53,920] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            "  9% 2/23 [00:06<01:09,  3.30s/it]\u001b[A[2024-03-07 21:09:00,554] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 13% 3/23 [00:13<01:33,  4.69s/it]\u001b[A[2024-03-07 21:09:07,087] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 17% 4/23 [00:19<01:42,  5.38s/it]\u001b[A[2024-03-07 21:09:13,679] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 22% 5/23 [00:26<01:44,  5.80s/it]\u001b[A[2024-03-07 21:09:20,309] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 26% 6/23 [00:32<01:43,  6.07s/it]\u001b[A[2024-03-07 21:09:26,836] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 30% 7/23 [00:39<01:39,  6.22s/it]\u001b[A[2024-03-07 21:09:33,459] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 35% 8/23 [00:46<01:35,  6.35s/it]\u001b[A[2024-03-07 21:09:40,025] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 39% 9/23 [00:52<01:29,  6.42s/it]\u001b[A[2024-03-07 21:09:46,547] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 43% 10/23 [00:59<01:23,  6.45s/it]\u001b[A[2024-03-07 21:09:53,042] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 48% 11/23 [01:05<01:17,  6.46s/it]\u001b[A[2024-03-07 21:09:59,562] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 52% 12/23 [01:12<01:11,  6.48s/it]\u001b[A[2024-03-07 21:10:06,187] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 57% 13/23 [01:18<01:05,  6.52s/it]\u001b[A[2024-03-07 21:10:12,829] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 61% 14/23 [01:25<00:59,  6.56s/it]\u001b[A[2024-03-07 21:10:19,508] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 65% 15/23 [01:32<00:52,  6.60s/it]\u001b[A[2024-03-07 21:10:26,210] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 70% 16/23 [01:38<00:46,  6.63s/it]\u001b[A[2024-03-07 21:10:32,891] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 74% 17/23 [01:45<00:39,  6.64s/it]\u001b[A[2024-03-07 21:10:39,513] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 78% 18/23 [01:52<00:33,  6.64s/it]\u001b[A[2024-03-07 21:10:46,074] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 83% 19/23 [01:58<00:26,  6.61s/it]\u001b[A[2024-03-07 21:10:52,692] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 87% 20/23 [02:05<00:19,  6.62s/it]\u001b[A[2024-03-07 21:10:59,290] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 91% 21/23 [02:11<00:13,  6.61s/it]\u001b[A[2024-03-07 21:11:05,908] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 96% 22/23 [02:18<00:06,  6.61s/it]\u001b[A[2024-03-07 21:11:12,454] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            "100% 23/23 [02:25<00:00,  6.59s/it]\u001b[A[2024-03-07 21:11:19,030] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            "24it [02:31,  6.59s/it]            \u001b[A[2024-03-07 21:11:25,595] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            "25it [02:38,  6.58s/it]\u001b[A[2024-03-07 21:11:29,744] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 1.0517123937606812, 'eval_runtime': 168.4018, 'eval_samples_per_second': 2.031, 'eval_steps_per_second': 0.511, 'epoch': 1.02}\n",
            " 34% 158/462 [2:00:26<3:35:04, 42.45s/it]\n",
            "26it [02:43,  5.85s/it]\u001b[A\n",
            "                       \u001b[A[2024-03-07 21:11:30,439] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 1249980\u001b[39m\n",
            "{'loss': 0.9584, 'grad_norm': 0.8059190511703491, 'learning_rate': 0.0001509998304621609, 'epoch': 1.01}\n",
            "{'loss': 0.7896, 'grad_norm': 0.805942714214325, 'learning_rate': 0.00015040074484992, 'epoch': 1.01}\n",
            "{'loss': 0.8721, 'grad_norm': 0.8893089890480042, 'learning_rate': 0.00014979922446689306, 'epoch': 1.02}\n",
            "{'loss': 0.879, 'grad_norm': 0.91402667760849, 'learning_rate': 0.00014919529837146528, 'epoch': 1.03}\n",
            "{'loss': 0.7656, 'grad_norm': 0.93336421251297, 'learning_rate': 0.00014858899573823753, 'epoch': 1.03}\n",
            "{'loss': 0.9183, 'grad_norm': 0.9207704663276672, 'learning_rate': 0.00014798034585661695, 'epoch': 1.04}\n",
            "{'loss': 0.8467, 'grad_norm': 0.8492830395698547, 'learning_rate': 0.00014736937812940217, 'epoch': 1.05}\n",
            "{'loss': 0.769, 'grad_norm': 0.791730523109436, 'learning_rate': 0.0001467561220713628, 'epoch': 1.05}\n",
            "{'loss': 0.7689, 'grad_norm': 0.7432780265808105, 'learning_rate': 0.00014614060730781377, 'epoch': 1.06}\n",
            "{'loss': 0.9242, 'grad_norm': 0.8340209722518921, 'learning_rate': 0.0001455228635731839, 'epoch': 1.06}\n",
            "{'loss': 0.823, 'grad_norm': 0.8374653458595276, 'learning_rate': 0.0001449029207095798, 'epoch': 1.07}\n",
            "{'loss': 0.8985, 'grad_norm': 0.905318558216095, 'learning_rate': 0.00014428080866534396, 'epoch': 1.08}\n",
            "{'loss': 0.8558, 'grad_norm': 0.8887437582015991, 'learning_rate': 0.00014365655749360833, 'epoch': 1.08}\n",
            "{'loss': 0.8154, 'grad_norm': 0.8086997866630554, 'learning_rate': 0.00014303019735084226, 'epoch': 1.09}\n",
            "{'loss': 0.8266, 'grad_norm': 0.875605583190918, 'learning_rate': 0.00014240175849539565, 'epoch': 1.1}\n",
            "{'loss': 0.7955, 'grad_norm': 0.9090399742126465, 'learning_rate': 0.00014177127128603745, 'epoch': 1.1}\n",
            "{'loss': 0.8048, 'grad_norm': 0.9211758971214294, 'learning_rate': 0.00014113876618048897, 'epoch': 1.11}\n",
            "{'loss': 0.879, 'grad_norm': 0.9627078771591187, 'learning_rate': 0.0001405042737339524, 'epoch': 1.12}\n",
            "{'loss': 0.8931, 'grad_norm': 0.9424527883529663, 'learning_rate': 0.000139867824597635, 'epoch': 1.12}\n",
            "{'loss': 0.77, 'grad_norm': 0.8894677758216858, 'learning_rate': 0.0001392294495172681, 'epoch': 1.13}\n",
            "{'loss': 0.8301, 'grad_norm': 0.9316359758377075, 'learning_rate': 0.0001385891793316221, 'epoch': 1.14}\n",
            "{'loss': 0.8326, 'grad_norm': 0.9038295745849609, 'learning_rate': 0.00013794704497101655, 'epoch': 1.14}\n",
            "{'loss': 0.851, 'grad_norm': 1.0152260065078735, 'learning_rate': 0.00013730307745582593, 'epoch': 1.15}\n",
            "{'loss': 0.8357, 'grad_norm': 0.8928052186965942, 'learning_rate': 0.0001366573078949813, 'epoch': 1.16}\n",
            "{'loss': 0.8639, 'grad_norm': 0.8989656567573547, 'learning_rate': 0.0001360097674844672, 'epoch': 1.16}\n",
            "{'loss': 0.7654, 'grad_norm': 0.814096987247467, 'learning_rate': 0.00013536048750581494, 'epoch': 1.17}\n",
            "{'loss': 0.7471, 'grad_norm': 0.8534835577011108, 'learning_rate': 0.00013470949932459117, 'epoch': 1.17}\n",
            "{'loss': 0.836, 'grad_norm': 0.8609703183174133, 'learning_rate': 0.00013405683438888282, 'epoch': 1.18}\n",
            "{'loss': 0.7645, 'grad_norm': 0.8538855910301208, 'learning_rate': 0.00013340252422777788, 'epoch': 1.19}\n",
            "{'loss': 0.7122, 'grad_norm': 0.8589795231819153, 'learning_rate': 0.00013274660044984224, 'epoch': 1.19}\n",
            "{'loss': 0.7802, 'grad_norm': 0.9655988216400146, 'learning_rate': 0.0001320890947415928, 'epoch': 1.2}\n",
            "{'loss': 0.7843, 'grad_norm': 0.8799877762794495, 'learning_rate': 0.00013143003886596669, 'epoch': 1.21}\n",
            "{'loss': 0.7287, 'grad_norm': 0.8275529742240906, 'learning_rate': 0.0001307694646607869, 'epoch': 1.21}\n",
            "{'loss': 0.7586, 'grad_norm': 0.906459391117096, 'learning_rate': 0.0001301074040372242, 'epoch': 1.22}\n",
            "{'loss': 0.762, 'grad_norm': 0.9151709675788879, 'learning_rate': 0.0001294438889782556, 'epoch': 1.23}\n",
            "{'loss': 0.7818, 'grad_norm': 0.8552376627922058, 'learning_rate': 0.00012877895153711935, 'epoch': 1.23}\n",
            "{'loss': 0.8959, 'grad_norm': 0.8487226963043213, 'learning_rate': 0.00012811262383576646, 'epoch': 1.24}\n",
            "{'loss': 0.7684, 'grad_norm': 0.8213558197021484, 'learning_rate': 0.0001274449380633089, 'epoch': 1.25}\n",
            "{'loss': 0.831, 'grad_norm': 0.8914306163787842, 'learning_rate': 0.00012677592647446472, 'epoch': 1.25}\n",
            "{'loss': 0.7636, 'grad_norm': 0.8455702662467957, 'learning_rate': 0.00012610562138799978, 'epoch': 1.26}\n",
            "{'loss': 0.7237, 'grad_norm': 0.803899884223938, 'learning_rate': 0.0001254340551851665, 'epoch': 1.27}\n",
            "{'loss': 0.8097, 'grad_norm': 0.9182079434394836, 'learning_rate': 0.00012476126030813963, 'epoch': 1.27}\n",
            "{'loss': 0.7936, 'grad_norm': 0.7995998859405518, 'learning_rate': 0.000124087269258449, 'epoch': 1.28}\n",
            "{'loss': 0.7979, 'grad_norm': 0.9465720653533936, 'learning_rate': 0.0001234121145954094, 'epoch': 1.28}\n",
            "{'loss': 0.7876, 'grad_norm': 0.930647611618042, 'learning_rate': 0.00012273582893454775, 'epoch': 1.29}\n",
            "{'loss': 0.8453, 'grad_norm': 0.9211527109146118, 'learning_rate': 0.0001220584449460274, 'epoch': 1.3}\n",
            "{'loss': 0.7859, 'grad_norm': 0.9236588478088379, 'learning_rate': 0.0001213799953530701, 'epoch': 1.3}\n",
            "{'loss': 0.7336, 'grad_norm': 0.822636067867279, 'learning_rate': 0.00012070051293037492, 'epoch': 1.31}\n",
            "{'loss': 0.7531, 'grad_norm': 0.829239547252655, 'learning_rate': 0.00012002003050253522, 'epoch': 1.32}\n",
            "{'loss': 0.7741, 'grad_norm': 0.8824236989021301, 'learning_rate': 0.00011933858094245281, 'epoch': 1.32}\n",
            "{'loss': 0.7533, 'grad_norm': 0.8799247145652771, 'learning_rate': 0.00011865619716974984, 'epoch': 1.33}\n",
            "{'loss': 0.7247, 'grad_norm': 0.8451051712036133, 'learning_rate': 0.00011797291214917881, 'epoch': 1.34}\n",
            "{'loss': 0.7282, 'grad_norm': 0.8970708250999451, 'learning_rate': 0.00011728875888902975, 'epoch': 1.34}\n",
            "{'loss': 0.7793, 'grad_norm': 0.8569312691688538, 'learning_rate': 0.00011660377043953588, 'epoch': 1.35}\n",
            "{'loss': 0.7307, 'grad_norm': 0.9270931482315063, 'learning_rate': 0.0001159179798912769, 'epoch': 1.36}\n",
            "{'loss': 0.6839, 'grad_norm': 0.9750560522079468, 'learning_rate': 0.0001152314203735805, 'epoch': 1.36}\n",
            "{'loss': 0.6589, 'grad_norm': 0.819709062576294, 'learning_rate': 0.000114544125052922, 'epoch': 1.37}\n",
            "{'loss': 0.7511, 'grad_norm': 0.9985460638999939, 'learning_rate': 0.0001138561271313219, 'epoch': 1.38}\n",
            "{'loss': 0.7361, 'grad_norm': 0.9021238684654236, 'learning_rate': 0.00011316745984474226, 'epoch': 1.38}\n",
            "{'loss': 0.7805, 'grad_norm': 0.9064633250236511, 'learning_rate': 0.00011247815646148087, 'epoch': 1.39}\n",
            "{'loss': 0.6466, 'grad_norm': 0.7919216752052307, 'learning_rate': 0.0001117882502805643, 'epoch': 1.39}\n",
            "{'loss': 0.7301, 'grad_norm': 0.8960740566253662, 'learning_rate': 0.00011109777463013915, 'epoch': 1.4}\n",
            "{'loss': 0.6443, 'grad_norm': 0.8417792320251465, 'learning_rate': 0.00011040676286586211, 'epoch': 1.41}\n",
            "{'loss': 0.696, 'grad_norm': 0.9371309876441956, 'learning_rate': 0.0001097152483692886, 'epoch': 1.41}\n",
            "{'loss': 0.7193, 'grad_norm': 1.0006157159805298, 'learning_rate': 0.0001090232645462601, 'epoch': 1.42}\n",
            "{'loss': 0.6519, 'grad_norm': 0.9227544069290161, 'learning_rate': 0.00010833084482529048, 'epoch': 1.43}\n",
            "{'loss': 0.7871, 'grad_norm': 0.9847474098205566, 'learning_rate': 0.00010763802265595102, 'epoch': 1.43}\n",
            "{'loss': 0.7915, 'grad_norm': 0.9319601655006409, 'learning_rate': 0.00010694483150725458, 'epoch': 1.44}\n",
            "{'loss': 0.7733, 'grad_norm': 0.91193026304245, 'learning_rate': 0.00010625130486603878, 'epoch': 1.45}\n",
            "{'loss': 0.753, 'grad_norm': 0.8485025763511658, 'learning_rate': 0.00010555747623534831, 'epoch': 1.45}\n",
            "{'loss': 0.7457, 'grad_norm': 0.8378700613975525, 'learning_rate': 0.00010486337913281632, 'epoch': 1.46}\n",
            "{'loss': 0.7177, 'grad_norm': 0.8743587136268616, 'learning_rate': 0.00010416904708904548, 'epoch': 1.47}\n",
            "{'loss': 0.7757, 'grad_norm': 0.9335366487503052, 'learning_rate': 0.00010347451364598804, 'epoch': 1.47}\n",
            "{'loss': 0.7155, 'grad_norm': 0.8673663139343262, 'learning_rate': 0.00010277981235532541, 'epoch': 1.48}\n",
            "{'loss': 0.7705, 'grad_norm': 0.8895627856254578, 'learning_rate': 0.00010208497677684754, 'epoch': 1.49}\n",
            "{'loss': 0.757, 'grad_norm': 0.9658058881759644, 'learning_rate': 0.00010139004047683151, 'epoch': 1.49}\n",
            "{'loss': 0.7422, 'grad_norm': 0.8970728516578674, 'learning_rate': 0.00010069503702642011, 'epoch': 1.5}\n",
            "{'loss': 0.7083, 'grad_norm': 0.9391387701034546, 'learning_rate': 0.0001, 'epoch': 1.5}\n",
            "{'loss': 0.5672, 'grad_norm': 0.7975174784660339, 'learning_rate': 9.930496297357993e-05, 'epoch': 1.51}\n",
            "{'loss': 0.8257, 'grad_norm': 1.004429578781128, 'learning_rate': 9.860995952316851e-05, 'epoch': 1.52}\n",
            "{'loss': 0.6881, 'grad_norm': 0.945957601070404, 'learning_rate': 9.791502322315249e-05, 'epoch': 1.52}\n",
            "{'loss': 0.764, 'grad_norm': 0.9531763792037964, 'learning_rate': 9.722018764467461e-05, 'epoch': 1.53}\n",
            "{'loss': 0.715, 'grad_norm': 0.917674720287323, 'learning_rate': 9.652548635401201e-05, 'epoch': 1.54}\n",
            "{'loss': 0.7851, 'grad_norm': 0.8831698894500732, 'learning_rate': 9.583095291095453e-05, 'epoch': 1.54}\n",
            "{'loss': 0.7552, 'grad_norm': 0.892392635345459, 'learning_rate': 9.513662086718372e-05, 'epoch': 1.55}\n",
            "{'loss': 0.631, 'grad_norm': 0.8495733141899109, 'learning_rate': 9.444252376465171e-05, 'epoch': 1.56}\n",
            "{'loss': 0.6671, 'grad_norm': 0.8334491848945618, 'learning_rate': 9.374869513396123e-05, 'epoch': 1.56}\n",
            "{'loss': 0.6694, 'grad_norm': 0.9030101299285889, 'learning_rate': 9.305516849274541e-05, 'epoch': 1.57}\n",
            "{'loss': 0.6799, 'grad_norm': 0.9220359325408936, 'learning_rate': 9.236197734404901e-05, 'epoch': 1.58}\n",
            "{'loss': 0.603, 'grad_norm': 0.842442512512207, 'learning_rate': 9.166915517470953e-05, 'epoch': 1.58}\n",
            "{'loss': 0.666, 'grad_norm': 0.9522773623466492, 'learning_rate': 9.09767354537399e-05, 'epoch': 1.59}\n",
            "{'loss': 0.7227, 'grad_norm': 1.0239084959030151, 'learning_rate': 9.028475163071141e-05, 'epoch': 1.6}\n",
            "{'loss': 0.701, 'grad_norm': 0.9549550414085388, 'learning_rate': 8.959323713413791e-05, 'epoch': 1.6}\n",
            "{'loss': 0.6259, 'grad_norm': 0.887194812297821, 'learning_rate': 8.890222536986085e-05, 'epoch': 1.61}\n",
            "{'loss': 0.6335, 'grad_norm': 0.9688299894332886, 'learning_rate': 8.821174971943572e-05, 'epoch': 1.61}\n",
            "{'loss': 0.708, 'grad_norm': 0.9550149440765381, 'learning_rate': 8.752184353851916e-05, 'epoch': 1.62}\n",
            "{'loss': 0.6472, 'grad_norm': 0.878365695476532, 'learning_rate': 8.683254015525776e-05, 'epoch': 1.63}\n",
            "{'loss': 0.5634, 'grad_norm': 0.8644038438796997, 'learning_rate': 8.614387286867814e-05, 'epoch': 1.63}\n",
            "{'loss': 0.7892, 'grad_norm': 0.9603064656257629, 'learning_rate': 8.545587494707803e-05, 'epoch': 1.64}\n",
            "{'loss': 0.726, 'grad_norm': 0.8786483407020569, 'learning_rate': 8.47685796264195e-05, 'epoch': 1.65}\n",
            "{'loss': 0.6827, 'grad_norm': 0.9278368353843689, 'learning_rate': 8.408202010872312e-05, 'epoch': 1.65}\n",
            "{'loss': 0.6613, 'grad_norm': 0.8786017298698425, 'learning_rate': 8.339622956046417e-05, 'epoch': 1.66}\n",
            "{'loss': 0.6014, 'grad_norm': 0.9192908406257629, 'learning_rate': 8.271124111097026e-05, 'epoch': 1.67}\n",
            "{'loss': 0.6548, 'grad_norm': 0.9489969611167908, 'learning_rate': 8.202708785082121e-05, 'epoch': 1.67}\n",
            "{'loss': 0.6131, 'grad_norm': 0.9589847326278687, 'learning_rate': 8.134380283025014e-05, 'epoch': 1.68}\n",
            "{'loss': 0.6382, 'grad_norm': 0.943722665309906, 'learning_rate': 8.066141905754723e-05, 'epoch': 1.69}\n",
            "{'loss': 0.6179, 'grad_norm': 0.933774471282959, 'learning_rate': 7.997996949746477e-05, 'epoch': 1.69}\n",
            "{'loss': 0.6742, 'grad_norm': 0.9430416822433472, 'learning_rate': 7.929948706962508e-05, 'epoch': 1.7}\n",
            "{'loss': 0.5855, 'grad_norm': 0.8935884237289429, 'learning_rate': 7.862000464692991e-05, 'epoch': 1.71}\n",
            "{'loss': 0.6274, 'grad_norm': 0.9096997380256653, 'learning_rate': 7.794155505397261e-05, 'epoch': 1.71}\n",
            "{'loss': 0.6256, 'grad_norm': 0.9256967902183533, 'learning_rate': 7.72641710654523e-05, 'epoch': 1.72}\n",
            "{'loss': 0.5646, 'grad_norm': 0.9228050112724304, 'learning_rate': 7.658788540459062e-05, 'epoch': 1.72}\n",
            "{'loss': 0.6279, 'grad_norm': 0.936499297618866, 'learning_rate': 7.591273074155104e-05, 'epoch': 1.73}\n",
            "{'loss': 0.5842, 'grad_norm': 0.9102290272712708, 'learning_rate': 7.523873969186039e-05, 'epoch': 1.74}\n",
            "{'loss': 0.5549, 'grad_norm': 0.8491805195808411, 'learning_rate': 7.456594481483355e-05, 'epoch': 1.74}\n",
            "{'loss': 0.6373, 'grad_norm': 0.9357823133468628, 'learning_rate': 7.389437861200024e-05, 'epoch': 1.75}\n",
            "{'loss': 0.6283, 'grad_norm': 0.9341374635696411, 'learning_rate': 7.322407352553529e-05, 'epoch': 1.76}\n",
            "{'loss': 0.6163, 'grad_norm': 1.0099265575408936, 'learning_rate': 7.25550619366911e-05, 'epoch': 1.76}\n",
            "{'loss': 0.5911, 'grad_norm': 0.9400248527526855, 'learning_rate': 7.188737616423356e-05, 'epoch': 1.77}\n",
            "{'loss': 0.5459, 'grad_norm': 0.881589412689209, 'learning_rate': 7.122104846288064e-05, 'epoch': 1.78}\n",
            "{'loss': 0.603, 'grad_norm': 1.0410468578338623, 'learning_rate': 7.055611102174442e-05, 'epoch': 1.78}\n",
            "{'loss': 0.628, 'grad_norm': 0.9875423312187195, 'learning_rate': 6.989259596277582e-05, 'epoch': 1.79}\n",
            "{'loss': 0.6021, 'grad_norm': 0.9495760798454285, 'learning_rate': 6.923053533921312e-05, 'epoch': 1.8}\n",
            "{'loss': 0.5578, 'grad_norm': 0.9244229197502136, 'learning_rate': 6.85699611340333e-05, 'epoch': 1.8}\n",
            "{'loss': 0.6329, 'grad_norm': 0.9907771348953247, 'learning_rate': 6.791090525840722e-05, 'epoch': 1.81}\n",
            "{'loss': 0.5867, 'grad_norm': 0.9305540323257446, 'learning_rate': 6.725339955015777e-05, 'epoch': 1.82}\n",
            "{'loss': 0.6467, 'grad_norm': 0.9852614998817444, 'learning_rate': 6.659747577222216e-05, 'epoch': 1.82}\n",
            "{'loss': 0.5925, 'grad_norm': 0.8471209406852722, 'learning_rate': 6.594316561111724e-05, 'epoch': 1.83}\n",
            "{'loss': 0.5791, 'grad_norm': 0.9041057229042053, 'learning_rate': 6.529050067540887e-05, 'epoch': 1.83}\n",
            "{'loss': 0.6098, 'grad_norm': 0.88790363073349, 'learning_rate': 6.46395124941851e-05, 'epoch': 1.84}\n",
            "{'loss': 0.5568, 'grad_norm': 0.9008603692054749, 'learning_rate': 6.39902325155328e-05, 'epoch': 1.85}\n",
            "{'loss': 0.5425, 'grad_norm': 0.861303448677063, 'learning_rate': 6.334269210501875e-05, 'epoch': 1.85}\n",
            "{'loss': 0.5522, 'grad_norm': 0.9056730270385742, 'learning_rate': 6.269692254417408e-05, 'epoch': 1.86}\n",
            "{'loss': 0.5101, 'grad_norm': 0.8913998007774353, 'learning_rate': 6.205295502898348e-05, 'epoch': 1.87}\n",
            "{'loss': 0.6343, 'grad_norm': 1.0451537370681763, 'learning_rate': 6.141082066837791e-05, 'epoch': 1.87}\n",
            "{'loss': 0.6913, 'grad_norm': 1.13619863986969, 'learning_rate': 6.0770550482731924e-05, 'epoch': 1.88}\n",
            "{'loss': 0.5331, 'grad_norm': 0.9497880935668945, 'learning_rate': 6.013217540236502e-05, 'epoch': 1.89}\n",
            "{'loss': 0.6061, 'grad_norm': 0.9614327549934387, 'learning_rate': 5.9495726266047605e-05, 'epoch': 1.89}\n",
            "{'loss': 0.57, 'grad_norm': 0.989098310470581, 'learning_rate': 5.886123381951103e-05, 'epoch': 1.9}\n",
            "{'loss': 0.5978, 'grad_norm': 1.0028269290924072, 'learning_rate': 5.8228728713962543e-05, 'epoch': 1.91}\n",
            "{'loss': 0.4455, 'grad_norm': 0.851463794708252, 'learning_rate': 5.759824150460435e-05, 'epoch': 1.91}\n",
            "{'loss': 0.5577, 'grad_norm': 0.8916828632354736, 'learning_rate': 5.696980264915777e-05, 'epoch': 1.92}\n",
            "{'loss': 0.616, 'grad_norm': 0.9443600177764893, 'learning_rate': 5.63434425063917e-05, 'epoch': 1.93}\n",
            "{'loss': 0.5768, 'grad_norm': 0.9115630388259888, 'learning_rate': 5.571919133465605e-05, 'epoch': 1.93}\n",
            "{'loss': 0.6027, 'grad_norm': 0.9724498987197876, 'learning_rate': 5.50970792904203e-05, 'epoch': 1.94}\n",
            "{'loss': 0.551, 'grad_norm': 0.9455156922340393, 'learning_rate': 5.447713642681612e-05, 'epoch': 1.94}\n",
            "{'loss': 0.4913, 'grad_norm': 0.9424174427986145, 'learning_rate': 5.385939269218625e-05, 'epoch': 1.95}\n",
            "{'loss': 0.5968, 'grad_norm': 0.970078706741333, 'learning_rate': 5.324387792863719e-05, 'epoch': 1.96}\n",
            "{'loss': 0.6088, 'grad_norm': 1.0067315101623535, 'learning_rate': 5.263062187059785e-05, 'epoch': 1.96}\n",
            "{'loss': 0.5277, 'grad_norm': 0.9556141495704651, 'learning_rate': 5.201965414338308e-05, 'epoch': 1.97}\n",
            "{'loss': 0.6017, 'grad_norm': 1.070285439491272, 'learning_rate': 5.14110042617625e-05, 'epoch': 1.98}\n",
            "{'loss': 0.6455, 'grad_norm': 1.0832639932632446, 'learning_rate': 5.080470162853472e-05, 'epoch': 1.98}\n",
            "{'loss': 0.5721, 'grad_norm': 1.0123610496520996, 'learning_rate': 5.020077553310694e-05, 'epoch': 1.99}\n",
            "{'loss': 0.5326, 'grad_norm': 0.9086759686470032, 'learning_rate': 4.959925515008002e-05, 'epoch': 2.0}\n",
            "{'loss': 0.5853, 'grad_norm': 0.9559667110443115, 'learning_rate': 4.900016953783912e-05, 'epoch': 2.0}\n",
            "{'loss': 0.5861, 'grad_norm': 1.069700002670288, 'learning_rate': 4.840354763714991e-05, 'epoch': 2.01}\n",
            "{'loss': 0.5373, 'grad_norm': 0.9308264851570129, 'learning_rate': 4.7809418269760545e-05, 'epoch': 2.02}\n",
            "{'loss': 0.5865, 'grad_norm': 0.9951556921005249, 'learning_rate': 4.7217810137009274e-05, 'epoch': 2.02}\n",
            " 68% 316/462 [3:58:26<1:49:24, 44.96s/it][2024-03-07 23:09:52,576] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "[2024-03-07 23:09:57,910] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "[2024-03-07 23:09:57,911] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            "  0% 0/23 [00:00<?, ?it/s]\u001b[A[2024-03-07 23:10:04,507] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            "  9% 2/23 [00:06<01:09,  3.30s/it]\u001b[A[2024-03-07 23:10:11,141] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 13% 3/23 [00:13<01:33,  4.69s/it]\u001b[A[2024-03-07 23:10:17,686] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 17% 4/23 [00:19<01:42,  5.38s/it]\u001b[A[2024-03-07 23:10:24,316] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 22% 5/23 [00:26<01:44,  5.82s/it]\u001b[A[2024-03-07 23:10:30,981] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 26% 6/23 [00:33<01:43,  6.10s/it]\u001b[A[2024-03-07 23:10:37,563] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 30% 7/23 [00:39<01:40,  6.25s/it]\u001b[A[2024-03-07 23:10:44,253] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 35% 8/23 [00:46<01:35,  6.39s/it]\u001b[A[2024-03-07 23:10:50,878] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 39% 9/23 [00:52<01:30,  6.46s/it]\u001b[A[2024-03-07 23:10:57,444] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 43% 10/23 [00:59<01:24,  6.49s/it]\u001b[A[2024-03-07 23:11:03,997] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 48% 11/23 [01:06<01:18,  6.51s/it]\u001b[A[2024-03-07 23:11:10,544] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 52% 12/23 [01:12<01:11,  6.52s/it]\u001b[A[2024-03-07 23:11:17,170] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 57% 13/23 [01:19<01:05,  6.55s/it]\u001b[A[2024-03-07 23:11:23,807] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 61% 14/23 [01:25<00:59,  6.58s/it]\u001b[A[2024-03-07 23:11:30,454] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 65% 15/23 [01:32<00:52,  6.60s/it]\u001b[A[2024-03-07 23:11:37,113] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 70% 16/23 [01:39<00:46,  6.62s/it]\u001b[A[2024-03-07 23:11:43,737] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 74% 17/23 [01:45<00:39,  6.62s/it]\u001b[A[2024-03-07 23:11:50,332] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 78% 18/23 [01:52<00:33,  6.61s/it]\u001b[A[2024-03-07 23:11:56,889] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 83% 19/23 [01:58<00:26,  6.60s/it]\u001b[A[2024-03-07 23:12:03,505] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 87% 20/23 [02:05<00:19,  6.60s/it]\u001b[A[2024-03-07 23:12:10,107] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 91% 21/23 [02:12<00:13,  6.60s/it]\u001b[A[2024-03-07 23:12:16,715] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 96% 22/23 [02:18<00:06,  6.60s/it]\u001b[A[2024-03-07 23:12:23,295] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            "100% 23/23 [02:25<00:00,  6.60s/it]\u001b[A[2024-03-07 23:12:29,877] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            "24it [02:31,  6.59s/it]            \u001b[A[2024-03-07 23:12:36,455] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            "25it [02:38,  6.59s/it]\u001b[A[2024-03-07 23:12:40,622] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.7260856032371521, 'eval_runtime': 168.7328, 'eval_samples_per_second': 2.027, 'eval_steps_per_second': 0.51, 'epoch': 2.02}\n",
            " 68% 316/462 [4:01:37<1:49:24, 44.96s/it]\n",
            "26it [02:43,  5.86s/it]\u001b[A\n",
            "                       \u001b[A[2024-03-07 23:12:41,320] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 1249980\u001b[39m\n",
            "{'loss': 0.5187, 'grad_norm': 0.9782883524894714, 'learning_rate': 4.6628751818437985e-05, 'epoch': 2.0}\n",
            "{'loss': 0.3761, 'grad_norm': 0.8854681253433228, 'learning_rate': 4.604227177041156e-05, 'epoch': 2.01}\n",
            "{'loss': 0.3427, 'grad_norm': 0.8238178491592407, 'learning_rate': 4.545839832474318e-05, 'epoch': 2.02}\n",
            "{'loss': 0.4172, 'grad_norm': 0.8656560778617859, 'learning_rate': 4.487715968732568e-05, 'epoch': 2.02}\n",
            "{'loss': 0.3388, 'grad_norm': 0.8284583687782288, 'learning_rate': 4.4298583936768976e-05, 'epoch': 2.03}\n",
            "{'loss': 0.3281, 'grad_norm': 0.9035162925720215, 'learning_rate': 4.372269902304363e-05, 'epoch': 2.04}\n",
            "{'loss': 0.3225, 'grad_norm': 0.9076411724090576, 'learning_rate': 4.314953276613066e-05, 'epoch': 2.04}\n",
            "{'loss': 0.371, 'grad_norm': 0.9728831052780151, 'learning_rate': 4.257911285467754e-05, 'epoch': 2.05}\n",
            "{'loss': 0.3746, 'grad_norm': 0.9597878456115723, 'learning_rate': 4.2011466844660655e-05, 'epoch': 2.06}\n",
            "{'loss': 0.3478, 'grad_norm': 0.9682226181030273, 'learning_rate': 4.144662215805426e-05, 'epoch': 2.06}\n",
            "{'loss': 0.3372, 'grad_norm': 1.0917763710021973, 'learning_rate': 4.0884606081505374e-05, 'epoch': 2.07}\n",
            "{'loss': 0.3628, 'grad_norm': 0.9609357714653015, 'learning_rate': 4.0325445765016145e-05, 'epoch': 2.07}\n",
            "{'loss': 0.3327, 'grad_norm': 0.9412081837654114, 'learning_rate': 3.9769168220631745e-05, 'epoch': 2.08}\n",
            "{'loss': 0.3708, 'grad_norm': 0.9623420834541321, 'learning_rate': 3.921580032113602e-05, 'epoch': 2.09}\n",
            "{'loss': 0.3868, 'grad_norm': 0.9358841776847839, 'learning_rate': 3.866536879875269e-05, 'epoch': 2.09}\n",
            "{'loss': 0.311, 'grad_norm': 0.874370276927948, 'learning_rate': 3.8117900243854595e-05, 'epoch': 2.1}\n",
            "{'loss': 0.3538, 'grad_norm': 0.8523198962211609, 'learning_rate': 3.757342110367871e-05, 'epoch': 2.11}\n",
            "{'loss': 0.3035, 'grad_norm': 0.8245704174041748, 'learning_rate': 3.7031957681048604e-05, 'epoch': 2.11}\n",
            "{'loss': 0.3767, 'grad_norm': 0.8885533809661865, 'learning_rate': 3.649353613310409e-05, 'epoch': 2.12}\n",
            "{'loss': 0.373, 'grad_norm': 0.8761260509490967, 'learning_rate': 3.595818247003713e-05, 'epoch': 2.13}\n",
            "{'loss': 0.334, 'grad_norm': 0.8636618852615356, 'learning_rate': 3.542592255383586e-05, 'epoch': 2.13}\n",
            "{'loss': 0.3425, 'grad_norm': 0.8576313853263855, 'learning_rate': 3.489678209703475e-05, 'epoch': 2.14}\n",
            "{'loss': 0.3529, 'grad_norm': 0.9542242288589478, 'learning_rate': 3.437078666147292e-05, 'epoch': 2.15}\n",
            "{'loss': 0.3094, 'grad_norm': 0.8304356336593628, 'learning_rate': 3.3847961657058845e-05, 'epoch': 2.15}\n",
            "{'loss': 0.3562, 'grad_norm': 0.9044414758682251, 'learning_rate': 3.332833234054331e-05, 'epoch': 2.16}\n",
            "{'loss': 0.2961, 'grad_norm': 0.8970943093299866, 'learning_rate': 3.281192381429894e-05, 'epoch': 2.17}\n",
            "{'loss': 0.318, 'grad_norm': 0.880820095539093, 'learning_rate': 3.2298761025107706e-05, 'epoch': 2.17}\n",
            "{'loss': 0.3557, 'grad_norm': 0.9550089240074158, 'learning_rate': 3.178886876295578e-05, 'epoch': 2.18}\n",
            "{'loss': 0.3619, 'grad_norm': 1.041892170906067, 'learning_rate': 3.1282271659835946e-05, 'epoch': 2.18}\n",
            "{'loss': 0.2865, 'grad_norm': 0.917749285697937, 'learning_rate': 3.077899418855772e-05, 'epoch': 2.19}\n",
            "{'loss': 0.3632, 'grad_norm': 0.9706226587295532, 'learning_rate': 3.0279060661565028e-05, 'epoch': 2.2}\n",
            "{'loss': 0.4035, 'grad_norm': 1.0491915941238403, 'learning_rate': 2.9782495229761808e-05, 'epoch': 2.2}\n",
            "{'loss': 0.3275, 'grad_norm': 0.9947534203529358, 'learning_rate': 2.9289321881345254e-05, 'epoch': 2.21}\n",
            "{'loss': 0.372, 'grad_norm': 1.004028558731079, 'learning_rate': 2.879956444064703e-05, 'epoch': 2.22}\n",
            "{'loss': 0.2979, 'grad_norm': 0.9569818377494812, 'learning_rate': 2.8313246566982345e-05, 'epoch': 2.22}\n",
            "{'loss': 0.3614, 'grad_norm': 0.9604949355125427, 'learning_rate': 2.783039175350699e-05, 'epoch': 2.23}\n",
            "{'loss': 0.3338, 'grad_norm': 0.9467352628707886, 'learning_rate': 2.735102332608247e-05, 'epoch': 2.24}\n",
            "{'loss': 0.3527, 'grad_norm': 0.9790467619895935, 'learning_rate': 2.6875164442149147e-05, 'epoch': 2.24}\n",
            "{'loss': 0.3604, 'grad_norm': 0.964695394039154, 'learning_rate': 2.640283808960754e-05, 'epoch': 2.25}\n",
            "{'loss': 0.2693, 'grad_norm': 0.824116587638855, 'learning_rate': 2.5934067085707834e-05, 'epoch': 2.26}\n",
            "{'loss': 0.34, 'grad_norm': 1.00774085521698, 'learning_rate': 2.54688740759476e-05, 'epoch': 2.26}\n",
            "{'loss': 0.3232, 'grad_norm': 0.9149210453033447, 'learning_rate': 2.500728153297788e-05, 'epoch': 2.27}\n",
            "{'loss': 0.3154, 'grad_norm': 0.9215195178985596, 'learning_rate': 2.4549311755517457e-05, 'epoch': 2.28}\n",
            "{'loss': 0.3321, 'grad_norm': 0.9131012558937073, 'learning_rate': 2.409498686727587e-05, 'epoch': 2.28}\n",
            "{'loss': 0.3684, 'grad_norm': 0.9608210325241089, 'learning_rate': 2.364432881588431e-05, 'epoch': 2.29}\n",
            "{'loss': 0.3238, 'grad_norm': 0.89280766248703, 'learning_rate': 2.3197359371835802e-05, 'epoch': 2.29}\n",
            "{'loss': 0.3008, 'grad_norm': 0.9746578931808472, 'learning_rate': 2.275410012743303e-05, 'epoch': 2.3}\n",
            "{'loss': 0.3326, 'grad_norm': 0.9919344186782837, 'learning_rate': 2.2314572495745746e-05, 'epoch': 2.31}\n",
            "{'loss': 0.2734, 'grad_norm': 0.8645761609077454, 'learning_rate': 2.1878797709575847e-05, 'epoch': 2.31}\n",
            "{'loss': 0.306, 'grad_norm': 0.8779214024543762, 'learning_rate': 2.1446796820432167e-05, 'epoch': 2.32}\n",
            "{'loss': 0.2944, 'grad_norm': 0.8843648433685303, 'learning_rate': 2.101859069751301e-05, 'epoch': 2.33}\n",
            "{'loss': 0.3742, 'grad_norm': 0.9311555624008179, 'learning_rate': 2.0594200026698363e-05, 'epoch': 2.33}\n",
            "{'loss': 0.3164, 'grad_norm': 0.8820371627807617, 'learning_rate': 2.0173645309550548e-05, 'epoch': 2.34}\n",
            "{'loss': 0.284, 'grad_norm': 0.8483016490936279, 'learning_rate': 1.9756946862323535e-05, 'epoch': 2.35}\n",
            "{'loss': 0.3297, 'grad_norm': 0.9403050541877747, 'learning_rate': 1.934412481498198e-05, 'epoch': 2.35}\n",
            "{'loss': 0.3159, 'grad_norm': 0.9198428392410278, 'learning_rate': 1.8935199110228275e-05, 'epoch': 2.36}\n",
            "{'loss': 0.2814, 'grad_norm': 0.8499435186386108, 'learning_rate': 1.8530189502539607e-05, 'epoch': 2.37}\n",
            "{'loss': 0.3229, 'grad_norm': 0.9205660820007324, 'learning_rate': 1.8129115557213262e-05, 'epoch': 2.37}\n",
            "{'loss': 0.3585, 'grad_norm': 0.9802770018577576, 'learning_rate': 1.7731996649421802e-05, 'epoch': 2.38}\n",
            "{'loss': 0.3273, 'grad_norm': 0.8783925771713257, 'learning_rate': 1.7338851963276825e-05, 'epoch': 2.39}\n",
            "{'loss': 0.2588, 'grad_norm': 0.8535107970237732, 'learning_rate': 1.6949700490902344e-05, 'epoch': 2.39}\n",
            "{'loss': 0.3341, 'grad_norm': 0.9192700982093811, 'learning_rate': 1.656456103151728e-05, 'epoch': 2.4}\n",
            "{'loss': 0.319, 'grad_norm': 0.9417779445648193, 'learning_rate': 1.6183452190527316e-05, 'epoch': 2.4}\n",
            "{'loss': 0.3138, 'grad_norm': 0.9770245552062988, 'learning_rate': 1.580639237862608e-05, 'epoch': 2.41}\n",
            "{'loss': 0.3358, 'grad_norm': 0.9522000551223755, 'learning_rate': 1.543339981090578e-05, 'epoch': 2.42}\n",
            "{'loss': 0.3145, 'grad_norm': 0.8762807846069336, 'learning_rate': 1.5064492505977234e-05, 'epoch': 2.42}\n",
            "{'loss': 0.3229, 'grad_norm': 0.9947384595870972, 'learning_rate': 1.4699688285099489e-05, 'epoch': 2.43}\n",
            "{'loss': 0.3269, 'grad_norm': 0.8954594731330872, 'learning_rate': 1.433900477131882e-05, 'epoch': 2.44}\n",
            "{'loss': 0.2932, 'grad_norm': 0.8639799356460571, 'learning_rate': 1.3982459388617452e-05, 'epoch': 2.44}\n",
            "{'loss': 0.3165, 'grad_norm': 0.9402013421058655, 'learning_rate': 1.363006936107183e-05, 'epoch': 2.45}\n",
            "{'loss': 0.3003, 'grad_norm': 0.9194073677062988, 'learning_rate': 1.328185171202052e-05, 'epoch': 2.46}\n",
            "{'loss': 0.2911, 'grad_norm': 0.9016866087913513, 'learning_rate': 1.29378232632419e-05, 'epoch': 2.46}\n",
            "{'loss': 0.313, 'grad_norm': 0.8971663117408752, 'learning_rate': 1.259800063414146e-05, 'epoch': 2.47}\n",
            "{'loss': 0.3098, 'grad_norm': 0.8267617225646973, 'learning_rate': 1.2262400240949023e-05, 'epoch': 2.48}\n",
            "{'loss': 0.2689, 'grad_norm': 0.8640105128288269, 'learning_rate': 1.1931038295925645e-05, 'epoch': 2.48}\n",
            "{'loss': 0.322, 'grad_norm': 0.9347134828567505, 'learning_rate': 1.1603930806580444e-05, 'epoch': 2.49}\n",
            "{'loss': 0.2873, 'grad_norm': 0.85684734582901, 'learning_rate': 1.1281093574897338e-05, 'epoch': 2.5}\n",
            "{'loss': 0.3063, 'grad_norm': 0.8678149580955505, 'learning_rate': 1.0962542196571634e-05, 'epoch': 2.5}\n",
            "{'loss': 0.3204, 'grad_norm': 0.9319157004356384, 'learning_rate': 1.0648292060256649e-05, 'epoch': 2.51}\n",
            "{'loss': 0.3266, 'grad_norm': 1.0313990116119385, 'learning_rate': 1.0338358346820353e-05, 'epoch': 2.51}\n",
            "{'loss': 0.3531, 'grad_norm': 0.993046760559082, 'learning_rate': 1.0032756028611878e-05, 'epoch': 2.52}\n",
            "{'loss': 0.2978, 'grad_norm': 0.9106035828590393, 'learning_rate': 9.731499868738447e-06, 'epoch': 2.53}\n",
            "{'loss': 0.2661, 'grad_norm': 0.8859044909477234, 'learning_rate': 9.434604420351911e-06, 'epoch': 2.53}\n",
            "{'loss': 0.3072, 'grad_norm': 0.9432482719421387, 'learning_rate': 9.142084025945984e-06, 'epoch': 2.54}\n",
            "{'loss': 0.3678, 'grad_norm': 0.9464848041534424, 'learning_rate': 8.853952816663213e-06, 'epoch': 2.55}\n",
            "{'loss': 0.3106, 'grad_norm': 0.944381833076477, 'learning_rate': 8.570224711612385e-06, 'epoch': 2.55}\n",
            "{'loss': 0.3389, 'grad_norm': 0.9373452663421631, 'learning_rate': 8.290913417196177e-06, 'epoch': 2.56}\n",
            "{'loss': 0.315, 'grad_norm': 0.9166316390037537, 'learning_rate': 8.016032426448817e-06, 'epoch': 2.57}\n",
            "{'loss': 0.2863, 'grad_norm': 0.859229326248169, 'learning_rate': 7.745595018384578e-06, 'epoch': 2.57}\n",
            "{'loss': 0.3082, 'grad_norm': 0.9369514584541321, 'learning_rate': 7.479614257355971e-06, 'epoch': 2.58}\n",
            "{'loss': 0.2825, 'grad_norm': 0.8862320780754089, 'learning_rate': 7.2181029924228814e-06, 'epoch': 2.59}\n",
            "{'loss': 0.2709, 'grad_norm': 0.8573410511016846, 'learning_rate': 6.961073856731648e-06, 'epoch': 2.59}\n",
            "{'loss': 0.3408, 'grad_norm': 0.9897618293762207, 'learning_rate': 6.708539266905001e-06, 'epoch': 2.6}\n",
            "{'loss': 0.3398, 'grad_norm': 0.9837896227836609, 'learning_rate': 6.460511422441984e-06, 'epoch': 2.61}\n",
            "{'loss': 0.27, 'grad_norm': 0.9116575717926025, 'learning_rate': 6.217002305128849e-06, 'epoch': 2.61}\n",
            "{'loss': 0.2896, 'grad_norm': 0.8764071464538574, 'learning_rate': 5.978023678460099e-06, 'epoch': 2.62}\n",
            "{'loss': 0.3113, 'grad_norm': 0.9786668419837952, 'learning_rate': 5.743587087070235e-06, 'epoch': 2.62}\n",
            "{'loss': 0.3197, 'grad_norm': 0.9266015887260437, 'learning_rate': 5.5137038561761115e-06, 'epoch': 2.63}\n",
            "{'loss': 0.3436, 'grad_norm': 0.9443569779396057, 'learning_rate': 5.2883850910297235e-06, 'epoch': 2.64}\n",
            "{'loss': 0.2737, 'grad_norm': 0.8378391265869141, 'learning_rate': 5.067641676381918e-06, 'epoch': 2.64}\n",
            "{'loss': 0.2756, 'grad_norm': 0.8421200513839722, 'learning_rate': 4.8514842759563306e-06, 'epoch': 2.65}\n",
            "{'loss': 0.3114, 'grad_norm': 0.935858964920044, 'learning_rate': 4.639923331934471e-06, 'epoch': 2.66}\n",
            "{'loss': 0.3013, 'grad_norm': 0.9135474562644958, 'learning_rate': 4.432969064451109e-06, 'epoch': 2.66}\n",
            "{'loss': 0.3147, 'grad_norm': 0.9531347155570984, 'learning_rate': 4.230631471100655e-06, 'epoch': 2.67}\n",
            "{'loss': 0.2993, 'grad_norm': 0.9104506969451904, 'learning_rate': 4.032920326454159e-06, 'epoch': 2.68}\n",
            "{'loss': 0.3152, 'grad_norm': 0.9628345370292664, 'learning_rate': 3.839845181587098e-06, 'epoch': 2.68}\n",
            "{'loss': 0.3203, 'grad_norm': 0.9403917193412781, 'learning_rate': 3.6514153636180383e-06, 'epoch': 2.69}\n",
            "{'loss': 0.3277, 'grad_norm': 0.9263449311256409, 'learning_rate': 3.467639975257997e-06, 'epoch': 2.7}\n",
            "{'loss': 0.3204, 'grad_norm': 0.9565795063972473, 'learning_rate': 3.288527894370752e-06, 'epoch': 2.7}\n",
            "{'loss': 0.328, 'grad_norm': 0.9131007194519043, 'learning_rate': 3.1140877735439387e-06, 'epoch': 2.71}\n",
            "{'loss': 0.3011, 'grad_norm': 0.8954637050628662, 'learning_rate': 2.944328039671085e-06, 'epoch': 2.72}\n",
            "{'loss': 0.3355, 'grad_norm': 1.0676594972610474, 'learning_rate': 2.7792568935444796e-06, 'epoch': 2.72}\n",
            "{'loss': 0.3349, 'grad_norm': 0.9679346084594727, 'learning_rate': 2.618882309459081e-06, 'epoch': 2.73}\n",
            "{'loss': 0.3226, 'grad_norm': 0.950360119342804, 'learning_rate': 2.4632120348272003e-06, 'epoch': 2.73}\n",
            "{'loss': 0.2915, 'grad_norm': 0.9013980627059937, 'learning_rate': 2.312253589804314e-06, 'epoch': 2.74}\n",
            "{'loss': 0.299, 'grad_norm': 0.8663946390151978, 'learning_rate': 2.166014266925731e-06, 'epoch': 2.75}\n",
            "{'loss': 0.342, 'grad_norm': 0.9608665108680725, 'learning_rate': 2.0245011307543416e-06, 'epoch': 2.75}\n",
            "{'loss': 0.2909, 'grad_norm': 1.2331898212432861, 'learning_rate': 1.88772101753929e-06, 'epoch': 2.76}\n",
            "{'loss': 0.3583, 'grad_norm': 1.0225828886032104, 'learning_rate': 1.7556805348858064e-06, 'epoch': 2.77}\n",
            "{'loss': 0.286, 'grad_norm': 0.9153128862380981, 'learning_rate': 1.6283860614358936e-06, 'epoch': 2.77}\n",
            "{'loss': 0.349, 'grad_norm': 1.0081124305725098, 'learning_rate': 1.5058437465602982e-06, 'epoch': 2.78}\n",
            "{'loss': 0.3132, 'grad_norm': 0.9056407809257507, 'learning_rate': 1.3880595100613792e-06, 'epoch': 2.79}\n",
            "{'loss': 0.2758, 'grad_norm': 0.8564803600311279, 'learning_rate': 1.2750390418871604e-06, 'epoch': 2.79}\n",
            "{'loss': 0.2848, 'grad_norm': 0.8507845997810364, 'learning_rate': 1.1667878018564171e-06, 'epoch': 2.8}\n",
            "{'loss': 0.2725, 'grad_norm': 0.9435500502586365, 'learning_rate': 1.063311019395008e-06, 'epoch': 2.81}\n",
            "{'loss': 0.3329, 'grad_norm': 1.067020297050476, 'learning_rate': 9.64613693283123e-07, 'epoch': 2.81}\n",
            "{'loss': 0.3125, 'grad_norm': 0.8869338035583496, 'learning_rate': 8.707005914139422e-07, 'epoch': 2.82}\n",
            "{'loss': 0.3053, 'grad_norm': 0.9306982159614563, 'learning_rate': 7.815762505632096e-07, 'epoch': 2.83}\n",
            "{'loss': 0.3458, 'grad_norm': 0.9634831547737122, 'learning_rate': 6.972449761700861e-07, 'epoch': 2.83}\n",
            "{'loss': 0.2746, 'grad_norm': 0.8726220726966858, 'learning_rate': 6.177108421292266e-07, 'epoch': 2.84}\n",
            "{'loss': 0.2941, 'grad_norm': 0.9016880989074707, 'learning_rate': 5.429776905938489e-07, 'epoch': 2.84}\n",
            "{'loss': 0.3216, 'grad_norm': 0.8883452415466309, 'learning_rate': 4.7304913179025965e-07, 'epoch': 2.85}\n",
            "{'loss': 0.2917, 'grad_norm': 0.8673025369644165, 'learning_rate': 4.0792854384338333e-07, 'epoch': 2.86}\n",
            "{'loss': 0.2473, 'grad_norm': 0.8197271823883057, 'learning_rate': 3.4761907261356976e-07, 'epoch': 2.86}\n",
            "{'loss': 0.2774, 'grad_norm': 0.8568001985549927, 'learning_rate': 2.921236315446385e-07, 'epoch': 2.87}\n",
            "{'loss': 0.2654, 'grad_norm': 0.8372655510902405, 'learning_rate': 2.414449015231357e-07, 'epoch': 2.88}\n",
            "{'loss': 0.2891, 'grad_norm': 0.9056695699691772, 'learning_rate': 1.9558533074882646e-07, 'epoch': 2.88}\n",
            "{'loss': 0.2992, 'grad_norm': 0.8683817982673645, 'learning_rate': 1.545471346164007e-07, 'epoch': 2.89}\n",
            "{'loss': 0.3358, 'grad_norm': 1.0043821334838867, 'learning_rate': 1.1833229560848092e-07, 'epoch': 2.9}\n",
            "{'loss': 0.287, 'grad_norm': 0.8615964651107788, 'learning_rate': 8.694256319987659e-08, 'epoch': 2.9}\n",
            "{'loss': 0.2974, 'grad_norm': 0.8673582077026367, 'learning_rate': 6.037945377297405e-08, 'epoch': 2.91}\n",
            "{'loss': 0.2825, 'grad_norm': 0.9060268402099609, 'learning_rate': 3.8644250544594975e-08, 'epoch': 2.92}\n",
            "{'loss': 0.3462, 'grad_norm': 1.0042473077774048, 'learning_rate': 2.1738003503946057e-08, 'epoch': 2.92}\n",
            "{'loss': 0.2768, 'grad_norm': 0.9257480502128601, 'learning_rate': 9.661529361892907e-09, 'epoch': 2.93}\n",
            "{'loss': 0.2438, 'grad_norm': 0.8297643065452576, 'learning_rate': 2.4154115115360144e-09, 'epoch': 2.94}\n",
            "{'loss': 0.2803, 'grad_norm': 0.9162737131118774, 'learning_rate': 0.0, 'epoch': 2.94}\n",
            "100% 462/462 [5:50:12<00:00, 44.70s/it][2024-03-08 01:01:25,291] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "[2024-03-08 01:01:30,445] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "[2024-03-08 01:01:30,447] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            "  0% 0/23 [00:00<?, ?it/s]\u001b[A[2024-03-08 01:01:36,937] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            "  9% 2/23 [00:06<01:08,  3.24s/it]\u001b[A[2024-03-08 01:01:43,599] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 13% 3/23 [00:13<01:33,  4.67s/it]\u001b[A[2024-03-08 01:01:50,368] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 17% 4/23 [00:19<01:43,  5.45s/it]\u001b[A[2024-03-08 01:01:57,278] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 22% 5/23 [00:26<01:47,  5.96s/it]\u001b[A[2024-03-08 01:02:04,119] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 26% 6/23 [00:33<01:46,  6.25s/it]\u001b[A[2024-03-08 01:02:10,740] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 30% 7/23 [00:40<01:41,  6.37s/it]\u001b[A[2024-03-08 01:02:17,378] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 35% 8/23 [00:46<01:36,  6.45s/it]\u001b[A[2024-03-08 01:02:23,916] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 39% 9/23 [00:53<01:30,  6.48s/it]\u001b[A[2024-03-08 01:02:30,354] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 43% 10/23 [00:59<01:24,  6.47s/it]\u001b[A[2024-03-08 01:02:36,757] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 48% 11/23 [01:06<01:17,  6.45s/it]\u001b[A[2024-03-08 01:02:43,209] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 52% 12/23 [01:12<01:10,  6.45s/it]\u001b[A[2024-03-08 01:02:49,773] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 57% 13/23 [01:19<01:04,  6.48s/it]\u001b[A[2024-03-08 01:02:56,417] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 61% 14/23 [01:25<00:58,  6.53s/it]\u001b[A[2024-03-08 01:03:03,117] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 65% 15/23 [01:32<00:52,  6.58s/it]\u001b[A[2024-03-08 01:03:09,848] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 70% 16/23 [01:39<00:46,  6.63s/it]\u001b[A[2024-03-08 01:03:16,570] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 74% 17/23 [01:46<00:39,  6.66s/it]\u001b[A[2024-03-08 01:03:23,241] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 78% 18/23 [01:52<00:33,  6.66s/it]\u001b[A[2024-03-08 01:03:29,839] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 83% 19/23 [01:59<00:26,  6.64s/it]\u001b[A[2024-03-08 01:03:36,468] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 87% 20/23 [02:06<00:19,  6.64s/it]\u001b[A[2024-03-08 01:03:43,064] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 91% 21/23 [02:12<00:13,  6.63s/it]\u001b[A[2024-03-08 01:03:49,655] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            " 96% 22/23 [02:19<00:06,  6.61s/it]\u001b[A[2024-03-08 01:03:56,175] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            "100% 23/23 [02:25<00:00,  6.59s/it]\u001b[A[2024-03-08 01:04:02,748] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            "24it [02:32,  6.58s/it]            \u001b[A[2024-03-08 01:04:09,297] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            "25it [02:38,  6.57s/it]\u001b[A[2024-03-08 01:04:13,457] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:48199] [RANK:0] packing_efficiency_estimate: 0.91 total_num_tokens per device: 96733\u001b[39m\n",
            "\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.6804437041282654, 'eval_runtime': 168.8517, 'eval_samples_per_second': 2.025, 'eval_steps_per_second': 0.509, 'epoch': 2.94}\n",
            "100% 462/462 [5:53:09<00:00, 44.70s/it]\n",
            "26it [02:43,  5.85s/it]\u001b[A\n",
            "{'train_runtime': 21190.3616, 'train_samples_per_second': 0.919, 'train_steps_per_second': 0.022, 'train_loss': 0.8692148915487966, 'epoch': 2.94}\n",
            "100% 462/462 [5:53:09<00:00, 45.87s/it]\n",
            "[2024-03-08 01:04:14,163] [INFO] [axolotl.train.log:61] [PID:48199] [RANK:0] Training Completed!!! Saving pre-trained model to /content/drive/MyDrive/DACON/Hansol_QA/models/yanolja-axolotl-aug-noise-sysprompt\u001b[39m\n",
            "(PeftModelForCausalLM(   (base_model): LoraModel(     (model): LlamaForCausalLM(       (model): LlamaModel(         (embed_tokens): Embedding(40960, 4096)         (layers): ModuleList(           (0-47): 48 x LlamaDecoderLayer(             (self_attn): LlamaSdpaAttention(               (q_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (k_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=1024, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (v_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=1024, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (o_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (rotary_emb): LlamaRotaryEmbedding()             )             (mlp): LlamaMLP(               (gate_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=14336, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (up_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=14336, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (down_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=14336, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (act_fn): SiLU()             )             (input_layernorm): LlamaRMSNorm()             (post_attention_layernorm): LlamaRMSNorm()           )         )         (norm): LlamaRMSNorm()       )       (lm_head): Linear(in_features=4096, out_features=40960, bias=False)     )   ) ), LlamaTokenizerFast(name_or_path='yanolja/KoSOLAR-10.7B-v0.2', vocab_size=40960, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|im_end|>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={ \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), \t32000: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), })\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "import time\n",
        "\n",
        "time.sleep(600)\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "RE1mrt-2fM-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m axolotl.cli.merge_lora config.yaml --lora_model_dir=\"/content/drive/MyDrive/DACON/Hansol_QA/models/yanolja-axolotl-aug-noise-sysprompt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKsKcxGQMNte",
        "outputId": "0092ebe4-a1aa-42c8-a047-d63b1ddc966a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cextension.py:31: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
            "[2024-03-08 07:14:43,701] [INFO] [numexpr.utils._init_num_threads:160] [PID:2300] NumExpr defaulting to 8 threads.\n",
            "[2024-03-08 07:14:44,046] [INFO] [datasets.<module>:58] [PID:2300] PyTorch version 2.1.1 available.\n",
            "[2024-03-08 07:14:44,047] [INFO] [datasets.<module>:95] [PID:2300] TensorFlow version 2.15.0 available.\n",
            "[2024-03-08 07:14:44,048] [INFO] [datasets.<module>:108] [PID:2300] JAX version 0.4.23 available.\n",
            "2024-03-08 07:14:46.225684: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 07:14:46.225727: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 07:14:46.227229: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 07:14:47.446345: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2024-03-08 07:14:48,660] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "                                 dP            dP   dP \n",
            "                                 88            88   88 \n",
            "      .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n",
            "      88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n",
            "      88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n",
            "      `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP \n",
            "                                                       \n",
            "                                                       \n",
            "\n",
            "config.json: 100% 691/691 [00:00<00:00, 3.65MB/s]\n",
            "[2024-03-08 07:14:52,879] [INFO] [axolotl.normalize_config:178] [PID:2300] [RANK:0] GPU memory usage baseline: 0.000GB ()\u001b[39m\n",
            "[2024-03-08 07:14:53,798] [INFO] [axolotl.common.cli.load_model_and_tokenizer:50] [PID:2300] [RANK:0] loading tokenizer... yanolja/KoSOLAR-10.7B-v0.2\u001b[39m\n",
            "tokenizer_config.json: 100% 1.20k/1.20k [00:00<00:00, 7.12MB/s]\n",
            "tokenizer.json: 100% 2.18M/2.18M [00:00<00:00, 8.68MB/s]\n",
            "special_tokens_map.json: 100% 557/557 [00:00<00:00, 2.77MB/s]\n",
            "[2024-03-08 07:14:55,354] [DEBUG] [axolotl.load_tokenizer:245] [PID:2300] [RANK:0] EOS: 32000 / <|im_end|>\u001b[39m\n",
            "[2024-03-08 07:14:55,354] [DEBUG] [axolotl.load_tokenizer:246] [PID:2300] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2024-03-08 07:14:55,354] [DEBUG] [axolotl.load_tokenizer:247] [PID:2300] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
            "[2024-03-08 07:14:55,354] [DEBUG] [axolotl.load_tokenizer:248] [PID:2300] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2024-03-08 07:14:55,354] [INFO] [axolotl.load_tokenizer:259] [PID:2300] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2024-03-08 07:14:55,354] [INFO] [axolotl.common.cli.load_model_and_tokenizer:52] [PID:2300] [RANK:0] loading model and (optionally) peft_config...\u001b[39m\n",
            "[2024-03-08 07:14:55,459] [INFO] [axolotl.load_model:348] [PID:2300] [RANK:0] patching llama _prepare_4d_causal_attention_mask*\u001b[39m\n",
            "[2024-03-08 07:14:55,465] [INFO] [axolotl.load_model:371] [PID:2300] [RANK:0] patching _expand_mask\u001b[39m\n",
            "model.safetensors.index.json: 100% 35.8k/35.8k [00:00<00:00, 103MB/s]\n",
            "Downloading shards:   0% 0/5 [00:00<?, ?it/s]\n",
            "model-00001-of-00005.safetensors:   0% 0.00/4.90G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   0% 10.5M/4.90G [00:00<04:59, 16.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   0% 21.0M/4.90G [00:00<02:52, 28.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   1% 31.5M/4.90G [00:01<02:12, 36.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   1% 41.9M/4.90G [00:01<01:53, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   1% 52.4M/4.90G [00:01<01:42, 47.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   1% 62.9M/4.90G [00:01<01:35, 50.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   1% 73.4M/4.90G [00:01<01:31, 52.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   2% 83.9M/4.90G [00:01<01:29, 53.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   2% 94.4M/4.90G [00:02<01:27, 55.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   2% 105M/4.90G [00:02<01:26, 55.7MB/s] \u001b[A\n",
            "model-00001-of-00005.safetensors:   2% 115M/4.90G [00:02<01:25, 56.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   3% 126M/4.90G [00:02<01:24, 56.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   3% 136M/4.90G [00:02<01:23, 56.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   3% 147M/4.90G [00:03<01:23, 57.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   3% 157M/4.90G [00:03<01:23, 57.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   3% 168M/4.90G [00:03<01:22, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   4% 178M/4.90G [00:03<01:23, 56.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   4% 189M/4.90G [00:03<01:22, 56.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   4% 199M/4.90G [00:04<01:51, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   4% 210M/4.90G [00:04<01:42, 45.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   4% 220M/4.90G [00:04<01:35, 48.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   5% 231M/4.90G [00:04<01:42, 45.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   5% 241M/4.90G [00:05<01:58, 39.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   5% 252M/4.90G [00:05<01:47, 43.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   5% 262M/4.90G [00:05<01:59, 38.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   6% 273M/4.90G [00:06<02:19, 33.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   6% 283M/4.90G [00:06<02:01, 37.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   6% 294M/4.90G [00:06<01:57, 39.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   6% 304M/4.90G [00:06<01:53, 40.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   6% 315M/4.90G [00:06<01:43, 44.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   7% 325M/4.90G [00:07<01:39, 46.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   7% 336M/4.90G [00:07<01:42, 44.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   7% 346M/4.90G [00:07<01:40, 45.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   7% 357M/4.90G [00:07<01:54, 39.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   7% 367M/4.90G [00:08<01:43, 43.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   8% 377M/4.90G [00:08<01:41, 44.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   8% 388M/4.90G [00:08<01:51, 40.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   8% 398M/4.90G [00:08<01:41, 44.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   8% 409M/4.90G [00:09<01:54, 39.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   9% 419M/4.90G [00:09<01:52, 40.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   9% 430M/4.90G [00:09<01:41, 43.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   9% 440M/4.90G [00:09<01:45, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   9% 451M/4.90G [00:10<01:55, 38.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   9% 461M/4.90G [00:10<01:43, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  10% 472M/4.90G [00:10<01:53, 38.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  10% 482M/4.90G [00:10<01:42, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  10% 493M/4.90G [00:11<01:47, 41.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  10% 503M/4.90G [00:11<01:37, 45.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  10% 514M/4.90G [00:11<01:32, 47.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  11% 524M/4.90G [00:11<01:28, 49.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  11% 535M/4.90G [00:11<01:24, 51.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  11% 545M/4.90G [00:12<01:21, 53.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  11% 556M/4.90G [00:12<01:19, 55.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  12% 566M/4.90G [00:12<01:17, 55.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  12% 577M/4.90G [00:12<01:16, 56.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  12% 587M/4.90G [00:12<01:16, 56.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  12% 598M/4.90G [00:13<01:15, 56.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  12% 608M/4.90G [00:13<01:19, 54.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  13% 619M/4.90G [00:13<01:17, 54.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  13% 629M/4.90G [00:13<01:32, 46.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  13% 640M/4.90G [00:14<01:48, 39.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  13% 650M/4.90G [00:14<01:39, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  13% 661M/4.90G [00:14<01:52, 37.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  14% 671M/4.90G [00:15<02:02, 34.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  14% 682M/4.90G [00:15<01:59, 35.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  14% 692M/4.90G [00:15<02:06, 33.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  14% 703M/4.90G [00:16<02:11, 31.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  15% 713M/4.90G [00:16<02:07, 32.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  15% 724M/4.90G [00:16<02:09, 32.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  15% 734M/4.90G [00:17<02:13, 31.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  15% 744M/4.90G [00:17<02:06, 32.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  15% 755M/4.90G [00:17<02:10, 31.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  16% 765M/4.90G [00:17<02:04, 33.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  16% 776M/4.90G [00:18<02:08, 32.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  16% 786M/4.90G [00:18<02:04, 33.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  16% 797M/4.90G [00:18<01:57, 34.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  16% 807M/4.90G [00:19<02:03, 33.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  17% 818M/4.90G [00:19<01:58, 34.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  17% 828M/4.90G [00:19<01:55, 35.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  17% 839M/4.90G [00:20<02:01, 33.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  17% 849M/4.90G [00:20<01:57, 34.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  18% 860M/4.90G [00:20<01:54, 35.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  18% 870M/4.90G [00:20<01:51, 36.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  18% 881M/4.90G [00:21<01:58, 34.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  18% 891M/4.90G [00:21<01:54, 35.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  18% 902M/4.90G [00:21<01:51, 35.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  19% 912M/4.90G [00:22<01:50, 36.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  19% 923M/4.90G [00:22<01:56, 34.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  19% 933M/4.90G [00:22<01:52, 35.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  19% 944M/4.90G [00:23<01:49, 36.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  19% 954M/4.90G [00:23<01:48, 36.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  20% 965M/4.90G [00:23<02:04, 31.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  20% 975M/4.90G [00:24<01:58, 33.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  20% 986M/4.90G [00:24<02:04, 31.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  20% 996M/4.90G [00:24<02:15, 28.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  21% 1.01G/4.90G [00:25<02:15, 28.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  21% 1.02G/4.90G [00:25<02:15, 28.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  21% 1.03G/4.90G [00:25<02:14, 28.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  21% 1.04G/4.90G [00:26<02:13, 28.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  21% 1.05G/4.90G [00:26<02:05, 30.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  22% 1.06G/4.90G [00:26<02:07, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  22% 1.07G/4.90G [00:27<02:07, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  22% 1.08G/4.90G [00:27<02:00, 31.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  22% 1.09G/4.90G [00:27<02:02, 31.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  22% 1.10G/4.90G [00:28<01:57, 32.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  23% 1.11G/4.90G [00:28<01:59, 31.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  23% 1.12G/4.90G [00:28<01:54, 33.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  23% 1.13G/4.90G [00:29<01:57, 32.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  23% 1.14G/4.90G [00:29<01:52, 33.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  24% 1.15G/4.90G [00:29<01:55, 32.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  24% 1.16G/4.90G [00:30<01:50, 33.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  24% 1.17G/4.90G [00:30<01:47, 34.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  24% 1.18G/4.90G [00:30<01:51, 33.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  24% 1.20G/4.90G [00:31<01:47, 34.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  25% 1.21G/4.90G [00:31<01:45, 35.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  25% 1.22G/4.90G [00:31<01:49, 33.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  25% 1.23G/4.90G [00:31<01:45, 34.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  25% 1.24G/4.90G [00:32<01:43, 35.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  25% 1.25G/4.90G [00:32<01:46, 34.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  26% 1.26G/4.90G [00:32<01:44, 34.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  26% 1.27G/4.90G [00:33<01:42, 35.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  26% 1.28G/4.90G [00:33<01:40, 35.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  26% 1.29G/4.90G [00:33<01:44, 34.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  27% 1.30G/4.90G [00:34<01:42, 35.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  27% 1.31G/4.90G [00:34<01:40, 35.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  27% 1.32G/4.90G [00:34<01:39, 36.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  27% 1.33G/4.90G [00:34<01:43, 34.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  27% 1.34G/4.90G [00:35<01:41, 35.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  28% 1.35G/4.90G [00:35<01:48, 32.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  28% 1.36G/4.90G [00:35<01:44, 33.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  28% 1.37G/4.90G [00:36<01:56, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  28% 1.38G/4.90G [00:36<01:58, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  28% 1.39G/4.90G [00:37<01:59, 29.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  29% 1.41G/4.90G [00:37<02:00, 29.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  29% 1.42G/4.90G [00:37<01:59, 29.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  29% 1.43G/4.90G [00:38<01:59, 29.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  29% 1.44G/4.90G [00:38<01:58, 29.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  30% 1.45G/4.90G [00:38<01:56, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  30% 1.46G/4.90G [00:39<01:51, 30.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  30% 1.47G/4.90G [00:39<01:52, 30.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  30% 1.48G/4.90G [00:39<01:48, 31.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  30% 1.49G/4.90G [00:40<01:48, 31.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  31% 1.50G/4.90G [00:40<01:44, 32.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  31% 1.51G/4.90G [00:40<01:45, 32.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  31% 1.52G/4.90G [00:41<01:41, 33.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  31% 1.53G/4.90G [00:41<01:43, 32.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  31% 1.54G/4.90G [00:41<01:39, 33.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  32% 1.55G/4.90G [00:41<01:37, 34.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  32% 1.56G/4.90G [00:42<01:40, 33.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  32% 1.57G/4.90G [00:42<01:36, 34.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  32% 1.58G/4.90G [00:42<01:35, 34.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  33% 1.59G/4.90G [00:43<01:37, 33.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  33% 1.60G/4.90G [00:43<01:34, 34.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  33% 1.61G/4.90G [00:43<01:33, 35.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  33% 1.63G/4.90G [00:44<01:35, 34.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  33% 1.64G/4.90G [00:44<01:33, 34.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  34% 1.65G/4.90G [00:44<01:31, 35.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  34% 1.66G/4.90G [00:44<01:31, 35.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  34% 1.67G/4.90G [00:45<01:33, 34.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  34% 1.68G/4.90G [00:45<01:33, 34.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  34% 1.69G/4.90G [00:46<01:44, 30.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  35% 1.70G/4.90G [00:46<01:49, 29.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  35% 1.71G/4.90G [00:46<01:54, 28.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  35% 1.72G/4.90G [00:47<01:54, 27.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  35% 1.73G/4.90G [00:47<01:53, 28.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  36% 1.74G/4.90G [00:47<01:53, 27.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  36% 1.75G/4.90G [00:48<01:54, 27.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  36% 1.76G/4.90G [00:48<01:53, 27.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  36% 1.77G/4.90G [00:49<01:51, 28.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  36% 1.78G/4.90G [00:49<01:50, 28.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  37% 1.79G/4.90G [00:49<01:48, 28.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  37% 1.80G/4.90G [00:50<01:47, 28.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  37% 1.81G/4.90G [00:50<01:44, 29.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  37% 1.82G/4.90G [00:50<01:44, 29.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  37% 1.84G/4.90G [00:51<01:43, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  38% 1.85G/4.90G [00:51<01:44, 29.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  38% 1.86G/4.90G [00:51<01:43, 29.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  38% 1.87G/4.90G [00:52<01:42, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  38% 1.88G/4.90G [00:52<01:42, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  39% 1.89G/4.90G [00:53<01:48, 27.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  39% 1.90G/4.90G [00:53<01:36, 31.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  39% 1.91G/4.90G [00:53<01:37, 30.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  39% 1.92G/4.90G [00:54<01:38, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  39% 1.93G/4.90G [00:54<01:39, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  40% 1.94G/4.90G [00:54<01:38, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  40% 1.95G/4.90G [00:55<01:37, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  40% 1.96G/4.90G [00:55<01:36, 30.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  40% 1.97G/4.90G [00:55<01:36, 30.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  40% 1.98G/4.90G [00:56<01:37, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  41% 1.99G/4.90G [00:56<01:37, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  41% 2.00G/4.90G [00:56<01:36, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  41% 2.01G/4.90G [00:57<01:35, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  41% 2.02G/4.90G [00:57<01:34, 30.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  42% 2.03G/4.90G [00:57<01:34, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  42% 2.04G/4.90G [00:58<01:35, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  42% 2.06G/4.90G [00:58<01:35, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  42% 2.07G/4.90G [00:58<01:34, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  42% 2.08G/4.90G [00:59<01:31, 30.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  43% 2.09G/4.90G [00:59<01:31, 30.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  43% 2.10G/4.90G [00:59<01:32, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  43% 2.11G/4.90G [01:00<01:31, 30.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  43% 2.12G/4.90G [01:00<01:29, 31.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  43% 2.13G/4.90G [01:00<01:29, 30.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  44% 2.14G/4.90G [01:01<01:30, 30.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  44% 2.15G/4.90G [01:01<01:26, 31.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  44% 2.16G/4.90G [01:01<01:27, 31.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  44% 2.17G/4.90G [01:02<01:24, 32.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  45% 2.18G/4.90G [01:02<01:25, 31.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  45% 2.19G/4.90G [01:02<01:23, 32.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  45% 2.20G/4.90G [01:03<01:23, 32.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  45% 2.21G/4.90G [01:03<01:20, 33.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  45% 2.22G/4.90G [01:03<01:21, 32.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  46% 2.23G/4.90G [01:04<01:18, 33.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  46% 2.24G/4.90G [01:04<01:16, 34.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  46% 2.25G/4.90G [01:04<01:18, 33.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  46% 2.26G/4.90G [01:05<01:15, 34.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  46% 2.28G/4.90G [01:05<01:13, 35.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  47% 2.29G/4.90G [01:05<01:11, 36.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  47% 2.30G/4.90G [01:05<01:10, 36.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  47% 2.31G/4.90G [01:06<01:09, 37.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  47% 2.32G/4.90G [01:06<01:08, 37.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  48% 2.33G/4.90G [01:06<01:07, 38.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  48% 2.34G/4.90G [01:06<01:06, 38.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  48% 2.35G/4.90G [01:07<01:02, 40.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  48% 2.36G/4.90G [01:07<01:01, 41.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  48% 2.37G/4.90G [01:07<01:02, 40.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  49% 2.38G/4.90G [01:07<01:02, 40.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  49% 2.39G/4.90G [01:08<00:57, 43.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  49% 2.40G/4.90G [01:08<00:58, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  49% 2.41G/4.90G [01:08<00:55, 45.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  49% 2.42G/4.90G [01:08<00:56, 43.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  50% 2.43G/4.90G [01:09<00:53, 46.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  50% 2.44G/4.90G [01:09<00:54, 44.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  50% 2.45G/4.90G [01:09<00:51, 47.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  50% 2.46G/4.90G [01:09<00:49, 49.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  51% 2.47G/4.90G [01:09<00:50, 48.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  51% 2.49G/4.90G [01:10<00:50, 48.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  51% 2.50G/4.90G [01:10<00:47, 50.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  51% 2.51G/4.90G [01:10<00:45, 52.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  51% 2.52G/4.90G [01:10<00:44, 53.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  52% 2.53G/4.90G [01:10<00:44, 53.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  52% 2.54G/4.90G [01:11<00:43, 54.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  52% 2.55G/4.90G [01:11<00:42, 55.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  52% 2.56G/4.90G [01:11<00:42, 55.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  52% 2.57G/4.90G [01:11<00:41, 55.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  53% 2.58G/4.90G [01:11<00:41, 56.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  53% 2.59G/4.90G [01:11<00:41, 56.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  53% 2.60G/4.90G [01:12<00:40, 56.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  53% 2.61G/4.90G [01:12<00:40, 56.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  54% 2.62G/4.90G [01:12<00:40, 55.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  54% 2.63G/4.90G [01:12<00:41, 55.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  54% 2.64G/4.90G [01:12<00:40, 55.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  54% 2.65G/4.90G [01:13<00:40, 55.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  54% 2.66G/4.90G [01:13<00:39, 56.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  55% 2.67G/4.90G [01:13<00:39, 56.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  55% 2.68G/4.90G [01:13<00:39, 56.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  55% 2.69G/4.90G [01:13<00:38, 56.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  55% 2.71G/4.90G [01:14<00:38, 57.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  55% 2.72G/4.90G [01:14<00:38, 57.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  56% 2.73G/4.90G [01:14<00:37, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  56% 2.74G/4.90G [01:14<00:37, 57.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  56% 2.75G/4.90G [01:14<00:37, 57.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  56% 2.76G/4.90G [01:14<00:37, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  57% 2.77G/4.90G [01:15<00:37, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  57% 2.78G/4.90G [01:15<00:36, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  57% 2.79G/4.90G [01:15<00:36, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  57% 2.80G/4.90G [01:15<00:36, 57.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  57% 2.81G/4.90G [01:15<00:36, 57.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  58% 2.82G/4.90G [01:16<00:36, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  58% 2.83G/4.90G [01:16<00:36, 57.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  58% 2.84G/4.90G [01:16<00:35, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  58% 2.85G/4.90G [01:16<00:35, 57.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  58% 2.86G/4.90G [01:16<00:35, 57.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  59% 2.87G/4.90G [01:16<00:36, 55.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  59% 2.88G/4.90G [01:17<00:40, 49.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  59% 2.89G/4.90G [01:17<00:38, 51.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  59% 2.90G/4.90G [01:17<00:43, 46.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  60% 2.92G/4.90G [01:17<00:45, 43.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  60% 2.93G/4.90G [01:18<00:42, 45.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  60% 2.94G/4.90G [01:18<00:44, 44.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  60% 2.95G/4.90G [01:18<00:46, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  60% 2.96G/4.90G [01:18<00:43, 45.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  61% 2.97G/4.90G [01:19<00:44, 43.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  61% 2.98G/4.90G [01:19<00:45, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  61% 2.99G/4.90G [01:19<00:42, 45.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  61% 3.00G/4.90G [01:19<00:43, 43.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  61% 3.01G/4.90G [01:20<00:41, 46.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  62% 3.02G/4.90G [01:20<00:42, 44.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  62% 3.03G/4.90G [01:20<00:40, 46.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  62% 3.04G/4.90G [01:20<00:41, 44.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  62% 3.05G/4.90G [01:21<00:39, 47.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  62% 3.06G/4.90G [01:21<00:41, 44.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  63% 3.07G/4.90G [01:21<00:38, 47.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  63% 3.08G/4.90G [01:21<00:40, 44.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  63% 3.09G/4.90G [01:21<00:38, 47.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  63% 3.10G/4.90G [01:22<00:39, 44.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  64% 3.11G/4.90G [01:22<00:37, 47.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  64% 3.12G/4.90G [01:22<00:36, 48.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  64% 3.14G/4.90G [01:22<00:37, 46.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  64% 3.15G/4.90G [01:22<00:35, 49.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  64% 3.16G/4.90G [01:23<00:38, 45.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  65% 3.17G/4.90G [01:23<00:35, 48.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  65% 3.18G/4.90G [01:23<00:34, 49.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  65% 3.19G/4.90G [01:23<00:36, 46.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  65% 3.20G/4.90G [01:24<00:34, 49.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  65% 3.21G/4.90G [01:24<00:35, 47.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  66% 3.22G/4.90G [01:24<00:35, 48.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  66% 3.23G/4.90G [01:24<00:33, 50.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  66% 3.24G/4.90G [01:24<00:35, 46.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  66% 3.25G/4.90G [01:25<00:33, 48.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  67% 3.26G/4.90G [01:25<00:32, 50.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  67% 3.27G/4.90G [01:25<00:34, 47.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  67% 3.28G/4.90G [01:25<00:32, 49.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  67% 3.29G/4.90G [01:26<00:31, 50.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  67% 3.30G/4.90G [01:26<00:33, 47.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  68% 3.31G/4.90G [01:26<00:32, 49.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  68% 3.32G/4.90G [01:26<00:30, 51.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  68% 3.33G/4.90G [01:26<00:32, 47.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  68% 3.34G/4.90G [01:27<00:31, 49.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  68% 3.36G/4.90G [01:27<00:30, 51.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  69% 3.37G/4.90G [01:27<00:32, 47.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  69% 3.38G/4.90G [01:27<00:30, 49.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  69% 3.39G/4.90G [01:27<00:29, 51.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  69% 3.40G/4.90G [01:28<00:32, 46.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  70% 3.41G/4.90G [01:28<00:33, 44.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  70% 3.42G/4.90G [01:28<00:38, 38.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  70% 3.43G/4.90G [01:29<00:38, 38.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  70% 3.44G/4.90G [01:29<00:38, 38.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  70% 3.45G/4.90G [01:29<00:37, 38.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  71% 3.46G/4.90G [01:29<00:37, 38.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  71% 3.47G/4.90G [01:30<00:37, 38.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  71% 3.48G/4.90G [01:30<00:33, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  71% 3.49G/4.90G [01:30<00:34, 40.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  71% 3.50G/4.90G [01:30<00:34, 40.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  72% 3.51G/4.90G [01:31<00:34, 40.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  72% 3.52G/4.90G [01:31<00:35, 39.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  72% 3.53G/4.90G [01:31<00:38, 35.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  72% 3.54G/4.90G [01:32<00:40, 33.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  73% 3.55G/4.90G [01:32<00:39, 34.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  73% 3.57G/4.90G [01:32<00:40, 32.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  73% 3.58G/4.90G [01:33<00:42, 31.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  73% 3.59G/4.90G [01:33<00:39, 33.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  73% 3.60G/4.90G [01:33<00:40, 32.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  74% 3.61G/4.90G [01:34<00:42, 30.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  74% 3.62G/4.90G [01:34<00:45, 27.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  74% 3.63G/4.90G [01:35<00:45, 27.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  74% 3.64G/4.90G [01:35<00:47, 26.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  74% 3.65G/4.90G [01:35<00:48, 25.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  75% 3.66G/4.90G [01:36<00:47, 26.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  75% 3.67G/4.90G [01:36<00:46, 26.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  75% 3.68G/4.90G [01:37<00:47, 25.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  75% 3.69G/4.90G [01:37<00:45, 26.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  76% 3.70G/4.90G [01:37<00:44, 27.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  76% 3.71G/4.90G [01:38<00:43, 27.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  76% 3.72G/4.90G [01:38<00:43, 27.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  76% 3.73G/4.90G [01:39<00:43, 26.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  76% 3.74G/4.90G [01:39<00:42, 27.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  77% 3.75G/4.90G [01:39<00:41, 27.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  77% 3.76G/4.90G [01:40<00:40, 28.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  77% 3.77G/4.90G [01:40<00:39, 28.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  77% 3.79G/4.90G [01:40<00:39, 28.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  77% 3.80G/4.90G [01:41<00:38, 28.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  78% 3.81G/4.90G [01:41<00:38, 28.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  78% 3.82G/4.90G [01:41<00:37, 28.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  78% 3.83G/4.90G [01:42<00:37, 28.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  78% 3.84G/4.90G [01:42<00:37, 28.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  79% 3.85G/4.90G [01:43<00:36, 28.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  79% 3.86G/4.90G [01:43<00:36, 28.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  79% 3.87G/4.90G [01:43<00:35, 28.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  79% 3.88G/4.90G [01:44<00:35, 28.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  79% 3.89G/4.90G [01:44<00:35, 28.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  80% 3.90G/4.90G [01:44<00:34, 28.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  80% 3.91G/4.90G [01:45<00:34, 28.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  80% 3.92G/4.90G [01:45<00:34, 28.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  80% 3.93G/4.90G [01:45<00:34, 28.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  80% 3.94G/4.90G [01:46<00:34, 27.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  81% 3.95G/4.90G [01:46<00:33, 28.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  81% 3.96G/4.90G [01:47<00:33, 28.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  81% 3.97G/4.90G [01:47<00:32, 28.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  81% 3.98G/4.90G [01:47<00:31, 28.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  82% 4.00G/4.90G [01:48<00:30, 29.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  82% 4.01G/4.90G [01:48<00:30, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  82% 4.02G/4.90G [01:48<00:29, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  82% 4.03G/4.90G [01:49<00:30, 28.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  82% 4.04G/4.90G [01:49<00:32, 26.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  83% 4.05G/4.90G [01:50<00:34, 24.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  83% 4.06G/4.90G [01:50<00:34, 24.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  83% 4.07G/4.90G [01:51<00:33, 24.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  83% 4.08G/4.90G [01:51<00:33, 24.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  83% 4.09G/4.90G [01:51<00:33, 24.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  84% 4.10G/4.90G [01:52<00:31, 25.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  84% 4.11G/4.90G [01:52<00:30, 25.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  84% 4.12G/4.90G [01:53<00:30, 25.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  84% 4.13G/4.90G [01:53<00:29, 26.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  85% 4.14G/4.90G [01:53<00:28, 26.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  85% 4.15G/4.90G [01:54<00:27, 27.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  85% 4.16G/4.90G [01:54<00:26, 27.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  85% 4.17G/4.90G [01:54<00:25, 28.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  85% 4.18G/4.90G [01:55<00:25, 28.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  86% 4.19G/4.90G [01:55<00:24, 28.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  86% 4.20G/4.90G [01:56<00:24, 28.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  86% 4.22G/4.90G [01:56<00:23, 28.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  86% 4.23G/4.90G [01:56<00:23, 28.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  86% 4.24G/4.90G [01:57<00:23, 28.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  87% 4.25G/4.90G [01:57<00:22, 28.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  87% 4.26G/4.90G [01:57<00:22, 28.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  87% 4.27G/4.90G [01:58<00:21, 28.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  87% 4.28G/4.90G [01:58<00:21, 29.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  88% 4.29G/4.90G [01:58<00:20, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  88% 4.30G/4.90G [01:59<00:19, 30.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  88% 4.31G/4.90G [01:59<00:19, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  88% 4.32G/4.90G [01:59<00:19, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  88% 4.33G/4.90G [02:00<00:19, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  89% 4.34G/4.90G [02:00<00:19, 29.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  89% 4.35G/4.90G [02:01<00:18, 29.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  89% 4.36G/4.90G [02:01<00:18, 29.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  89% 4.37G/4.90G [02:01<00:17, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  89% 4.38G/4.90G [02:02<00:16, 30.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  90% 4.39G/4.90G [02:02<00:16, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  90% 4.40G/4.90G [02:02<00:16, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  90% 4.41G/4.90G [02:03<00:16, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  90% 4.42G/4.90G [02:03<00:16, 29.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  91% 4.44G/4.90G [02:03<00:15, 29.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  91% 4.45G/4.90G [02:04<00:15, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  91% 4.46G/4.90G [02:04<00:14, 30.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  91% 4.47G/4.90G [02:04<00:14, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  91% 4.48G/4.90G [02:05<00:14, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  92% 4.49G/4.90G [02:05<00:13, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  92% 4.50G/4.90G [02:05<00:13, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  92% 4.51G/4.90G [02:06<00:12, 31.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  92% 4.52G/4.90G [02:06<00:12, 30.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  92% 4.53G/4.90G [02:06<00:12, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  93% 4.54G/4.90G [02:07<00:11, 31.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  93% 4.55G/4.90G [02:07<00:11, 31.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  93% 4.56G/4.90G [02:07<00:10, 31.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  93% 4.57G/4.90G [02:08<00:10, 32.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  94% 4.58G/4.90G [02:08<00:10, 31.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  94% 4.59G/4.90G [02:08<00:09, 32.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  94% 4.60G/4.90G [02:09<00:08, 33.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  94% 4.61G/4.90G [02:09<00:08, 32.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  94% 4.62G/4.90G [02:09<00:08, 34.1MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  95% 4.63G/4.90G [02:10<00:07, 34.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  95% 4.65G/4.90G [02:10<00:07, 33.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  95% 4.66G/4.90G [02:10<00:06, 34.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  95% 4.67G/4.90G [02:10<00:06, 35.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  95% 4.68G/4.90G [02:11<00:06, 36.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  96% 4.69G/4.90G [02:11<00:05, 37.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  96% 4.70G/4.90G [02:11<00:05, 37.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  96% 4.71G/4.90G [02:12<00:05, 37.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  96% 4.72G/4.90G [02:12<00:04, 37.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  97% 4.73G/4.90G [02:12<00:04, 38.3MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  97% 4.74G/4.90G [02:12<00:04, 39.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  97% 4.75G/4.90G [02:13<00:03, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  97% 4.76G/4.90G [02:13<00:03, 40.9MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  97% 4.77G/4.90G [02:13<00:03, 40.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  98% 4.78G/4.90G [02:13<00:02, 43.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  98% 4.79G/4.90G [02:14<00:02, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  98% 4.80G/4.90G [02:14<00:02, 45.4MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  98% 4.81G/4.90G [02:14<00:01, 43.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  98% 4.82G/4.90G [02:14<00:01, 46.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  99% 4.83G/4.90G [02:14<00:01, 44.6MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  99% 4.84G/4.90G [02:15<00:01, 47.2MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  99% 4.85G/4.90G [02:15<00:00, 49.5MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  99% 4.87G/4.90G [02:15<00:00, 39.7MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors: 100% 4.89G/4.90G [02:15<00:00, 52.0MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors: 100% 4.90G/4.90G [02:16<00:00, 36.0MB/s]\n",
            "Downloading shards:  20% 1/5 [02:16<09:06, 136.73s/it]\n",
            "model-00002-of-00005.safetensors:   0% 0.00/4.92G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   1% 31.5M/4.92G [00:00<00:18, 271MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   1% 62.9M/4.92G [00:00<00:17, 274MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   2% 94.4M/4.92G [00:00<00:17, 277MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   3% 126M/4.92G [00:00<00:17, 275MB/s] \u001b[A\n",
            "model-00002-of-00005.safetensors:   3% 157M/4.92G [00:00<00:17, 273MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   4% 189M/4.92G [00:00<00:17, 277MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   4% 220M/4.92G [00:00<00:16, 280MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   5% 252M/4.92G [00:00<00:16, 282MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   6% 283M/4.92G [00:01<00:16, 282MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   6% 315M/4.92G [00:01<00:16, 284MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   7% 346M/4.92G [00:01<00:16, 283MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   8% 377M/4.92G [00:01<00:16, 271MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   8% 409M/4.92G [00:01<00:16, 266MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   9% 440M/4.92G [00:01<00:16, 268MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  10% 472M/4.92G [00:01<00:16, 270MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  10% 503M/4.92G [00:01<00:16, 267MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  11% 535M/4.92G [00:01<00:16, 265MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  12% 566M/4.92G [00:02<00:16, 268MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  12% 598M/4.92G [00:02<00:18, 230MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  13% 629M/4.92G [00:02<00:17, 244MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  13% 661M/4.92G [00:02<00:16, 257MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  14% 692M/4.92G [00:02<00:15, 268MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  15% 724M/4.92G [00:02<00:15, 275MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  15% 755M/4.92G [00:02<00:14, 282MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  16% 786M/4.92G [00:02<00:14, 286MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  17% 818M/4.92G [00:03<00:14, 289MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  17% 849M/4.92G [00:03<00:14, 289MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  18% 881M/4.92G [00:03<00:13, 288MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  19% 912M/4.92G [00:03<00:13, 288MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  19% 944M/4.92G [00:03<00:13, 290MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  20% 975M/4.92G [00:03<00:13, 286MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  20% 1.01G/4.92G [00:03<00:13, 282MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  21% 1.04G/4.92G [00:03<00:13, 280MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  22% 1.07G/4.92G [00:03<00:13, 278MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  22% 1.10G/4.92G [00:04<00:14, 269MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  23% 1.13G/4.92G [00:04<00:14, 269MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  24% 1.16G/4.92G [00:04<00:13, 272MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  24% 1.20G/4.92G [00:04<00:13, 266MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  25% 1.23G/4.92G [00:04<00:14, 255MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  26% 1.26G/4.92G [00:04<00:14, 256MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  26% 1.29G/4.92G [00:04<00:13, 261MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  27% 1.32G/4.92G [00:04<00:13, 265MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  28% 1.35G/4.92G [00:04<00:13, 267MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  28% 1.38G/4.92G [00:05<00:12, 274MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  29% 1.42G/4.92G [00:05<00:12, 277MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  29% 1.45G/4.92G [00:05<00:12, 280MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  30% 1.48G/4.92G [00:05<00:12, 284MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  31% 1.51G/4.92G [00:05<00:11, 286MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  31% 1.54G/4.92G [00:05<00:11, 284MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  32% 1.57G/4.92G [00:05<00:11, 284MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  33% 1.60G/4.92G [00:05<00:11, 286MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  33% 1.64G/4.92G [00:05<00:11, 286MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  34% 1.67G/4.92G [00:06<00:11, 284MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  35% 1.70G/4.92G [00:06<00:12, 263MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  35% 1.73G/4.92G [00:06<00:12, 261MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  36% 1.76G/4.92G [00:06<00:12, 253MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  36% 1.79G/4.92G [00:06<00:12, 258MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  37% 1.82G/4.92G [00:06<00:11, 260MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  38% 1.86G/4.92G [00:06<00:11, 271MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  38% 1.89G/4.92G [00:06<00:11, 261MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  39% 1.92G/4.92G [00:07<00:11, 252MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  40% 1.95G/4.92G [00:07<00:12, 241MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  40% 1.98G/4.92G [00:07<00:11, 246MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  41% 2.01G/4.92G [00:07<00:11, 247MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  42% 2.04G/4.92G [00:07<00:11, 255MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  42% 2.08G/4.92G [00:07<00:10, 268MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  43% 2.11G/4.92G [00:07<00:10, 274MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  44% 2.14G/4.92G [00:07<00:10, 276MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  44% 2.17G/4.92G [00:08<00:09, 282MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  45% 2.20G/4.92G [00:08<00:09, 287MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  45% 2.23G/4.92G [00:08<00:09, 287MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  46% 2.26G/4.92G [00:08<00:09, 288MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  47% 2.30G/4.92G [00:08<00:09, 287MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  47% 2.33G/4.92G [00:08<00:09, 285MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  48% 2.36G/4.92G [00:08<00:08, 286MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  49% 2.39G/4.92G [00:08<00:08, 292MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  49% 2.42G/4.92G [00:08<00:08, 295MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  50% 2.45G/4.92G [00:08<00:08, 285MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  51% 2.49G/4.92G [00:09<00:08, 273MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  51% 2.52G/4.92G [00:09<00:08, 274MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  52% 2.55G/4.92G [00:09<00:08, 278MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  52% 2.58G/4.92G [00:09<00:08, 282MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  53% 2.61G/4.92G [00:09<00:08, 281MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  54% 2.64G/4.92G [00:09<00:08, 277MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  54% 2.67G/4.92G [00:09<00:08, 269MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  55% 2.71G/4.92G [00:09<00:08, 270MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  56% 2.74G/4.92G [00:10<00:08, 267MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  56% 2.77G/4.92G [00:10<00:08, 265MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  57% 2.80G/4.92G [00:10<00:07, 265MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  58% 2.83G/4.92G [00:10<00:07, 267MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  58% 2.86G/4.92G [00:10<00:07, 269MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  59% 2.89G/4.92G [00:10<00:07, 270MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  60% 2.93G/4.92G [00:10<00:07, 274MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  60% 2.96G/4.92G [00:10<00:07, 275MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  61% 2.99G/4.92G [00:10<00:07, 273MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  61% 3.02G/4.92G [00:11<00:06, 275MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  62% 3.05G/4.92G [00:11<00:06, 274MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  63% 3.08G/4.92G [00:11<00:06, 275MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  63% 3.11G/4.92G [00:11<00:06, 277MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  64% 3.16G/4.92G [00:11<00:06, 285MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  65% 3.19G/4.92G [00:11<00:06, 287MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  65% 3.22G/4.92G [00:11<00:05, 285MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  66% 3.25G/4.92G [00:11<00:05, 286MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  67% 3.28G/4.92G [00:11<00:05, 290MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  67% 3.31G/4.92G [00:12<00:05, 294MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  68% 3.36G/4.92G [00:12<00:05, 300MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  69% 3.39G/4.92G [00:12<00:05, 298MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  70% 3.42G/4.92G [00:12<00:04, 300MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  70% 3.45G/4.92G [00:12<00:04, 297MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  71% 3.48G/4.92G [00:12<00:04, 291MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  71% 3.51G/4.92G [00:12<00:04, 291MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  72% 3.54G/4.92G [00:12<00:04, 286MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  73% 3.59G/4.92G [00:13<00:04, 297MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  74% 3.62G/4.92G [00:13<00:04, 296MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  74% 3.65G/4.92G [00:13<00:04, 298MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  75% 3.68G/4.92G [00:13<00:04, 290MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  76% 3.71G/4.92G [00:13<00:04, 280MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  76% 3.74G/4.92G [00:13<00:04, 278MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  77% 3.77G/4.92G [00:13<00:04, 280MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  77% 3.81G/4.92G [00:13<00:04, 272MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  78% 3.84G/4.92G [00:13<00:03, 281MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  79% 3.88G/4.92G [00:14<00:03, 293MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  80% 3.92G/4.92G [00:14<00:03, 307MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  81% 3.96G/4.92G [00:14<00:02, 321MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  81% 4.01G/4.92G [00:14<00:02, 326MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  82% 4.05G/4.92G [00:14<00:02, 326MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  83% 4.09G/4.92G [00:14<00:02, 320MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  84% 4.13G/4.92G [00:14<00:02, 314MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  85% 4.16G/4.92G [00:14<00:02, 309MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  86% 4.20G/4.92G [00:15<00:02, 309MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  86% 4.25G/4.92G [00:15<00:02, 316MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  87% 4.29G/4.92G [00:15<00:02, 305MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  88% 4.32G/4.92G [00:15<00:01, 300MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  89% 4.36G/4.92G [00:15<00:01, 303MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  90% 4.40G/4.92G [00:15<00:01, 309MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  90% 4.44G/4.92G [00:15<00:01, 308MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  91% 4.47G/4.92G [00:15<00:01, 306MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  92% 4.51G/4.92G [00:16<00:01, 312MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  93% 4.55G/4.92G [00:16<00:01, 310MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  93% 4.58G/4.92G [00:16<00:01, 305MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  94% 4.61G/4.92G [00:16<00:00, 303MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  94% 4.65G/4.92G [00:16<00:00, 303MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  95% 4.69G/4.92G [00:16<00:00, 307MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  96% 4.72G/4.92G [00:16<00:00, 304MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  97% 4.75G/4.92G [00:16<00:00, 298MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  97% 4.79G/4.92G [00:16<00:00, 302MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  98% 4.82G/4.92G [00:17<00:00, 303MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  99% 4.87G/4.92G [00:17<00:00, 205MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors: 100% 4.92G/4.92G [00:17<00:00, 280MB/s]\n",
            "Downloading shards:  40% 2/5 [02:34<03:20, 66.77s/it] \n",
            "model-00003-of-00005.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   0% 10.5M/5.00G [00:00<03:47, 21.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   0% 21.0M/5.00G [00:00<02:22, 35.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   1% 31.5M/5.00G [00:00<01:55, 43.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   1% 41.9M/5.00G [00:01<01:44, 47.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   1% 52.4M/5.00G [00:01<01:37, 50.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   1% 62.9M/5.00G [00:01<01:33, 52.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   1% 73.4M/5.00G [00:01<01:30, 54.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   2% 83.9M/5.00G [00:01<01:38, 49.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   2% 94.4M/5.00G [00:01<01:34, 51.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   2% 105M/5.00G [00:02<01:31, 53.5MB/s] \u001b[A\n",
            "model-00003-of-00005.safetensors:   2% 115M/5.00G [00:02<01:29, 54.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   3% 126M/5.00G [00:02<01:27, 55.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   3% 136M/5.00G [00:02<01:29, 54.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   3% 147M/5.00G [00:02<01:28, 54.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   3% 157M/5.00G [00:03<01:27, 55.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   3% 168M/5.00G [00:03<01:27, 55.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   4% 178M/5.00G [00:03<01:38, 49.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   4% 189M/5.00G [00:03<01:34, 51.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   4% 199M/5.00G [00:03<01:30, 53.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   4% 210M/5.00G [00:04<01:29, 53.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   4% 220M/5.00G [00:04<01:26, 55.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   5% 231M/5.00G [00:04<01:25, 55.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   5% 241M/5.00G [00:04<01:24, 56.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   5% 252M/5.00G [00:04<01:23, 56.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   5% 262M/5.00G [00:05<01:23, 56.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   5% 273M/5.00G [00:05<01:22, 57.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   6% 283M/5.00G [00:05<01:22, 57.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   6% 294M/5.00G [00:05<01:23, 56.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   6% 304M/5.00G [00:05<01:22, 56.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   6% 315M/5.00G [00:05<01:22, 57.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   7% 325M/5.00G [00:06<01:21, 57.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   7% 336M/5.00G [00:06<01:21, 57.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   7% 346M/5.00G [00:06<01:21, 57.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   7% 357M/5.00G [00:06<01:21, 57.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   7% 367M/5.00G [00:06<01:27, 53.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   8% 377M/5.00G [00:07<01:25, 54.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   8% 388M/5.00G [00:07<01:24, 54.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   8% 398M/5.00G [00:07<01:22, 55.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   8% 409M/5.00G [00:07<01:22, 56.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   8% 419M/5.00G [00:07<01:21, 56.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   9% 430M/5.00G [00:08<01:20, 56.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   9% 440M/5.00G [00:08<01:20, 56.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   9% 451M/5.00G [00:08<01:19, 57.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   9% 461M/5.00G [00:08<01:20, 56.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   9% 472M/5.00G [00:08<01:18, 57.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  10% 482M/5.00G [00:08<01:18, 57.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  10% 493M/5.00G [00:09<01:18, 57.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  10% 503M/5.00G [00:09<01:18, 57.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  10% 514M/5.00G [00:09<01:18, 57.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  10% 524M/5.00G [00:09<01:18, 57.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  11% 535M/5.00G [00:09<01:17, 57.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  11% 545M/5.00G [00:10<01:17, 57.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  11% 556M/5.00G [00:10<01:18, 56.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  11% 566M/5.00G [00:10<01:17, 57.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  12% 577M/5.00G [00:10<01:16, 57.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  12% 587M/5.00G [00:10<01:16, 57.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  12% 598M/5.00G [00:10<01:15, 58.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  12% 608M/5.00G [00:11<01:15, 57.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  12% 619M/5.00G [00:11<01:15, 57.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  13% 629M/5.00G [00:11<01:15, 57.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  13% 640M/5.00G [00:11<01:15, 57.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  13% 650M/5.00G [00:11<01:15, 57.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  13% 661M/5.00G [00:12<01:15, 57.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  13% 671M/5.00G [00:12<01:15, 57.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  14% 682M/5.00G [00:12<01:14, 57.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  14% 692M/5.00G [00:12<01:14, 57.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  14% 703M/5.00G [00:12<01:24, 51.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  14% 713M/5.00G [00:13<01:31, 46.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  14% 724M/5.00G [00:13<01:36, 44.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  15% 734M/5.00G [00:13<01:40, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  15% 744M/5.00G [00:13<01:33, 45.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  15% 755M/5.00G [00:14<01:38, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  15% 765M/5.00G [00:14<01:40, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  16% 776M/5.00G [00:14<01:34, 44.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  16% 786M/5.00G [00:14<01:37, 43.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  16% 797M/5.00G [00:15<01:31, 46.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  16% 807M/5.00G [00:15<01:35, 43.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  16% 818M/5.00G [00:15<01:37, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  17% 828M/5.00G [00:15<01:31, 45.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  17% 839M/5.00G [00:16<01:35, 43.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  17% 849M/5.00G [00:16<01:29, 46.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  17% 860M/5.00G [00:16<01:33, 44.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  17% 870M/5.00G [00:16<01:28, 46.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  18% 881M/5.00G [00:16<01:31, 44.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  18% 891M/5.00G [00:17<01:27, 47.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  18% 902M/5.00G [00:17<01:29, 45.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  18% 912M/5.00G [00:17<01:27, 46.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  18% 923M/5.00G [00:17<01:22, 49.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  19% 933M/5.00G [00:18<01:28, 46.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  19% 944M/5.00G [00:18<01:23, 48.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  19% 954M/5.00G [00:18<01:27, 46.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  19% 965M/5.00G [00:18<01:23, 48.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  20% 975M/5.00G [00:18<01:20, 50.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  20% 986M/5.00G [00:19<01:26, 46.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  20% 996M/5.00G [00:19<01:21, 48.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  20% 1.01G/5.00G [00:19<01:26, 46.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  20% 1.02G/5.00G [00:19<01:22, 48.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  21% 1.03G/5.00G [00:19<01:18, 50.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  21% 1.04G/5.00G [00:20<01:24, 46.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  21% 1.05G/5.00G [00:20<01:21, 48.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  21% 1.06G/5.00G [00:20<01:18, 50.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  21% 1.07G/5.00G [00:20<01:23, 47.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  22% 1.08G/5.00G [00:21<01:22, 47.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  22% 1.09G/5.00G [00:21<01:22, 47.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  22% 1.10G/5.00G [00:21<01:19, 48.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  22% 1.11G/5.00G [00:21<01:16, 50.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  22% 1.12G/5.00G [00:21<01:22, 47.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  23% 1.13G/5.00G [00:22<01:18, 49.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  23% 1.14G/5.00G [00:22<01:16, 50.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  23% 1.15G/5.00G [00:22<01:21, 47.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  23% 1.16G/5.00G [00:22<01:17, 49.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  23% 1.17G/5.00G [00:22<01:15, 50.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  24% 1.18G/5.00G [00:23<01:20, 47.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  24% 1.20G/5.00G [00:23<01:16, 49.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  24% 1.21G/5.00G [00:23<01:21, 46.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  24% 1.22G/5.00G [00:23<01:18, 47.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  25% 1.23G/5.00G [00:24<01:16, 49.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  25% 1.24G/5.00G [00:24<01:19, 47.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  25% 1.25G/5.00G [00:24<01:16, 48.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  25% 1.26G/5.00G [00:24<01:14, 50.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  25% 1.27G/5.00G [00:24<01:17, 48.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  26% 1.28G/5.00G [00:25<01:16, 48.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  26% 1.29G/5.00G [00:25<01:14, 49.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  26% 1.30G/5.00G [00:25<01:17, 47.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  26% 1.31G/5.00G [00:25<01:14, 49.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  26% 1.32G/5.00G [00:26<01:16, 47.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  27% 1.33G/5.00G [00:26<01:16, 47.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  27% 1.34G/5.00G [00:26<01:13, 50.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  27% 1.35G/5.00G [00:26<01:15, 48.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  27% 1.36G/5.00G [00:26<01:15, 48.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  27% 1.37G/5.00G [00:27<01:12, 50.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  28% 1.38G/5.00G [00:27<01:14, 48.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  28% 1.39G/5.00G [00:27<01:14, 48.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  28% 1.41G/5.00G [00:27<01:11, 50.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  28% 1.42G/5.00G [00:27<01:14, 48.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  29% 1.43G/5.00G [00:28<01:12, 49.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  29% 1.44G/5.00G [00:28<01:10, 50.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  29% 1.45G/5.00G [00:28<01:13, 48.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  29% 1.46G/5.00G [00:28<01:11, 49.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  29% 1.47G/5.00G [00:28<01:09, 50.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  30% 1.48G/5.00G [00:29<01:12, 48.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  30% 1.49G/5.00G [00:29<01:11, 49.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  30% 1.50G/5.00G [00:29<01:09, 50.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  30% 1.51G/5.00G [00:29<01:11, 49.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  30% 1.52G/5.00G [00:30<01:10, 49.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  31% 1.53G/5.00G [00:30<01:08, 50.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  31% 1.54G/5.00G [00:30<01:06, 52.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  31% 1.55G/5.00G [00:30<01:09, 49.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  31% 1.56G/5.00G [00:30<01:08, 50.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  31% 1.57G/5.00G [00:31<01:09, 49.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  32% 1.58G/5.00G [00:31<01:06, 51.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  32% 1.59G/5.00G [00:31<01:06, 51.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  32% 1.60G/5.00G [00:31<01:07, 50.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  32% 1.61G/5.00G [00:31<01:05, 52.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  33% 1.63G/5.00G [00:32<01:05, 51.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  33% 1.64G/5.00G [00:32<01:04, 51.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  33% 1.65G/5.00G [00:32<01:12, 46.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  33% 1.66G/5.00G [00:32<01:16, 43.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  33% 1.67G/5.00G [00:33<01:28, 37.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  34% 1.68G/5.00G [00:33<01:43, 32.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  34% 1.69G/5.00G [00:34<01:47, 30.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  34% 1.70G/5.00G [00:34<01:49, 30.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  34% 1.71G/5.00G [00:34<01:50, 29.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  34% 1.72G/5.00G [00:35<01:51, 29.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  35% 1.73G/5.00G [00:35<01:51, 29.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  35% 1.74G/5.00G [00:35<01:51, 29.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  35% 1.75G/5.00G [00:36<01:44, 31.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  35% 1.76G/5.00G [00:36<01:46, 30.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  35% 1.77G/5.00G [00:36<01:47, 30.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  36% 1.78G/5.00G [00:37<01:48, 29.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  36% 1.79G/5.00G [00:37<01:41, 31.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  36% 1.80G/5.00G [00:37<01:43, 30.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  36% 1.81G/5.00G [00:38<01:53, 28.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  36% 1.82G/5.00G [00:38<02:00, 26.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  37% 1.84G/5.00G [00:39<02:05, 25.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  37% 1.85G/5.00G [00:39<02:08, 24.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  37% 1.86G/5.00G [00:40<02:10, 24.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  37% 1.87G/5.00G [00:40<02:03, 25.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  38% 1.88G/5.00G [00:40<02:06, 24.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  38% 1.89G/5.00G [00:41<02:08, 24.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  38% 1.90G/5.00G [00:41<02:02, 25.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  38% 1.91G/5.00G [00:42<02:04, 24.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  38% 1.92G/5.00G [00:42<01:59, 25.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  39% 1.93G/5.00G [00:42<02:02, 25.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  39% 1.94G/5.00G [00:43<01:58, 25.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  39% 1.95G/5.00G [00:43<01:54, 26.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  39% 1.96G/5.00G [00:44<01:58, 25.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  39% 1.97G/5.00G [00:44<01:54, 26.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  40% 1.98G/5.00G [00:44<01:58, 25.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  40% 1.99G/5.00G [00:45<01:54, 26.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  40% 2.00G/5.00G [00:45<01:51, 26.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  40% 2.01G/5.00G [00:46<01:55, 25.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  40% 2.02G/5.00G [00:46<01:51, 26.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  41% 2.03G/5.00G [00:46<01:49, 27.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  41% 2.04G/5.00G [00:47<01:53, 26.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  41% 2.06G/5.00G [00:47<01:50, 26.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  41% 2.07G/5.00G [00:48<01:48, 27.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  42% 2.08G/5.00G [00:48<01:52, 26.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  42% 2.09G/5.00G [00:48<01:49, 26.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  42% 2.10G/5.00G [00:49<01:47, 27.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  42% 2.11G/5.00G [00:49<01:50, 26.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  42% 2.12G/5.00G [00:50<01:47, 26.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  43% 2.13G/5.00G [00:50<01:49, 26.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  43% 2.14G/5.00G [00:50<01:48, 26.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  43% 2.15G/5.00G [00:51<01:45, 27.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  43% 2.16G/5.00G [00:51<01:44, 27.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  43% 2.17G/5.00G [00:52<01:47, 26.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  44% 2.18G/5.00G [00:52<01:44, 26.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  44% 2.19G/5.00G [00:52<01:42, 27.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  44% 2.20G/5.00G [00:53<01:40, 27.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  44% 2.21G/5.00G [00:53<01:39, 27.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  44% 2.22G/5.00G [00:53<01:38, 28.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  45% 2.23G/5.00G [00:54<01:37, 28.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  45% 2.24G/5.00G [00:54<01:36, 28.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  45% 2.25G/5.00G [00:54<01:35, 28.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  45% 2.26G/5.00G [00:55<01:34, 28.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  46% 2.28G/5.00G [00:55<01:34, 28.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  46% 2.29G/5.00G [00:56<01:33, 28.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  46% 2.30G/5.00G [00:56<01:27, 30.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  46% 2.31G/5.00G [00:56<01:28, 30.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  46% 2.32G/5.00G [00:57<01:28, 30.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  47% 2.33G/5.00G [00:57<01:23, 32.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  47% 2.34G/5.00G [00:57<01:21, 32.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  47% 2.35G/5.00G [00:57<01:21, 32.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  47% 2.36G/5.00G [00:58<01:17, 34.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  47% 2.37G/5.00G [00:58<01:14, 35.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  48% 2.38G/5.00G [00:58<01:16, 34.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  48% 2.39G/5.00G [00:59<01:15, 34.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  48% 2.40G/5.00G [00:59<01:12, 35.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  48% 2.41G/5.00G [00:59<01:10, 36.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  48% 2.42G/5.00G [00:59<01:09, 37.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  49% 2.43G/5.00G [01:00<01:02, 40.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  49% 2.44G/5.00G [01:00<01:03, 40.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  49% 2.45G/5.00G [01:00<01:03, 39.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  49% 2.46G/5.00G [01:00<01:03, 39.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  49% 2.47G/5.00G [01:01<00:58, 43.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  50% 2.49G/5.00G [01:01<00:59, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  50% 2.50G/5.00G [01:01<00:55, 45.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  50% 2.51G/5.00G [01:01<00:57, 43.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  50% 2.52G/5.00G [01:02<00:53, 46.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  51% 2.53G/5.00G [01:02<00:55, 44.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  51% 2.54G/5.00G [01:02<00:52, 47.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  51% 2.55G/5.00G [01:02<00:49, 49.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  51% 2.56G/5.00G [01:02<00:52, 46.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  51% 2.57G/5.00G [01:03<00:49, 48.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  52% 2.58G/5.00G [01:03<00:47, 50.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  52% 2.59G/5.00G [01:03<00:45, 52.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  52% 2.60G/5.00G [01:03<00:44, 54.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  52% 2.61G/5.00G [01:03<00:43, 55.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  52% 2.62G/5.00G [01:04<00:42, 55.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  53% 2.63G/5.00G [01:04<00:41, 56.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  53% 2.64G/5.00G [01:04<00:41, 56.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  53% 2.65G/5.00G [01:04<00:41, 57.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  53% 2.66G/5.00G [01:04<00:40, 57.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  53% 2.67G/5.00G [01:04<00:40, 57.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  54% 2.68G/5.00G [01:05<00:40, 57.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  54% 2.69G/5.00G [01:05<00:40, 56.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  54% 2.71G/5.00G [01:05<00:40, 56.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  54% 2.72G/5.00G [01:05<00:40, 55.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  55% 2.73G/5.00G [01:05<00:40, 56.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  55% 2.74G/5.00G [01:06<00:40, 56.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  55% 2.75G/5.00G [01:06<00:39, 56.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  55% 2.76G/5.00G [01:06<00:39, 56.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  55% 2.77G/5.00G [01:06<00:40, 55.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  56% 2.78G/5.00G [01:06<00:40, 55.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  56% 2.79G/5.00G [01:07<00:40, 54.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  56% 2.80G/5.00G [01:07<00:39, 55.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  56% 2.81G/5.00G [01:07<00:38, 56.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  56% 2.82G/5.00G [01:07<00:38, 56.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  57% 2.83G/5.00G [01:07<00:38, 57.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  57% 2.84G/5.00G [01:07<00:37, 57.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  57% 2.85G/5.00G [01:08<00:37, 57.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  57% 2.86G/5.00G [01:08<00:37, 57.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  57% 2.87G/5.00G [01:08<00:37, 57.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  58% 2.88G/5.00G [01:08<00:36, 57.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  58% 2.89G/5.00G [01:08<00:36, 57.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  58% 2.90G/5.00G [01:09<00:36, 57.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  58% 2.92G/5.00G [01:09<00:36, 57.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  59% 2.93G/5.00G [01:09<00:36, 57.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  59% 2.94G/5.00G [01:09<00:35, 57.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  59% 2.95G/5.00G [01:09<00:39, 51.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  59% 2.96G/5.00G [01:09<00:34, 59.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  59% 2.97G/5.00G [01:10<00:34, 58.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  60% 2.98G/5.00G [01:10<00:34, 58.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  60% 2.99G/5.00G [01:10<00:34, 58.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  60% 3.00G/5.00G [01:10<00:34, 58.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  60% 3.01G/5.00G [01:10<00:34, 57.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  60% 3.02G/5.00G [01:11<00:34, 57.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  61% 3.03G/5.00G [01:11<00:34, 57.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  61% 3.04G/5.00G [01:11<00:34, 57.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  61% 3.05G/5.00G [01:11<00:33, 57.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  61% 3.06G/5.00G [01:11<00:33, 57.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  61% 3.07G/5.00G [01:11<00:33, 57.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  62% 3.08G/5.00G [01:12<00:33, 57.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  62% 3.09G/5.00G [01:12<00:33, 56.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  62% 3.10G/5.00G [01:12<00:33, 56.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  62% 3.11G/5.00G [01:12<00:33, 56.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  62% 3.12G/5.00G [01:12<00:32, 56.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  63% 3.14G/5.00G [01:13<00:34, 53.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  63% 3.15G/5.00G [01:13<00:31, 58.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  63% 3.16G/5.00G [01:13<00:31, 58.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  63% 3.17G/5.00G [01:13<00:31, 57.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  64% 3.18G/5.00G [01:13<00:31, 57.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  64% 3.19G/5.00G [01:14<00:32, 56.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  64% 3.20G/5.00G [01:14<00:31, 56.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  64% 3.21G/5.00G [01:14<00:32, 55.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  64% 3.22G/5.00G [01:14<00:32, 55.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  65% 3.23G/5.00G [01:14<00:31, 55.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  65% 3.24G/5.00G [01:14<00:31, 56.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  65% 3.25G/5.00G [01:15<00:30, 56.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  65% 3.26G/5.00G [01:15<00:30, 56.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  65% 3.27G/5.00G [01:15<00:30, 56.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  66% 3.28G/5.00G [01:15<00:30, 57.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  66% 3.29G/5.00G [01:15<00:29, 57.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  66% 3.30G/5.00G [01:16<00:29, 57.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  66% 3.31G/5.00G [01:16<00:29, 57.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  66% 3.32G/5.00G [01:16<00:29, 57.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  67% 3.33G/5.00G [01:16<00:28, 57.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  67% 3.34G/5.00G [01:16<00:28, 57.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  67% 3.36G/5.00G [01:16<00:28, 57.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  67% 3.37G/5.00G [01:17<00:28, 57.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  68% 3.38G/5.00G [01:17<00:28, 57.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  68% 3.39G/5.00G [01:17<00:28, 57.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  68% 3.40G/5.00G [01:17<00:27, 57.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  68% 3.41G/5.00G [01:17<00:27, 57.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  68% 3.42G/5.00G [01:18<00:27, 57.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  69% 3.43G/5.00G [01:18<00:27, 57.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  69% 3.44G/5.00G [01:18<00:27, 56.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  69% 3.45G/5.00G [01:18<00:27, 57.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  69% 3.46G/5.00G [01:18<00:27, 56.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  69% 3.47G/5.00G [01:19<00:29, 51.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  70% 3.48G/5.00G [01:19<00:32, 46.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  70% 3.49G/5.00G [01:19<00:34, 43.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  70% 3.50G/5.00G [01:19<00:32, 46.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  70% 3.51G/5.00G [01:20<00:33, 44.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  70% 3.52G/5.00G [01:20<00:34, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  71% 3.53G/5.00G [01:20<00:32, 45.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  71% 3.54G/5.00G [01:20<00:33, 43.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  71% 3.55G/5.00G [01:20<00:31, 46.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  71% 3.57G/5.00G [01:21<00:32, 44.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  72% 3.58G/5.00G [01:21<00:30, 47.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  72% 3.59G/5.00G [01:21<00:31, 45.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  72% 3.60G/5.00G [01:21<00:29, 47.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  72% 3.61G/5.00G [01:22<00:28, 49.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  72% 3.62G/5.00G [01:22<00:29, 46.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  73% 3.63G/5.00G [01:22<00:28, 48.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  73% 3.64G/5.00G [01:22<00:26, 50.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  73% 3.65G/5.00G [01:22<00:25, 52.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  73% 3.66G/5.00G [01:23<00:27, 48.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  73% 3.67G/5.00G [01:23<00:26, 50.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  74% 3.68G/5.00G [01:23<00:25, 51.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  74% 3.69G/5.00G [01:23<00:26, 48.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  74% 3.70G/5.00G [01:23<00:25, 50.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  74% 3.71G/5.00G [01:24<00:24, 51.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  74% 3.72G/5.00G [01:24<00:24, 52.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  75% 3.73G/5.00G [01:24<00:23, 53.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  75% 3.74G/5.00G [01:24<00:22, 54.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  75% 3.75G/5.00G [01:24<00:22, 55.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  75% 3.76G/5.00G [01:25<00:23, 52.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  76% 3.77G/5.00G [01:25<00:23, 51.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  76% 3.79G/5.00G [01:25<00:22, 53.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  76% 3.80G/5.00G [01:25<00:22, 54.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  76% 3.81G/5.00G [01:25<00:21, 54.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  76% 3.82G/5.00G [01:26<00:21, 55.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  77% 3.83G/5.00G [01:26<00:21, 54.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  77% 3.84G/5.00G [01:26<00:21, 53.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  77% 3.85G/5.00G [01:26<00:21, 54.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  77% 3.86G/5.00G [01:26<00:20, 55.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  77% 3.87G/5.00G [01:27<00:20, 55.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  78% 3.88G/5.00G [01:27<00:19, 56.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  78% 3.89G/5.00G [01:27<00:19, 56.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  78% 3.90G/5.00G [01:27<00:19, 57.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  78% 3.91G/5.00G [01:27<00:19, 57.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  78% 3.92G/5.00G [01:27<00:18, 56.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  79% 3.93G/5.00G [01:28<00:19, 55.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  79% 3.94G/5.00G [01:28<00:18, 56.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  79% 3.95G/5.00G [01:28<00:18, 56.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  79% 3.96G/5.00G [01:28<00:18, 56.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  79% 3.97G/5.00G [01:28<00:17, 57.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  80% 3.98G/5.00G [01:29<00:17, 57.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  80% 4.00G/5.00G [01:29<00:17, 57.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  80% 4.01G/5.00G [01:29<00:17, 57.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  80% 4.02G/5.00G [01:29<00:17, 56.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  81% 4.03G/5.00G [01:29<00:17, 56.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  81% 4.04G/5.00G [01:29<00:16, 56.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  81% 4.05G/5.00G [01:30<00:16, 57.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  81% 4.06G/5.00G [01:30<00:16, 57.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  81% 4.07G/5.00G [01:30<00:16, 57.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  82% 4.08G/5.00G [01:30<00:16, 57.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  82% 4.09G/5.00G [01:30<00:15, 57.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  82% 4.10G/5.00G [01:31<00:15, 57.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  82% 4.11G/5.00G [01:31<00:15, 57.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  82% 4.12G/5.00G [01:31<00:15, 56.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  83% 4.13G/5.00G [01:31<00:15, 57.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  83% 4.14G/5.00G [01:31<00:14, 57.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  83% 4.15G/5.00G [01:31<00:15, 55.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  83% 4.16G/5.00G [01:32<00:14, 56.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  83% 4.17G/5.00G [01:32<00:14, 56.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  84% 4.18G/5.00G [01:32<00:14, 56.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  84% 4.19G/5.00G [01:32<00:14, 56.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  84% 4.20G/5.00G [01:32<00:13, 56.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  84% 4.22G/5.00G [01:33<00:13, 56.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  85% 4.23G/5.00G [01:33<00:13, 56.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  85% 4.24G/5.00G [01:33<00:13, 56.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  85% 4.25G/5.00G [01:33<00:13, 57.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  85% 4.26G/5.00G [01:33<00:12, 57.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  85% 4.27G/5.00G [01:34<00:12, 57.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  86% 4.28G/5.00G [01:34<00:12, 57.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  86% 4.29G/5.00G [01:34<00:12, 57.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  86% 4.30G/5.00G [01:34<00:12, 54.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  86% 4.31G/5.00G [01:34<00:12, 57.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  86% 4.32G/5.00G [01:34<00:11, 58.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  87% 4.33G/5.00G [01:35<00:11, 57.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  87% 4.34G/5.00G [01:35<00:11, 57.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  87% 4.35G/5.00G [01:35<00:11, 57.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  87% 4.36G/5.00G [01:35<00:11, 57.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  87% 4.37G/5.00G [01:35<00:11, 56.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  88% 4.38G/5.00G [01:36<00:11, 52.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  88% 4.39G/5.00G [01:36<00:12, 47.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  88% 4.40G/5.00G [01:36<00:13, 44.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  88% 4.41G/5.00G [01:36<00:13, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  89% 4.42G/5.00G [01:37<00:12, 45.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  89% 4.44G/5.00G [01:37<00:13, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  89% 4.45G/5.00G [01:37<00:14, 37.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  89% 4.46G/5.00G [01:38<00:15, 35.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  89% 4.47G/5.00G [01:38<00:15, 35.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  90% 4.48G/5.00G [01:38<00:15, 33.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  90% 4.49G/5.00G [01:39<00:14, 34.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  90% 4.50G/5.00G [01:39<00:15, 32.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  90% 4.51G/5.00G [01:39<00:14, 34.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  90% 4.52G/5.00G [01:39<00:14, 32.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  91% 4.53G/5.00G [01:40<00:13, 34.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  91% 4.54G/5.00G [01:40<00:13, 35.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  91% 4.55G/5.00G [01:40<00:13, 33.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  91% 4.56G/5.00G [01:41<00:12, 34.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  91% 4.57G/5.00G [01:41<00:12, 35.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  92% 4.58G/5.00G [01:41<00:12, 33.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  92% 4.59G/5.00G [01:42<00:11, 34.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  92% 4.60G/5.00G [01:42<00:11, 35.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  92% 4.61G/5.00G [01:42<00:10, 36.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  92% 4.62G/5.00G [01:42<00:10, 36.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  93% 4.63G/5.00G [01:43<00:10, 34.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  93% 4.65G/5.00G [01:43<00:09, 35.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  93% 4.66G/5.00G [01:43<00:09, 36.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  93% 4.67G/5.00G [01:44<00:09, 36.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  94% 4.68G/5.00G [01:44<00:08, 37.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  94% 4.69G/5.00G [01:44<00:08, 36.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  94% 4.70G/5.00G [01:44<00:08, 35.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  94% 4.71G/5.00G [01:45<00:08, 34.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  94% 4.72G/5.00G [01:45<00:07, 37.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  95% 4.73G/5.00G [01:45<00:08, 32.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  95% 4.74G/5.00G [01:46<00:06, 38.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  95% 4.75G/5.00G [01:46<00:06, 38.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  95% 4.76G/5.00G [01:46<00:06, 36.3MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  95% 4.77G/5.00G [01:46<00:06, 36.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  96% 4.78G/5.00G [01:47<00:05, 37.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  96% 4.79G/5.00G [01:47<00:05, 37.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  96% 4.80G/5.00G [01:47<00:05, 37.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  96% 4.81G/5.00G [01:48<00:04, 37.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  96% 4.82G/5.00G [01:48<00:04, 36.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  97% 4.83G/5.00G [01:48<00:04, 35.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  97% 4.84G/5.00G [01:49<00:04, 33.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  97% 4.85G/5.00G [01:49<00:04, 31.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  97% 4.87G/5.00G [01:49<00:04, 30.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  98% 4.88G/5.00G [01:50<00:04, 29.8MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  98% 4.89G/5.00G [01:50<00:03, 29.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  98% 4.90G/5.00G [01:50<00:03, 29.7MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  98% 4.91G/5.00G [01:51<00:03, 29.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  98% 4.92G/5.00G [01:51<00:02, 30.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  99% 4.93G/5.00G [01:51<00:02, 30.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  99% 4.94G/5.00G [01:52<00:01, 30.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  99% 4.95G/5.00G [01:52<00:01, 32.1MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  99% 4.96G/5.00G [01:52<00:01, 31.6MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  99% 4.97G/5.00G [01:53<00:00, 33.0MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors: 100% 4.98G/5.00G [01:53<00:00, 33.4MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors: 100% 4.99G/5.00G [01:53<00:00, 33.2MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors: 100% 5.00G/5.00G [01:54<00:00, 43.8MB/s]\n",
            "Downloading shards:  60% 3/5 [04:28<02:56, 88.46s/it]\n",
            "model-00004-of-00005.safetensors:   0% 0.00/4.92G [00:00<?, ?B/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:   1% 41.9M/4.92G [00:00<00:13, 368MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:   2% 83.9M/4.92G [00:00<00:12, 391MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:   3% 126M/4.92G [00:00<00:11, 402MB/s] \u001b[A\n",
            "model-00004-of-00005.safetensors:   3% 168M/4.92G [00:00<00:11, 403MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:   4% 210M/4.92G [00:00<00:11, 402MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:   5% 252M/4.92G [00:00<00:11, 404MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:   6% 294M/4.92G [00:00<00:11, 406MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:   7% 336M/4.92G [00:00<00:11, 405MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:   8% 377M/4.92G [00:00<00:11, 399MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:   9% 419M/4.92G [00:01<00:11, 399MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:   9% 461M/4.92G [00:01<00:11, 389MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  10% 503M/4.92G [00:01<00:11, 376MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  11% 545M/4.92G [00:01<00:11, 368MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  12% 587M/4.92G [00:01<00:11, 367MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  13% 629M/4.92G [00:01<00:11, 364MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  14% 671M/4.92G [00:01<00:11, 360MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  15% 713M/4.92G [00:01<00:11, 357MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  15% 755M/4.92G [00:02<00:11, 348MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  16% 797M/4.92G [00:02<00:11, 349MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  17% 839M/4.92G [00:02<00:11, 340MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  18% 881M/4.92G [00:02<00:11, 350MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  19% 923M/4.92G [00:02<00:11, 361MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  20% 965M/4.92G [00:02<00:10, 374MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  20% 1.01G/4.92G [00:02<00:10, 384MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  22% 1.06G/4.92G [00:02<00:09, 397MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  22% 1.10G/4.92G [00:02<00:09, 396MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  23% 1.14G/4.92G [00:03<00:09, 396MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  24% 1.18G/4.92G [00:03<00:09, 396MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  25% 1.23G/4.92G [00:03<00:09, 398MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  26% 1.27G/4.92G [00:03<00:09, 385MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  27% 1.31G/4.92G [00:03<00:09, 393MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  28% 1.35G/4.92G [00:03<00:08, 396MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  28% 1.39G/4.92G [00:03<00:08, 399MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  29% 1.44G/4.92G [00:03<00:08, 402MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  30% 1.48G/4.92G [00:03<00:08, 399MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  31% 1.52G/4.92G [00:03<00:08, 391MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  32% 1.56G/4.92G [00:04<00:08, 374MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  33% 1.60G/4.92G [00:04<00:09, 365MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  33% 1.65G/4.92G [00:04<00:09, 356MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  34% 1.69G/4.92G [00:04<00:08, 364MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  35% 1.73G/4.92G [00:04<00:08, 372MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  36% 1.77G/4.92G [00:04<00:08, 365MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  37% 1.81G/4.92G [00:04<00:08, 371MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  38% 1.86G/4.92G [00:04<00:08, 370MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  39% 1.90G/4.92G [00:05<00:08, 376MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  39% 1.94G/4.92G [00:05<00:07, 376MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  40% 1.98G/4.92G [00:05<00:07, 373MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  41% 2.02G/4.92G [00:05<00:07, 371MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  42% 2.07G/4.92G [00:05<00:07, 367MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  43% 2.11G/4.92G [00:05<00:07, 363MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  44% 2.15G/4.92G [00:05<00:07, 363MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  45% 2.19G/4.92G [00:05<00:07, 348MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  45% 2.23G/4.92G [00:05<00:07, 345MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  46% 2.28G/4.92G [00:06<00:07, 349MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  47% 2.32G/4.92G [00:06<00:07, 352MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  48% 2.36G/4.92G [00:06<00:07, 354MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  49% 2.40G/4.92G [00:06<00:07, 349MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  50% 2.44G/4.92G [00:06<00:07, 350MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  51% 2.49G/4.92G [00:06<00:07, 345MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  51% 2.53G/4.92G [00:06<00:06, 350MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  52% 2.57G/4.92G [00:06<00:06, 350MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  53% 2.61G/4.92G [00:07<00:06, 345MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  54% 2.65G/4.92G [00:07<00:06, 343MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  55% 2.69G/4.92G [00:07<00:06, 348MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  56% 2.74G/4.92G [00:07<00:06, 361MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  57% 2.79G/4.92G [00:07<00:05, 380MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  58% 2.83G/4.92G [00:07<00:05, 382MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  58% 2.87G/4.92G [00:07<00:05, 390MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  59% 2.92G/4.92G [00:07<00:05, 397MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  60% 2.96G/4.92G [00:07<00:04, 403MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  61% 3.00G/4.92G [00:08<00:04, 393MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  62% 3.04G/4.92G [00:08<00:04, 391MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  63% 3.08G/4.92G [00:08<00:04, 398MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  64% 3.12G/4.92G [00:08<00:04, 402MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  64% 3.17G/4.92G [00:08<00:04, 399MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  65% 3.21G/4.92G [00:08<00:04, 404MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  66% 3.25G/4.92G [00:08<00:06, 251MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  67% 3.29G/4.92G [00:08<00:05, 279MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  68% 3.33G/4.92G [00:09<00:05, 310MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  69% 3.38G/4.92G [00:09<00:04, 327MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  70% 3.42G/4.92G [00:09<00:04, 339MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  70% 3.46G/4.92G [00:09<00:04, 346MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  71% 3.50G/4.92G [00:09<00:03, 362MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  72% 3.54G/4.92G [00:09<00:03, 376MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  73% 3.59G/4.92G [00:09<00:03, 382MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  74% 3.63G/4.92G [00:09<00:03, 391MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  75% 3.67G/4.92G [00:09<00:03, 392MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  76% 3.71G/4.92G [00:10<00:03, 383MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  76% 3.75G/4.92G [00:10<00:03, 366MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  77% 3.80G/4.92G [00:10<00:03, 363MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  78% 3.84G/4.92G [00:10<00:03, 359MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  79% 3.88G/4.92G [00:10<00:02, 359MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  80% 3.92G/4.92G [00:10<00:02, 354MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  81% 3.96G/4.92G [00:10<00:02, 355MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  81% 4.01G/4.92G [00:10<00:02, 350MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  82% 4.05G/4.92G [00:11<00:02, 351MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  83% 4.09G/4.92G [00:11<00:02, 355MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  84% 4.13G/4.92G [00:11<00:02, 362MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  85% 4.17G/4.92G [00:11<00:02, 354MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  86% 4.22G/4.92G [00:11<00:01, 356MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  87% 4.26G/4.92G [00:11<00:01, 350MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  87% 4.30G/4.92G [00:11<00:01, 334MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  88% 4.34G/4.92G [00:11<00:01, 334MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  89% 4.38G/4.92G [00:11<00:01, 338MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  90% 4.42G/4.92G [00:12<00:01, 340MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  91% 4.47G/4.92G [00:12<00:01, 341MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  92% 4.51G/4.92G [00:12<00:01, 348MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  93% 4.55G/4.92G [00:12<00:01, 364MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  93% 4.59G/4.92G [00:12<00:00, 377MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  94% 4.63G/4.92G [00:12<00:00, 384MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  95% 4.68G/4.92G [00:12<00:00, 392MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  96% 4.72G/4.92G [00:12<00:00, 400MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  97% 4.76G/4.92G [00:12<00:00, 401MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  98% 4.80G/4.92G [00:13<00:00, 403MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  99% 4.84G/4.92G [00:13<00:00, 403MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors: 100% 4.92G/4.92G [00:13<00:00, 368MB/s]\n",
            "Downloading shards:  80% 4/5 [04:42<00:58, 58.90s/it]\n",
            "model-00005-of-00005.safetensors:   0% 0.00/1.88G [00:00<?, ?B/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:   3% 52.4M/1.88G [00:00<00:04, 428MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:   6% 105M/1.88G [00:00<00:04, 417MB/s] \u001b[A\n",
            "model-00005-of-00005.safetensors:   8% 147M/1.88G [00:00<00:04, 412MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  10% 189M/1.88G [00:00<00:04, 413MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  12% 231M/1.88G [00:00<00:04, 408MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  15% 273M/1.88G [00:00<00:04, 400MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  17% 315M/1.88G [00:00<00:04, 383MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  19% 357M/1.88G [00:00<00:03, 384MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  21% 398M/1.88G [00:01<00:03, 388MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  23% 440M/1.88G [00:01<00:03, 381MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  26% 482M/1.88G [00:01<00:03, 371MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  28% 524M/1.88G [00:01<00:03, 364MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  30% 566M/1.88G [00:01<00:03, 359MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  32% 608M/1.88G [00:01<00:03, 365MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  35% 650M/1.88G [00:01<00:03, 367MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  37% 692M/1.88G [00:01<00:03, 375MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  39% 734M/1.88G [00:01<00:02, 387MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  41% 776M/1.88G [00:02<00:02, 385MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  44% 818M/1.88G [00:02<00:02, 387MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  46% 860M/1.88G [00:02<00:02, 393MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  48% 902M/1.88G [00:02<00:02, 391MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  50% 944M/1.88G [00:02<00:02, 394MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  52% 986M/1.88G [00:02<00:02, 398MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  55% 1.03G/1.88G [00:02<00:02, 393MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  57% 1.07G/1.88G [00:02<00:02, 397MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  59% 1.11G/1.88G [00:02<00:01, 401MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  61% 1.15G/1.88G [00:02<00:01, 395MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  64% 1.20G/1.88G [00:03<00:01, 398MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  66% 1.24G/1.88G [00:03<00:01, 399MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  68% 1.28G/1.88G [00:03<00:01, 386MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  70% 1.32G/1.88G [00:03<00:01, 382MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  73% 1.36G/1.88G [00:03<00:01, 371MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  75% 1.41G/1.88G [00:03<00:01, 372MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  77% 1.45G/1.88G [00:03<00:01, 304MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  79% 1.49G/1.88G [00:03<00:01, 309MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  81% 1.53G/1.88G [00:04<00:01, 306MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  84% 1.57G/1.88G [00:04<00:00, 314MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  86% 1.61G/1.88G [00:04<00:00, 311MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  88% 1.66G/1.88G [00:04<00:00, 313MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  90% 1.70G/1.88G [00:04<00:00, 319MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  93% 1.74G/1.88G [00:04<00:00, 315MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  95% 1.78G/1.88G [00:04<00:00, 310MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  97% 1.81G/1.88G [00:05<00:00, 304MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  98% 1.85G/1.88G [00:05<00:00, 300MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors: 100% 1.88G/1.88G [00:05<00:00, 357MB/s]\n",
            "Downloading shards: 100% 5/5 [04:47<00:00, 57.57s/it]\n",
            "Loading checkpoint shards: 100% 5/5 [00:10<00:00,  2.05s/it]\n",
            "generation_config.json: 100% 133/133 [00:00<00:00, 766kB/s]\n",
            "[2024-03-08 07:19:55,051] [INFO] [axolotl.load_model:710] [PID:2300] [RANK:0] converting modules to torch.float16 for flash attention\u001b[39m\n",
            "[2024-03-08 07:19:55,289] [INFO] [axolotl.load_lora:825] [PID:2300] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'q_proj', 'v_proj', 'k_proj', 'up_proj', 'o_proj']\u001b[39m\n",
            "[2024-03-08 07:19:55,290] [DEBUG] [axolotl.load_lora:853] [PID:2300] [RANK:0] Loading pretrained PEFT - LoRA\u001b[39m\n",
            "trainable params: 125,829,120 || all params: 10,930,753,536 || trainable%: 1.1511477190075399\n",
            "[2024-03-08 07:20:04,678] [INFO] [axolotl.load_model:750] [PID:2300] [RANK:0] GPU memory usage after adapters: 0.000GB ()\u001b[39m\n",
            "[2024-03-08 07:20:04,679] [INFO] [axolotl.scripts.do_merge_lora:129] [PID:2300] [RANK:0] running merge of LoRA with base model\u001b[39m\n",
            "Unloading and merging model: 100% 1014/1014 [00:28<00:00, 35.56it/s]\n",
            "[2024-03-08 07:20:33,208] [INFO] [axolotl.scripts.do_merge_lora:138] [PID:2300] [RANK:0] saving merged model to: /content/drive/MyDrive/DACON/Hansol_QA/models/yanolja-axolotl-aug-noise-sysprompt/merged\u001b[39m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m axolotl.cli.inference config.yaml --lora_model_dir=\"/content/drive/MyDrive/DACON/Hansol_QA/models/yanolja-axolotl_v0.2\""
      ],
      "metadata": {
        "id": "G9rmwV8ja3B2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "043294bf-d2fa-43fd-afb5-1c76bab7f4b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-03-09 08:06:25,626] [INFO] [numexpr.utils._init_num_threads:160] [PID:3657] NumExpr defaulting to 2 threads.\n",
            "[2024-03-09 08:06:25,944] [INFO] [datasets.<module>:58] [PID:3657] PyTorch version 2.1.1 available.\n",
            "[2024-03-09 08:06:25,945] [INFO] [datasets.<module>:95] [PID:3657] TensorFlow version 2.15.0 available.\n",
            "[2024-03-09 08:06:25,946] [INFO] [datasets.<module>:108] [PID:3657] JAX version 0.4.23 available.\n",
            "2024-03-09 08:06:28.014528: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-09 08:06:28.014578: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-09 08:06:28.016077: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-09 08:06:29.894541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2024-03-09 08:06:32,003] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "                                 dP            dP   dP \n",
            "                                 88            88   88 \n",
            "      .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n",
            "      88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n",
            "      88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n",
            "      `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP \n",
            "                                                       \n",
            "                                                       \n",
            "\n",
            "[2024-03-09 08:06:35,105] [INFO] [axolotl.normalize_config:178] [PID:3657] [RANK:0] GPU memory usage baseline: 0.000GB (+0.255GB misc)\u001b[39m\n",
            "[2024-03-09 08:06:35,109] [INFO] [axolotl.common.cli.load_model_and_tokenizer:50] [PID:3657] [RANK:0] loading tokenizer... yanolja/KoSOLAR-10.7B-v0.2\u001b[39m\n",
            "[2024-03-09 08:06:35,425] [DEBUG] [axolotl.load_tokenizer:255] [PID:3657] [RANK:0] EOS: 32000 / <|im_end|>\u001b[39m\n",
            "[2024-03-09 08:06:35,425] [DEBUG] [axolotl.load_tokenizer:256] [PID:3657] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2024-03-09 08:06:35,425] [DEBUG] [axolotl.load_tokenizer:257] [PID:3657] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
            "[2024-03-09 08:06:35,425] [DEBUG] [axolotl.load_tokenizer:258] [PID:3657] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2024-03-09 08:06:35,426] [INFO] [axolotl.load_tokenizer:269] [PID:3657] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2024-03-09 08:06:35,426] [INFO] [axolotl.common.cli.load_model_and_tokenizer:52] [PID:3657] [RANK:0] loading model and (optionally) peft_config...\u001b[39m\n",
            "Loading checkpoint shards: 100% 5/5 [01:32<00:00, 18.47s/it]\n",
            "[2024-03-09 08:08:10,276] [INFO] [axolotl.load_model:851] [PID:3657] [RANK:0] GPU memory usage after model load: 5.766GB (+0.023GB cache, +0.368GB misc)\u001b[39m\n",
            "[2024-03-09 08:08:10,319] [INFO] [axolotl.load_model:895] [PID:3657] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n",
            "[2024-03-09 08:08:10,329] [INFO] [axolotl.load_model:904] [PID:3657] [RANK:0] converting modules to torch.float16 for flash attention\u001b[39m\n",
            "[2024-03-09 08:08:10,338] [INFO] [axolotl.load_lora:1048] [PID:3657] [RANK:0] found linear modules: ['gate_proj', 'o_proj', 'k_proj', 'v_proj', 'up_proj', 'down_proj', 'q_proj']\u001b[39m\n",
            "[2024-03-09 08:08:10,338] [DEBUG] [axolotl.load_lora:1081] [PID:3657] [RANK:0] Loading pretrained PEFT - LoRA\u001b[39m\n",
            "trainable params: 125,829,120 || all params: 10,930,753,536 || trainable%: 1.1511477190075399\n",
            "[2024-03-09 08:08:27,059] [INFO] [axolotl.load_model:949] [PID:3657] [RANK:0] GPU memory usage after adapters: 6.236GB (+1.249GB cache, +0.368GB misc)\u001b[39m\n",
            "================================================================================\n",
            "Give me an instruction (Ctrl + D to submit): \n",
            "제시된 모든 질문에 대해 친절하고 명확하게 대답하시오.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "git clone https://github.com/ggerganov/llama.cpp\n",
        "\n",
        "cd llama.cpp\n",
        "\n",
        "pip install -r requirements.txt\n",
        "make LLAMA_OPENBLAS=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5FlrUCL657d",
        "outputId": "3cdb8a40-3b20-41ab-c471-7c6e5bc7736c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 19904, done.\u001b[K\n",
            "remote: Total 19904 (delta 0), reused 0 (delta 0), pack-reused 19904\u001b[K\n",
            "Receiving objects: 100% (19904/19904), 23.79 MiB | 16.10 MiB/s, done.\n",
            "Resolving deltas: 100% (13978/13978), done.\n",
            "Collecting numpy~=1.24.4 (from -r ./requirements/requirements-convert.txt (line 1))\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece~=0.1.98 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert.txt (line 2)) (0.1.99)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert.txt (line 3)) (4.38.2)\n",
            "Collecting gguf>=0.1.0 (from -r ./requirements/requirements-convert.txt (line 4))\n",
            "  Downloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.0 (from -r ./requirements/requirements-convert.txt (line 5))\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch~=2.1.1 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.1)\n",
            "Requirement already satisfied: einops~=0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert-hf-to-gguf.txt (line 3)) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.4.99)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)\n",
            "Installing collected packages: protobuf, numpy, gguf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.3 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gguf-0.6.0 numpy-1.24.4 protobuf-4.25.3\n",
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native  -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/ \n",
            "I NVCCFLAGS: -std=c++11 -O3 \n",
            "I LDFLAGS:   -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native  -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c llama.cpp -o llama.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c common/common.cpp -o common.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c common/sampling.cpp -o sampling.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c common/build-info.cpp -o build-info.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c common/console.cpp -o console.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native  -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native  -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native  -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/main/main.cpp -o examples/main/main.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/main/main.o -o main -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/quantize/quantize.cpp -o examples/quantize/quantize.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/quantize/quantize.o -o quantize -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/quantize-stats/quantize-stats.cpp -o examples/quantize-stats/quantize-stats.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/quantize-stats/quantize-stats.o -o quantize-stats -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/perplexity/perplexity.cpp -o examples/perplexity/perplexity.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/perplexity/perplexity.o -o perplexity -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/imatrix/imatrix.cpp -o examples/imatrix/imatrix.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/imatrix/imatrix.o -o imatrix -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/embedding/embedding.cpp -o examples/embedding/embedding.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/embedding/embedding.o -o embedding -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c pocs/vdot/vdot.cpp -o pocs/vdot/vdot.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o pocs/vdot/vdot.o -o vdot -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c pocs/vdot/q8dot.cpp -o pocs/vdot/q8dot.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o pocs/vdot/q8dot.o -o q8dot -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c common/train.cpp -o train.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/train-text-from-scratch/train-text-from-scratch.cpp -o examples/train-text-from-scratch/train-text-from-scratch.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/train-text-from-scratch/train-text-from-scratch.o -o train-text-from-scratch -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp -o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o -o convert-llama2c-to-ggml -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/simple/simple.cpp -o examples/simple/simple.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/simple/simple.o -o simple -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/batched/batched.cpp -o examples/batched/batched.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/batched/batched.o -o batched -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/batched-bench/batched-bench.cpp -o examples/batched-bench/batched-bench.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  build-info.o ggml.o llama.o common.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/batched-bench/batched-bench.o -o batched-bench -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/save-load-state/save-load-state.cpp -o examples/save-load-state/save-load-state.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/save-load-state/save-load-state.o -o save-load-state -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/server/server.cpp -o examples/server/server.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -Iexamples/server examples/server/server.o -o server -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/gguf/gguf.cpp -o examples/gguf/gguf.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/gguf/gguf.o -o gguf -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/llama-bench/llama-bench.cpp -o examples/llama-bench/llama-bench.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/llama-bench/llama-bench.o -o llama-bench -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/llava/llava-cli.cpp -o examples/llava/llava-cli.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/llava/clip.cpp  -o examples/llava/clip.o -Wno-cast-qual\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/llava/llava.cpp -o examples/llava/llava.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/llava/llava-cli.o examples/llava/clip.o examples/llava/llava.o -o llava-cli -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/baby-llama/baby-llama.cpp -o examples/baby-llama/baby-llama.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/baby-llama/baby-llama.o -o baby-llama -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/beam-search/beam-search.cpp -o examples/beam-search/beam-search.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/beam-search/beam-search.o -o beam-search -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/speculative/speculative.cpp -o examples/speculative/speculative.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/speculative/speculative.o -o speculative -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/infill/infill.cpp -o examples/infill/infill.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/infill/infill.o -o infill -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/tokenize/tokenize.cpp -o examples/tokenize/tokenize.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/tokenize/tokenize.o -o tokenize -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/benchmark/benchmark-matmult.cpp -o examples/benchmark/benchmark-matmult.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  build-info.o ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/benchmark/benchmark-matmult.o -o benchmark-matmult -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/parallel/parallel.cpp -o examples/parallel/parallel.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/parallel/parallel.o -o parallel -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/finetune/finetune.cpp -o examples/finetune/finetune.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/finetune/finetune.o -o finetune -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/export-lora/export-lora.cpp -o examples/export-lora/export-lora.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/export-lora/export-lora.o -o export-lora -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/lookahead/lookahead.cpp -o examples/lookahead/lookahead.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/lookahead/lookahead.o -o lookahead -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/lookup/lookup.cpp -o examples/lookup/lookup.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/lookup/lookup.o -o lookup -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -c examples/passkey/passkey.cpp -o examples/passkey/passkey.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/passkey/passkey.o -o passkey -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native  -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python llama.cpp/convert.py /content/drive/MyDrive/DACON/Hansol_QA/model/yanolja-axolotl-aug-noise-sysprompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-W1Hfh566eq",
        "outputId": "933ef9ad-6659-474f-e023-7ce760406455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model file /content/drive/MyDrive/DACON/Hansol_QA/model/yanolja-axolotl-aug-noise-sysprompt/pytorch_model-00001-of-00005.bin\n",
            "Loading model file /content/drive/MyDrive/DACON/Hansol_QA/model/yanolja-axolotl-aug-noise-sysprompt/pytorch_model-00001-of-00005.bin\n",
            "Loading model file /content/drive/MyDrive/DACON/Hansol_QA/model/yanolja-axolotl-aug-noise-sysprompt/pytorch_model-00002-of-00005.bin\n",
            "Loading model file /content/drive/MyDrive/DACON/Hansol_QA/model/yanolja-axolotl-aug-noise-sysprompt/pytorch_model-00003-of-00005.bin\n",
            "Loading model file /content/drive/MyDrive/DACON/Hansol_QA/model/yanolja-axolotl-aug-noise-sysprompt/pytorch_model-00004-of-00005.bin\n",
            "Loading model file /content/drive/MyDrive/DACON/Hansol_QA/model/yanolja-axolotl-aug-noise-sysprompt/pytorch_model-00005-of-00005.bin\n",
            "params = Params(n_vocab=40960, n_embd=4096, n_layer=48, n_ctx=4096, n_ff=14336, n_head=32, n_head_kv=8, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('/content/drive/MyDrive/DACON/Hansol_QA/model/yanolja-axolotl-aug-noise-sysprompt'))\n",
            "Found vocab files: {'spm': None, 'bpe': None, 'hfft': PosixPath('/content/drive/MyDrive/DACON/Hansol_QA/model/yanolja-axolotl-aug-noise-sysprompt/tokenizer.json')}\n",
            "Loading vocab file PosixPath('/content/drive/MyDrive/DACON/Hansol_QA/model/yanolja-axolotl-aug-noise-sysprompt/tokenizer.json'), type 'hfft'\n",
            "fname_tokenizer: /content/drive/MyDrive/DACON/Hansol_QA/model/yanolja-axolotl-aug-noise-sysprompt\n",
            "Vocab info: <HfVocab with 40960 base tokens and 0 added tokens>\n",
            "Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 32000, 'unk': 0, 'pad': 2}, add special tokens {'bos': True, 'eos': False}>\n",
            "Permuting layer 0\n",
            "Permuting layer 1\n",
            "Permuting layer 2\n",
            "Permuting layer 3\n",
            "Permuting layer 4\n",
            "Permuting layer 5\n",
            "Permuting layer 6\n",
            "Permuting layer 7\n",
            "Permuting layer 8\n",
            "Permuting layer 9\n",
            "Permuting layer 10\n",
            "Permuting layer 11\n",
            "Permuting layer 12\n",
            "Permuting layer 13\n",
            "Permuting layer 14\n",
            "Permuting layer 15\n",
            "Permuting layer 16\n",
            "Permuting layer 17\n",
            "Permuting layer 18\n",
            "Permuting layer 19\n",
            "Permuting layer 20\n",
            "Permuting layer 21\n",
            "Permuting layer 22\n",
            "Permuting layer 23\n",
            "Permuting layer 24\n",
            "Permuting layer 25\n",
            "Permuting layer 26\n",
            "Permuting layer 27\n",
            "Permuting layer 28\n",
            "Permuting layer 29\n",
            "Permuting layer 30\n",
            "Permuting layer 31\n",
            "Permuting layer 32\n",
            "Permuting layer 33\n",
            "Permuting layer 34\n",
            "Permuting layer 35\n",
            "Permuting layer 36\n",
            "Permuting layer 37\n",
            "Permuting layer 38\n",
            "Permuting layer 39\n",
            "Permuting layer 40\n",
            "Permuting layer 41\n",
            "Permuting layer 42\n",
            "Permuting layer 43\n",
            "Permuting layer 44\n",
            "Permuting layer 45\n",
            "Permuting layer 46\n",
            "Permuting layer 47\n",
            "model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [40960, 4096]\n",
            "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.32.self_attn.q_proj.weight          -> blk.32.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.32.self_attn.k_proj.weight          -> blk.32.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.32.self_attn.v_proj.weight          -> blk.32.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.32.self_attn.o_proj.weight          -> blk.32.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.32.mlp.gate_proj.weight             -> blk.32.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.32.mlp.up_proj.weight               -> blk.32.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.32.mlp.down_proj.weight             -> blk.32.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.32.input_layernorm.weight           -> blk.32.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.32.post_attention_layernorm.weight  -> blk.32.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.33.self_attn.q_proj.weight          -> blk.33.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.33.self_attn.k_proj.weight          -> blk.33.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.33.self_attn.v_proj.weight          -> blk.33.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.33.self_attn.o_proj.weight          -> blk.33.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.33.mlp.gate_proj.weight             -> blk.33.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.33.mlp.up_proj.weight               -> blk.33.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.33.mlp.down_proj.weight             -> blk.33.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.33.input_layernorm.weight           -> blk.33.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.33.post_attention_layernorm.weight  -> blk.33.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.34.self_attn.q_proj.weight          -> blk.34.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.34.self_attn.k_proj.weight          -> blk.34.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.34.self_attn.v_proj.weight          -> blk.34.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.34.self_attn.o_proj.weight          -> blk.34.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.34.mlp.gate_proj.weight             -> blk.34.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.34.mlp.up_proj.weight               -> blk.34.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.34.mlp.down_proj.weight             -> blk.34.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.34.input_layernorm.weight           -> blk.34.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.34.post_attention_layernorm.weight  -> blk.34.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.35.self_attn.q_proj.weight          -> blk.35.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.35.self_attn.k_proj.weight          -> blk.35.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.35.self_attn.v_proj.weight          -> blk.35.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.35.self_attn.o_proj.weight          -> blk.35.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.35.mlp.gate_proj.weight             -> blk.35.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.35.mlp.up_proj.weight               -> blk.35.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.35.mlp.down_proj.weight             -> blk.35.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.35.input_layernorm.weight           -> blk.35.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.35.post_attention_layernorm.weight  -> blk.35.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.36.self_attn.q_proj.weight          -> blk.36.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.36.self_attn.k_proj.weight          -> blk.36.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.36.self_attn.v_proj.weight          -> blk.36.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.36.self_attn.o_proj.weight          -> blk.36.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.36.mlp.gate_proj.weight             -> blk.36.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.36.mlp.up_proj.weight               -> blk.36.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.36.mlp.down_proj.weight             -> blk.36.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.36.input_layernorm.weight           -> blk.36.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.36.post_attention_layernorm.weight  -> blk.36.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.37.self_attn.q_proj.weight          -> blk.37.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.37.self_attn.k_proj.weight          -> blk.37.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.37.self_attn.v_proj.weight          -> blk.37.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.37.self_attn.o_proj.weight          -> blk.37.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.37.mlp.gate_proj.weight             -> blk.37.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.37.mlp.up_proj.weight               -> blk.37.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.37.mlp.down_proj.weight             -> blk.37.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.37.input_layernorm.weight           -> blk.37.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.37.post_attention_layernorm.weight  -> blk.37.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.38.self_attn.q_proj.weight          -> blk.38.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.38.self_attn.k_proj.weight          -> blk.38.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.38.self_attn.v_proj.weight          -> blk.38.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.38.self_attn.o_proj.weight          -> blk.38.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.38.mlp.gate_proj.weight             -> blk.38.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.38.mlp.up_proj.weight               -> blk.38.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.38.mlp.down_proj.weight             -> blk.38.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.38.input_layernorm.weight           -> blk.38.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.38.post_attention_layernorm.weight  -> blk.38.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.39.self_attn.q_proj.weight          -> blk.39.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.39.self_attn.k_proj.weight          -> blk.39.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.39.self_attn.v_proj.weight          -> blk.39.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.39.self_attn.o_proj.weight          -> blk.39.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.39.mlp.gate_proj.weight             -> blk.39.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.39.mlp.up_proj.weight               -> blk.39.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.39.mlp.down_proj.weight             -> blk.39.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.39.input_layernorm.weight           -> blk.39.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.39.post_attention_layernorm.weight  -> blk.39.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.40.self_attn.q_proj.weight          -> blk.40.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.40.self_attn.k_proj.weight          -> blk.40.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.40.self_attn.v_proj.weight          -> blk.40.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.40.self_attn.o_proj.weight          -> blk.40.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.40.mlp.gate_proj.weight             -> blk.40.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.40.mlp.up_proj.weight               -> blk.40.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.40.mlp.down_proj.weight             -> blk.40.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.40.input_layernorm.weight           -> blk.40.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.40.post_attention_layernorm.weight  -> blk.40.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.41.self_attn.q_proj.weight          -> blk.41.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.41.self_attn.k_proj.weight          -> blk.41.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.41.self_attn.v_proj.weight          -> blk.41.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.41.self_attn.o_proj.weight          -> blk.41.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.41.mlp.gate_proj.weight             -> blk.41.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.41.mlp.up_proj.weight               -> blk.41.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.41.mlp.down_proj.weight             -> blk.41.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.41.input_layernorm.weight           -> blk.41.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.41.post_attention_layernorm.weight  -> blk.41.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.42.self_attn.q_proj.weight          -> blk.42.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.42.self_attn.k_proj.weight          -> blk.42.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.42.self_attn.v_proj.weight          -> blk.42.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.42.self_attn.o_proj.weight          -> blk.42.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.42.mlp.gate_proj.weight             -> blk.42.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.42.mlp.up_proj.weight               -> blk.42.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.42.mlp.down_proj.weight             -> blk.42.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.42.input_layernorm.weight           -> blk.42.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.42.post_attention_layernorm.weight  -> blk.42.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.43.self_attn.q_proj.weight          -> blk.43.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.43.self_attn.k_proj.weight          -> blk.43.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.43.self_attn.v_proj.weight          -> blk.43.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.43.self_attn.o_proj.weight          -> blk.43.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.43.mlp.gate_proj.weight             -> blk.43.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.43.mlp.up_proj.weight               -> blk.43.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.43.mlp.down_proj.weight             -> blk.43.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.43.input_layernorm.weight           -> blk.43.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.43.post_attention_layernorm.weight  -> blk.43.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.44.self_attn.q_proj.weight          -> blk.44.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.44.self_attn.k_proj.weight          -> blk.44.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.44.self_attn.v_proj.weight          -> blk.44.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.44.self_attn.o_proj.weight          -> blk.44.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.44.mlp.gate_proj.weight             -> blk.44.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.44.mlp.up_proj.weight               -> blk.44.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.44.mlp.down_proj.weight             -> blk.44.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.44.input_layernorm.weight           -> blk.44.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.44.post_attention_layernorm.weight  -> blk.44.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.45.self_attn.q_proj.weight          -> blk.45.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.45.self_attn.k_proj.weight          -> blk.45.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.45.self_attn.v_proj.weight          -> blk.45.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.45.self_attn.o_proj.weight          -> blk.45.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.45.mlp.gate_proj.weight             -> blk.45.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.45.mlp.up_proj.weight               -> blk.45.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.45.mlp.down_proj.weight             -> blk.45.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.45.input_layernorm.weight           -> blk.45.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.45.post_attention_layernorm.weight  -> blk.45.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.46.self_attn.q_proj.weight          -> blk.46.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.46.self_attn.k_proj.weight          -> blk.46.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.46.self_attn.v_proj.weight          -> blk.46.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.46.self_attn.o_proj.weight          -> blk.46.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.46.mlp.gate_proj.weight             -> blk.46.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.46.mlp.up_proj.weight               -> blk.46.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.46.mlp.down_proj.weight             -> blk.46.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.46.input_layernorm.weight           -> blk.46.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.46.post_attention_layernorm.weight  -> blk.46.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.47.self_attn.q_proj.weight          -> blk.47.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.47.self_attn.k_proj.weight          -> blk.47.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.47.self_attn.v_proj.weight          -> blk.47.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.47.self_attn.o_proj.weight          -> blk.47.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.47.mlp.gate_proj.weight             -> blk.47.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.47.mlp.up_proj.weight               -> blk.47.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.47.mlp.down_proj.weight             -> blk.47.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.47.input_layernorm.weight           -> blk.47.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.47.post_attention_layernorm.weight  -> blk.47.ffn_norm.weight                   | F16    | [4096]\n",
            "model.norm.weight                                -> output_norm.weight                       | F16    | [4096]\n",
            "lm_head.weight                                   -> output.weight                            | F16    | [40960, 4096]\n",
            "Writing /content/drive/MyDrive/DACON/Hansol_QA/model/yanolja-axolotl-aug-noise-sysprompt/ggml-model-f16.gguf, format 1\n",
            "Ignoring added_tokens.json since model matches vocab size without it.\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "gguf: Setting special token type bos to 1\n",
            "gguf: Setting special token type eos to 32000\n",
            "gguf: Setting special token type unk to 0\n",
            "gguf: Setting special token type pad to 2\n",
            "gguf: Setting add_bos_token to True\n",
            "gguf: Setting add_eos_token to False\n",
            "[  1/435] Writing tensor token_embd.weight                      | size  40960 x   4096  | type F16  | T+   4\n",
            "[  2/435] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   6\n",
            "[  3/435] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   7\n",
            "[  4/435] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   7\n",
            "[  5/435] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   7\n",
            "[  6/435] Writing tensor blk.0.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   7\n",
            "[  7/435] Writing tensor blk.0.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   9\n",
            "[  8/435] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  11\n",
            "[  9/435] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+  13\n",
            "[ 10/435] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+  13\n",
            "[ 11/435] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  13\n",
            "[ 12/435] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  13\n",
            "[ 13/435] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  13\n",
            "[ 14/435] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+  13\n",
            "[ 15/435] Writing tensor blk.1.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  14\n",
            "[ 16/435] Writing tensor blk.1.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  16\n",
            "[ 17/435] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  17\n",
            "[ 18/435] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+  18\n",
            "[ 19/435] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  18\n",
            "[ 20/435] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  18\n",
            "[ 21/435] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  19\n",
            "[ 22/435] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  19\n",
            "[ 23/435] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+  19\n",
            "[ 24/435] Writing tensor blk.2.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  20\n",
            "[ 25/435] Writing tensor blk.2.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  22\n",
            "[ 26/435] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  25\n",
            "[ 27/435] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  26\n",
            "[ 28/435] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  26\n",
            "[ 29/435] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  26\n",
            "[ 30/435] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  27\n",
            "[ 31/435] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  27\n",
            "[ 32/435] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+  27\n",
            "[ 33/435] Writing tensor blk.3.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  27\n",
            "[ 34/435] Writing tensor blk.3.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  29\n",
            "[ 35/435] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  30\n",
            "[ 36/435] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  31\n",
            "[ 37/435] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  31\n",
            "[ 38/435] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  31\n",
            "[ 39/435] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  31\n",
            "[ 40/435] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  31\n",
            "[ 41/435] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+  31\n",
            "[ 42/435] Writing tensor blk.4.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  32\n",
            "[ 43/435] Writing tensor blk.4.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  34\n",
            "[ 44/435] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  37\n",
            "[ 45/435] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  37\n",
            "[ 46/435] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  37\n",
            "[ 47/435] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  37\n",
            "[ 48/435] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  38\n",
            "[ 49/435] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  38\n",
            "[ 50/435] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+  38\n",
            "[ 51/435] Writing tensor blk.5.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  38\n",
            "[ 52/435] Writing tensor blk.5.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  39\n",
            "[ 53/435] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  41\n",
            "[ 54/435] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  42\n",
            "[ 55/435] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  42\n",
            "[ 56/435] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  42\n",
            "[ 57/435] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  43\n",
            "[ 58/435] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  43\n",
            "[ 59/435] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+  43\n",
            "[ 60/435] Writing tensor blk.6.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  43\n",
            "[ 61/435] Writing tensor blk.6.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  47\n",
            "[ 62/435] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  48\n",
            "[ 63/435] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  48\n",
            "[ 64/435] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  49\n",
            "[ 65/435] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  49\n",
            "[ 66/435] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  49\n",
            "[ 67/435] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  49\n",
            "[ 68/435] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+  49\n",
            "[ 69/435] Writing tensor blk.7.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  54\n",
            "[ 70/435] Writing tensor blk.7.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  57\n",
            "[ 71/435] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  58\n",
            "[ 72/435] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  59\n",
            "[ 73/435] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  59\n",
            "[ 74/435] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  59\n",
            "[ 75/435] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  59\n",
            "[ 76/435] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  59\n",
            "[ 77/435] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+  59\n",
            "[ 78/435] Writing tensor blk.8.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  60\n",
            "[ 79/435] Writing tensor blk.8.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  62\n",
            "[ 80/435] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  64\n",
            "[ 81/435] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  65\n",
            "[ 82/435] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  65\n",
            "[ 83/435] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  65\n",
            "[ 84/435] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  65\n",
            "[ 85/435] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  65\n",
            "[ 86/435] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+  65\n",
            "[ 87/435] Writing tensor blk.9.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  66\n",
            "[ 88/435] Writing tensor blk.9.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  68\n",
            "[ 89/435] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  70\n",
            "[ 90/435] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  71\n",
            "[ 91/435] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  71\n",
            "[ 92/435] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  71\n",
            "[ 93/435] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  71\n",
            "[ 94/435] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  71\n",
            "[ 95/435] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+  72\n",
            "[ 96/435] Writing tensor blk.10.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  74\n",
            "[ 97/435] Writing tensor blk.10.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  74\n",
            "[ 98/435] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  76\n",
            "[ 99/435] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  77\n",
            "[100/435] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  77\n",
            "[101/435] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  77\n",
            "[102/435] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  77\n",
            "[103/435] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  77\n",
            "[104/435] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+  77\n",
            "[105/435] Writing tensor blk.11.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  77\n",
            "[106/435] Writing tensor blk.11.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  79\n",
            "[107/435] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  81\n",
            "[108/435] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  81\n",
            "[109/435] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  81\n",
            "[110/435] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  82\n",
            "[111/435] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  82\n",
            "[112/435] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  82\n",
            "[113/435] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+  82\n",
            "[114/435] Writing tensor blk.12.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  82\n",
            "[115/435] Writing tensor blk.12.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  83\n",
            "[116/435] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  86\n",
            "[117/435] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  87\n",
            "[118/435] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  87\n",
            "[119/435] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  87\n",
            "[120/435] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  87\n",
            "[121/435] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  87\n",
            "[122/435] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+  87\n",
            "[123/435] Writing tensor blk.13.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  87\n",
            "[124/435] Writing tensor blk.13.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  88\n",
            "[125/435] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  90\n",
            "[126/435] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  90\n",
            "[127/435] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  90\n",
            "[128/435] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  91\n",
            "[129/435] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  91\n",
            "[130/435] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  91\n",
            "[131/435] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+  91\n",
            "[132/435] Writing tensor blk.14.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  91\n",
            "[133/435] Writing tensor blk.14.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  93\n",
            "[134/435] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  94\n",
            "[135/435] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  95\n",
            "[136/435] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  95\n",
            "[137/435] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  95\n",
            "[138/435] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  95\n",
            "[139/435] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  95\n",
            "[140/435] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+  95\n",
            "[141/435] Writing tensor blk.15.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  97\n",
            "[142/435] Writing tensor blk.15.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  97\n",
            "[143/435] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  98\n",
            "[144/435] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+ 100\n",
            "[145/435] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+ 100\n",
            "[146/435] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 100\n",
            "[147/435] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 100\n",
            "[148/435] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 100\n",
            "[149/435] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 100\n",
            "[150/435] Writing tensor blk.16.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 100\n",
            "[151/435] Writing tensor blk.16.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 101\n",
            "[152/435] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 102\n",
            "[153/435] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+ 103\n",
            "[154/435] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+ 103\n",
            "[155/435] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 103\n",
            "[156/435] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 106\n",
            "[157/435] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 106\n",
            "[158/435] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 106\n",
            "[159/435] Writing tensor blk.17.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 106\n",
            "[160/435] Writing tensor blk.17.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 107\n",
            "[161/435] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 108\n",
            "[162/435] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+ 108\n",
            "[163/435] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+ 108\n",
            "[164/435] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 108\n",
            "[165/435] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 108\n",
            "[166/435] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 108\n",
            "[167/435] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 108\n",
            "[168/435] Writing tensor blk.18.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 109\n",
            "[169/435] Writing tensor blk.18.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 110\n",
            "[170/435] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 112\n",
            "[171/435] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+ 112\n",
            "[172/435] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+ 112\n",
            "[173/435] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 112\n",
            "[174/435] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 112\n",
            "[175/435] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 112\n",
            "[176/435] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 112\n",
            "[177/435] Writing tensor blk.19.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 113\n",
            "[178/435] Writing tensor blk.19.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 115\n",
            "[179/435] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 116\n",
            "[180/435] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+ 116\n",
            "[181/435] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+ 116\n",
            "[182/435] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 116\n",
            "[183/435] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 116\n",
            "[184/435] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 116\n",
            "[185/435] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 117\n",
            "[186/435] Writing tensor blk.20.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 118\n",
            "[187/435] Writing tensor blk.20.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 118\n",
            "[188/435] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 120\n",
            "[189/435] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+ 122\n",
            "[190/435] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+ 122\n",
            "[191/435] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 122\n",
            "[192/435] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 122\n",
            "[193/435] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 122\n",
            "[194/435] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 122\n",
            "[195/435] Writing tensor blk.21.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 122\n",
            "[196/435] Writing tensor blk.21.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 123\n",
            "[197/435] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 123\n",
            "[198/435] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+ 126\n",
            "[199/435] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+ 126\n",
            "[200/435] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 126\n",
            "[201/435] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 126\n",
            "[202/435] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 126\n",
            "[203/435] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 126\n",
            "[204/435] Writing tensor blk.22.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 126\n",
            "[205/435] Writing tensor blk.22.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 127\n",
            "[206/435] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 128\n",
            "[207/435] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+ 128\n",
            "[208/435] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+ 128\n",
            "[209/435] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 128\n",
            "[210/435] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 129\n",
            "[211/435] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 132\n",
            "[212/435] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 132\n",
            "[213/435] Writing tensor blk.23.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 132\n",
            "[214/435] Writing tensor blk.23.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 132\n",
            "[215/435] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 133\n",
            "[216/435] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+ 133\n",
            "[217/435] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+ 133\n",
            "[218/435] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 133\n",
            "[219/435] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 133\n",
            "[220/435] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 133\n",
            "[221/435] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 133\n",
            "[222/435] Writing tensor blk.24.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 134\n",
            "[223/435] Writing tensor blk.24.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 135\n",
            "[224/435] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 137\n",
            "[225/435] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+ 137\n",
            "[226/435] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+ 138\n",
            "[227/435] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 138\n",
            "[228/435] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 138\n",
            "[229/435] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 138\n",
            "[230/435] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 138\n",
            "[231/435] Writing tensor blk.25.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 138\n",
            "[232/435] Writing tensor blk.25.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 139\n",
            "[233/435] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 141\n",
            "[234/435] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+ 141\n",
            "[235/435] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+ 141\n",
            "[236/435] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 141\n",
            "[237/435] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 141\n",
            "[238/435] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 141\n",
            "[239/435] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 141\n",
            "[240/435] Writing tensor blk.26.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 142\n",
            "[241/435] Writing tensor blk.26.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 143\n",
            "[242/435] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 144\n",
            "[243/435] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+ 147\n",
            "[244/435] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+ 147\n",
            "[245/435] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 147\n",
            "[246/435] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 147\n",
            "[247/435] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 147\n",
            "[248/435] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 147\n",
            "[249/435] Writing tensor blk.27.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 147\n",
            "[250/435] Writing tensor blk.27.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 148\n",
            "[251/435] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 149\n",
            "[252/435] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+ 149\n",
            "[253/435] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+ 150\n",
            "[254/435] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 150\n",
            "[255/435] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 150\n",
            "[256/435] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 150\n",
            "[257/435] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 150\n",
            "[258/435] Writing tensor blk.28.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 150\n",
            "[259/435] Writing tensor blk.28.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 152\n",
            "[260/435] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 152\n",
            "[261/435] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+ 153\n",
            "[262/435] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+ 153\n",
            "[263/435] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 153\n",
            "[264/435] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 153\n",
            "[265/435] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 153\n",
            "[266/435] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 153\n",
            "[267/435] Writing tensor blk.29.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 153\n",
            "[268/435] Writing tensor blk.29.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 155\n",
            "[269/435] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 156\n",
            "[270/435] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+ 156\n",
            "[271/435] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 156\n",
            "[272/435] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 156\n",
            "[273/435] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 156\n",
            "[274/435] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 156\n",
            "[275/435] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 156\n",
            "[276/435] Writing tensor blk.30.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 157\n",
            "[277/435] Writing tensor blk.30.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 158\n",
            "[278/435] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 159\n",
            "[279/435] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 160\n",
            "[280/435] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 161\n",
            "[281/435] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 161\n",
            "[282/435] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 161\n",
            "[283/435] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 161\n",
            "[284/435] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 161\n",
            "[285/435] Writing tensor blk.31.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 161\n",
            "[286/435] Writing tensor blk.31.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 162\n",
            "[287/435] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 163\n",
            "[288/435] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 163\n",
            "[289/435] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 163\n",
            "[290/435] Writing tensor blk.32.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 163\n",
            "[291/435] Writing tensor blk.32.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 163\n",
            "[292/435] Writing tensor blk.32.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 163\n",
            "[293/435] Writing tensor blk.32.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 163\n",
            "[294/435] Writing tensor blk.32.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 164\n",
            "[295/435] Writing tensor blk.32.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 166\n",
            "[296/435] Writing tensor blk.32.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 167\n",
            "[297/435] Writing tensor blk.32.attn_norm.weight                | size   4096           | type F32  | T+ 167\n",
            "[298/435] Writing tensor blk.32.ffn_norm.weight                 | size   4096           | type F32  | T+ 167\n",
            "[299/435] Writing tensor blk.33.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 167\n",
            "[300/435] Writing tensor blk.33.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 167\n",
            "[301/435] Writing tensor blk.33.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 167\n",
            "[302/435] Writing tensor blk.33.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 167\n",
            "[303/435] Writing tensor blk.33.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 167\n",
            "[304/435] Writing tensor blk.33.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 169\n",
            "[305/435] Writing tensor blk.33.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 170\n",
            "[306/435] Writing tensor blk.33.attn_norm.weight                | size   4096           | type F32  | T+ 170\n",
            "[307/435] Writing tensor blk.33.ffn_norm.weight                 | size   4096           | type F32  | T+ 170\n",
            "[308/435] Writing tensor blk.34.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 170\n",
            "[309/435] Writing tensor blk.34.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 170\n",
            "[310/435] Writing tensor blk.34.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 170\n",
            "[311/435] Writing tensor blk.34.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 170\n",
            "[312/435] Writing tensor blk.34.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 171\n",
            "[313/435] Writing tensor blk.34.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 172\n",
            "[314/435] Writing tensor blk.34.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 173\n",
            "[315/435] Writing tensor blk.34.attn_norm.weight                | size   4096           | type F32  | T+ 173\n",
            "[316/435] Writing tensor blk.34.ffn_norm.weight                 | size   4096           | type F32  | T+ 173\n",
            "[317/435] Writing tensor blk.35.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 173\n",
            "[318/435] Writing tensor blk.35.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 173\n",
            "[319/435] Writing tensor blk.35.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 173\n",
            "[320/435] Writing tensor blk.35.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 173\n",
            "[321/435] Writing tensor blk.35.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 174\n",
            "[322/435] Writing tensor blk.35.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 176\n",
            "[323/435] Writing tensor blk.35.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 177\n",
            "[324/435] Writing tensor blk.35.attn_norm.weight                | size   4096           | type F32  | T+ 177\n",
            "[325/435] Writing tensor blk.35.ffn_norm.weight                 | size   4096           | type F32  | T+ 177\n",
            "[326/435] Writing tensor blk.36.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 177\n",
            "[327/435] Writing tensor blk.36.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 177\n",
            "[328/435] Writing tensor blk.36.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 177\n",
            "[329/435] Writing tensor blk.36.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 177\n",
            "[330/435] Writing tensor blk.36.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 177\n",
            "[331/435] Writing tensor blk.36.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 178\n",
            "[332/435] Writing tensor blk.36.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 180\n",
            "[333/435] Writing tensor blk.36.attn_norm.weight                | size   4096           | type F32  | T+ 180\n",
            "[334/435] Writing tensor blk.36.ffn_norm.weight                 | size   4096           | type F32  | T+ 180\n",
            "[335/435] Writing tensor blk.37.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 180\n",
            "[336/435] Writing tensor blk.37.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 181\n",
            "[337/435] Writing tensor blk.37.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 181\n",
            "[338/435] Writing tensor blk.37.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 181\n",
            "[339/435] Writing tensor blk.37.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 181\n",
            "[340/435] Writing tensor blk.37.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 182\n",
            "[341/435] Writing tensor blk.37.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 183\n",
            "[342/435] Writing tensor blk.37.attn_norm.weight                | size   4096           | type F32  | T+ 184\n",
            "[343/435] Writing tensor blk.37.ffn_norm.weight                 | size   4096           | type F32  | T+ 184\n",
            "[344/435] Writing tensor blk.38.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 184\n",
            "[345/435] Writing tensor blk.38.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 184\n",
            "[346/435] Writing tensor blk.38.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 184\n",
            "[347/435] Writing tensor blk.38.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 184\n",
            "[348/435] Writing tensor blk.38.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 184\n",
            "[349/435] Writing tensor blk.38.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 185\n",
            "[350/435] Writing tensor blk.38.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 186\n",
            "[351/435] Writing tensor blk.38.attn_norm.weight                | size   4096           | type F32  | T+ 187\n",
            "[352/435] Writing tensor blk.38.ffn_norm.weight                 | size   4096           | type F32  | T+ 187\n",
            "[353/435] Writing tensor blk.39.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 187\n",
            "[354/435] Writing tensor blk.39.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 187\n",
            "[355/435] Writing tensor blk.39.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 187\n",
            "[356/435] Writing tensor blk.39.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 187\n",
            "[357/435] Writing tensor blk.39.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 188\n",
            "[358/435] Writing tensor blk.39.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 188\n",
            "[359/435] Writing tensor blk.39.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 190\n",
            "[360/435] Writing tensor blk.39.attn_norm.weight                | size   4096           | type F32  | T+ 192\n",
            "[361/435] Writing tensor blk.39.ffn_norm.weight                 | size   4096           | type F32  | T+ 192\n",
            "[362/435] Writing tensor blk.40.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 192\n",
            "[363/435] Writing tensor blk.40.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 192\n",
            "[364/435] Writing tensor blk.40.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 192\n",
            "[365/435] Writing tensor blk.40.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 192\n",
            "[366/435] Writing tensor blk.40.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 192\n",
            "[367/435] Writing tensor blk.40.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 192\n",
            "[368/435] Writing tensor blk.40.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 193\n",
            "[369/435] Writing tensor blk.40.attn_norm.weight                | size   4096           | type F32  | T+ 193\n",
            "[370/435] Writing tensor blk.40.ffn_norm.weight                 | size   4096           | type F32  | T+ 193\n",
            "[371/435] Writing tensor blk.41.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 193\n",
            "[372/435] Writing tensor blk.41.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 194\n",
            "[373/435] Writing tensor blk.41.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 194\n",
            "[374/435] Writing tensor blk.41.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 194\n",
            "[375/435] Writing tensor blk.41.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 194\n",
            "[376/435] Writing tensor blk.41.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 195\n",
            "[377/435] Writing tensor blk.41.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 196\n",
            "[378/435] Writing tensor blk.41.attn_norm.weight                | size   4096           | type F32  | T+ 197\n",
            "[379/435] Writing tensor blk.41.ffn_norm.weight                 | size   4096           | type F32  | T+ 197\n",
            "[380/435] Writing tensor blk.42.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 197\n",
            "[381/435] Writing tensor blk.42.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 198\n",
            "[382/435] Writing tensor blk.42.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 198\n",
            "[383/435] Writing tensor blk.42.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 198\n",
            "[384/435] Writing tensor blk.42.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 198\n",
            "[385/435] Writing tensor blk.42.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 199\n",
            "[386/435] Writing tensor blk.42.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 199\n",
            "[387/435] Writing tensor blk.42.attn_norm.weight                | size   4096           | type F32  | T+ 200\n",
            "[388/435] Writing tensor blk.42.ffn_norm.weight                 | size   4096           | type F32  | T+ 200\n",
            "[389/435] Writing tensor blk.43.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 200\n",
            "[390/435] Writing tensor blk.43.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 202\n",
            "[391/435] Writing tensor blk.43.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 202\n",
            "[392/435] Writing tensor blk.43.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 202\n",
            "[393/435] Writing tensor blk.43.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 202\n",
            "[394/435] Writing tensor blk.43.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 202\n",
            "[395/435] Writing tensor blk.43.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 203\n",
            "[396/435] Writing tensor blk.43.attn_norm.weight                | size   4096           | type F32  | T+ 203\n",
            "[397/435] Writing tensor blk.43.ffn_norm.weight                 | size   4096           | type F32  | T+ 203\n",
            "[398/435] Writing tensor blk.44.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 203\n",
            "[399/435] Writing tensor blk.44.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 203\n",
            "[400/435] Writing tensor blk.44.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 203\n",
            "[401/435] Writing tensor blk.44.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 203\n",
            "[402/435] Writing tensor blk.44.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 204\n",
            "[403/435] Writing tensor blk.44.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 205\n",
            "[404/435] Writing tensor blk.44.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 206\n",
            "[405/435] Writing tensor blk.44.attn_norm.weight                | size   4096           | type F32  | T+ 207\n",
            "[406/435] Writing tensor blk.44.ffn_norm.weight                 | size   4096           | type F32  | T+ 207\n",
            "[407/435] Writing tensor blk.45.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 207\n",
            "[408/435] Writing tensor blk.45.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 208\n",
            "[409/435] Writing tensor blk.45.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 208\n",
            "[410/435] Writing tensor blk.45.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 208\n",
            "[411/435] Writing tensor blk.45.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 208\n",
            "[412/435] Writing tensor blk.45.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 208\n",
            "[413/435] Writing tensor blk.45.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 209\n",
            "[414/435] Writing tensor blk.45.attn_norm.weight                | size   4096           | type F32  | T+ 209\n",
            "[415/435] Writing tensor blk.45.ffn_norm.weight                 | size   4096           | type F32  | T+ 209\n",
            "[416/435] Writing tensor blk.46.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 209\n",
            "[417/435] Writing tensor blk.46.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 210\n",
            "[418/435] Writing tensor blk.46.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 210\n",
            "[419/435] Writing tensor blk.46.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 210\n",
            "[420/435] Writing tensor blk.46.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 212\n",
            "[421/435] Writing tensor blk.46.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 212\n",
            "[422/435] Writing tensor blk.46.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 213\n",
            "[423/435] Writing tensor blk.46.attn_norm.weight                | size   4096           | type F32  | T+ 213\n",
            "[424/435] Writing tensor blk.46.ffn_norm.weight                 | size   4096           | type F32  | T+ 213\n",
            "[425/435] Writing tensor blk.47.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 213\n",
            "[426/435] Writing tensor blk.47.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 213\n",
            "[427/435] Writing tensor blk.47.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 213\n",
            "[428/435] Writing tensor blk.47.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 213\n",
            "[429/435] Writing tensor blk.47.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 213\n",
            "[430/435] Writing tensor blk.47.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 218\n",
            "[431/435] Writing tensor blk.47.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 218\n",
            "[432/435] Writing tensor blk.47.attn_norm.weight                | size   4096           | type F32  | T+ 218\n",
            "[433/435] Writing tensor blk.47.ffn_norm.weight                 | size   4096           | type F32  | T+ 218\n",
            "[434/435] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 218\n",
            "[435/435] Writing tensor output.weight                          | size  40960 x   4096  | type F16  | T+ 218\n",
            "Wrote /content/drive/MyDrive/DACON/Hansol_QA/model/yanolja-axolotl-aug-noise-sysprompt/ggml-model-f16.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = \"/content/drive/MyDrive/DACON/Hansol_QA/model\"\n",
        "model_name = \"yanolja-axolotl-aug-noise-sysprompt\"\n",
        "quant_opt = \"q8_0\"\n",
        "\n",
        "# 명령어 실행\n",
        "!llama.cpp/quantize \"{model_dir}/{model_name}/ggml-model-f16.gguf\" \\\n",
        "                    \"{model_dir}/{model_name}/ggml-model-{quant_opt}.gguf\" \\\n",
        "                    {quant_opt}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_OSEUnS69TI",
        "outputId": "557efa28-da95-4c00-b39e-a47684e46e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 2360 (6cdabe65)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/drive/MyDrive/DACON/Hansol_QA/model/yanolja-axolotl-aug-noise-sysprompt/ggml-model-f16.gguf' to '/content/drive/MyDrive/DACON/Hansol_QA/model/yanolja-axolotl-aug-noise-sysprompt/ggml-model-q8_0.gguf' as Q8_0\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 435 tensors from /content/drive/MyDrive/DACON/Hansol_QA/model/yanolja-axolotl-aug-noise-sysprompt/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 48\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,40960]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,40960]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,40960]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   97 tensors\n",
            "llama_model_loader: - type  f16:  338 tensors\n",
            "llama_model_quantize_internal: meta size = 961568 bytes\n",
            "[   1/ 435]                    token_embd.weight - [ 4096, 40960,     1,     1], type =    f16, quantizing to q8_0 .. size =   320.00 MiB ->   170.00 MiB | hist: 0.000 0.029 0.022 0.034 0.052 0.070 0.089 0.101 0.207 0.101 0.089 0.069 0.051 0.034 0.022 0.029 \n",
            "[   2/ 435]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.022 0.010 0.014 0.020 0.029 0.041 0.067 0.591 0.067 0.041 0.029 0.020 0.014 0.010 0.022 \n",
            "[   3/ 435]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.023 0.012 0.017 0.025 0.035 0.050 0.078 0.519 0.078 0.050 0.035 0.025 0.018 0.012 0.023 \n",
            "[   4/ 435]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.024 0.013 0.020 0.031 0.047 0.072 0.118 0.351 0.118 0.072 0.047 0.031 0.020 0.013 0.024 \n",
            "[   5/ 435]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.026 0.018 0.028 0.043 0.064 0.087 0.111 0.246 0.111 0.087 0.064 0.044 0.028 0.018 0.026 \n",
            "[   6/ 435]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.107 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[   7/ 435]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.107 0.087 0.066 0.046 0.030 0.019 0.027 \n",
            "[   8/ 435]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[   9/ 435]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  10/ 435]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  11/ 435]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.025 0.015 0.022 0.034 0.048 0.065 0.081 0.422 0.081 0.065 0.048 0.034 0.023 0.015 0.025 \n",
            "[  12/ 435]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.024 0.014 0.021 0.032 0.047 0.065 0.083 0.428 0.083 0.064 0.047 0.032 0.021 0.014 0.024 \n",
            "[  13/ 435]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.025 0.015 0.023 0.034 0.048 0.063 0.081 0.420 0.081 0.064 0.048 0.034 0.023 0.015 0.025 \n",
            "[  14/ 435]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[  15/ 435]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
            "[  16/ 435]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.087 0.107 0.232 0.107 0.087 0.066 0.047 0.031 0.019 0.027 \n",
            "[  17/ 435]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[  18/ 435]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  19/ 435]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  20/ 435]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.087 0.109 0.240 0.109 0.087 0.065 0.045 0.030 0.019 0.027 \n",
            "[  21/ 435]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.064 0.087 0.109 0.244 0.109 0.087 0.064 0.044 0.029 0.018 0.026 \n",
            "[  22/ 435]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.018 0.029 0.044 0.063 0.086 0.110 0.249 0.109 0.086 0.063 0.044 0.029 0.018 0.027 \n",
            "[  23/ 435]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[  24/ 435]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[  25/ 435]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
            "[  26/ 435]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
            "[  27/ 435]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  28/ 435]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  29/ 435]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.107 0.232 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[  30/ 435]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.065 0.088 0.109 0.239 0.109 0.088 0.065 0.045 0.029 0.018 0.026 \n",
            "[  31/ 435]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.107 0.236 0.107 0.087 0.065 0.046 0.031 0.019 0.027 \n",
            "[  32/ 435]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
            "[  33/ 435]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[  34/ 435]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
            "[  35/ 435]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[  36/ 435]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  37/ 435]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  38/ 435]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[  39/ 435]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[  40/ 435]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.107 0.237 0.107 0.087 0.065 0.046 0.030 0.019 0.027 \n",
            "[  41/ 435]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[  42/ 435]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[  43/ 435]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[  44/ 435]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[  45/ 435]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  46/ 435]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  47/ 435]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.232 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[  48/ 435]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.109 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[  49/ 435]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.107 0.237 0.107 0.087 0.065 0.046 0.031 0.019 0.027 \n",
            "[  50/ 435]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  51/ 435]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.228 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[  52/ 435]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[  53/ 435]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[  54/ 435]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  55/ 435]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  56/ 435]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[  57/ 435]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.088 0.109 0.235 0.108 0.088 0.066 0.045 0.030 0.019 0.027 \n",
            "[  58/ 435]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.108 0.238 0.108 0.087 0.065 0.046 0.030 0.019 0.027 \n",
            "[  59/ 435]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[  60/ 435]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[  61/ 435]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.232 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[  62/ 435]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[  63/ 435]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  64/ 435]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  65/ 435]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[  66/ 435]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[  67/ 435]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.064 0.086 0.107 0.243 0.107 0.086 0.064 0.045 0.030 0.019 0.027 \n",
            "[  68/ 435]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[  69/ 435]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.067 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[  70/ 435]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[  71/ 435]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[  72/ 435]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  73/ 435]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  74/ 435]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[  75/ 435]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.088 0.109 0.235 0.109 0.088 0.066 0.046 0.030 0.019 0.026 \n",
            "[  76/ 435]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.107 0.237 0.107 0.087 0.065 0.046 0.030 0.019 0.027 \n",
            "[  77/ 435]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
            "[  78/ 435]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[  79/ 435]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[  80/ 435]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[  81/ 435]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  82/ 435]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  83/ 435]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[  84/ 435]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[  85/ 435]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.064 0.087 0.109 0.241 0.108 0.087 0.064 0.045 0.030 0.019 0.027 \n",
            "[  86/ 435]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  87/ 435]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[  88/ 435]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[  89/ 435]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[  90/ 435]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  91/ 435]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  92/ 435]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[  93/ 435]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.109 0.235 0.109 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[  94/ 435]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.087 0.108 0.239 0.108 0.087 0.065 0.045 0.030 0.019 0.027 \n",
            "[  95/ 435]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[  96/ 435]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[  97/ 435]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[  98/ 435]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[  99/ 435]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 100/ 435]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 101/ 435]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 102/ 435]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 103/ 435]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.087 0.108 0.239 0.108 0.087 0.065 0.045 0.030 0.019 0.027 \n",
            "[ 104/ 435]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 105/ 435]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 106/ 435]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 107/ 435]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.088 0.109 0.236 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 108/ 435]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 109/ 435]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 110/ 435]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 111/ 435]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.109 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 112/ 435]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.018 0.029 0.044 0.064 0.087 0.109 0.245 0.109 0.087 0.064 0.044 0.029 0.018 0.027 \n",
            "[ 113/ 435]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 114/ 435]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 115/ 435]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 116/ 435]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 117/ 435]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 118/ 435]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 119/ 435]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 120/ 435]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 121/ 435]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.087 0.108 0.239 0.108 0.087 0.065 0.045 0.030 0.019 0.027 \n",
            "[ 122/ 435]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 123/ 435]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 124/ 435]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 125/ 435]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.088 0.109 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 126/ 435]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 127/ 435]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 128/ 435]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 129/ 435]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.088 0.108 0.236 0.109 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 130/ 435]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.029 0.045 0.064 0.086 0.109 0.243 0.108 0.087 0.064 0.045 0.029 0.019 0.027 \n",
            "[ 131/ 435]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
            "[ 132/ 435]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 133/ 435]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 134/ 435]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 135/ 435]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 136/ 435]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 137/ 435]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 138/ 435]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.087 0.108 0.238 0.108 0.087 0.065 0.045 0.030 0.019 0.027 \n",
            "[ 139/ 435]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.029 0.045 0.064 0.087 0.109 0.243 0.108 0.087 0.064 0.045 0.029 0.018 0.027 \n",
            "[ 140/ 435]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.031 0.020 0.027 \n",
            "[ 141/ 435]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 142/ 435]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 143/ 435]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 144/ 435]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 145/ 435]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 146/ 435]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 147/ 435]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.235 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 148/ 435]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.029 0.044 0.064 0.087 0.109 0.243 0.108 0.087 0.064 0.045 0.029 0.019 0.027 \n",
            "[ 149/ 435]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 150/ 435]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 151/ 435]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 152/ 435]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 153/ 435]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 154/ 435]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 155/ 435]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.108 0.232 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 156/ 435]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.019 0.030 0.046 0.065 0.088 0.109 0.236 0.109 0.088 0.065 0.045 0.030 0.019 0.027 \n",
            "[ 157/ 435]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.018 0.029 0.045 0.064 0.087 0.109 0.243 0.109 0.087 0.064 0.044 0.029 0.018 0.026 \n",
            "[ 158/ 435]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
            "[ 159/ 435]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 160/ 435]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 161/ 435]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 162/ 435]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 163/ 435]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 164/ 435]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.231 0.108 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 165/ 435]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 166/ 435]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.018 0.029 0.044 0.064 0.087 0.109 0.245 0.109 0.087 0.064 0.044 0.029 0.018 0.026 \n",
            "[ 167/ 435]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 168/ 435]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 169/ 435]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 170/ 435]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 171/ 435]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 172/ 435]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 173/ 435]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 174/ 435]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 175/ 435]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.064 0.087 0.108 0.242 0.108 0.087 0.065 0.045 0.030 0.019 0.027 \n",
            "[ 176/ 435]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 177/ 435]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 178/ 435]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
            "[ 179/ 435]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 180/ 435]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 181/ 435]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 182/ 435]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 183/ 435]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.019 0.030 0.045 0.065 0.087 0.109 0.236 0.109 0.088 0.065 0.045 0.030 0.019 0.027 \n",
            "[ 184/ 435]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.107 0.238 0.107 0.087 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 185/ 435]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 186/ 435]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 187/ 435]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 188/ 435]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 189/ 435]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 190/ 435]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 191/ 435]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 192/ 435]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 193/ 435]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.087 0.108 0.239 0.108 0.087 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 194/ 435]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 195/ 435]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 196/ 435]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 197/ 435]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 198/ 435]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 199/ 435]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 200/ 435]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 201/ 435]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.235 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 202/ 435]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.031 0.046 0.066 0.087 0.107 0.233 0.107 0.087 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 203/ 435]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 204/ 435]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 205/ 435]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 206/ 435]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
            "[ 207/ 435]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 208/ 435]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 209/ 435]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 210/ 435]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.088 0.109 0.236 0.109 0.088 0.065 0.045 0.029 0.018 0.026 \n",
            "[ 211/ 435]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.087 0.107 0.235 0.107 0.087 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 212/ 435]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 213/ 435]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 214/ 435]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 215/ 435]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
            "[ 216/ 435]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 217/ 435]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 218/ 435]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 219/ 435]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.065 0.088 0.110 0.238 0.109 0.088 0.065 0.045 0.029 0.018 0.026 \n",
            "[ 220/ 435]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.107 0.237 0.107 0.087 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 221/ 435]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
            "[ 222/ 435]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 223/ 435]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.108 0.232 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 224/ 435]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 225/ 435]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 226/ 435]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 227/ 435]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 228/ 435]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 229/ 435]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.108 0.239 0.107 0.086 0.065 0.045 0.030 0.019 0.027 \n",
            "[ 230/ 435]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 231/ 435]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 232/ 435]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 233/ 435]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 234/ 435]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 235/ 435]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 236/ 435]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 237/ 435]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.066 0.088 0.109 0.235 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 238/ 435]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.087 0.108 0.239 0.108 0.087 0.065 0.045 0.030 0.019 0.027 \n",
            "[ 239/ 435]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 240/ 435]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 241/ 435]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 242/ 435]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 243/ 435]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 244/ 435]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 245/ 435]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 246/ 435]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 247/ 435]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.108 0.239 0.108 0.087 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 248/ 435]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 249/ 435]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 250/ 435]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 251/ 435]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 252/ 435]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 253/ 435]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 254/ 435]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.236 0.108 0.087 0.065 0.045 0.030 0.019 0.027 \n",
            "[ 255/ 435]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 256/ 435]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.018 0.029 0.044 0.064 0.086 0.109 0.245 0.109 0.086 0.064 0.044 0.029 0.018 0.027 \n",
            "[ 257/ 435]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 258/ 435]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 259/ 435]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 260/ 435]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 261/ 435]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 262/ 435]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 263/ 435]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 264/ 435]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 265/ 435]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.087 0.108 0.240 0.108 0.087 0.065 0.045 0.030 0.019 0.027 \n",
            "[ 266/ 435]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 267/ 435]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 268/ 435]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 269/ 435]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 270/ 435]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 271/ 435]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 272/ 435]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 273/ 435]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.235 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 274/ 435]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.064 0.086 0.108 0.243 0.108 0.087 0.064 0.045 0.030 0.019 0.027 \n",
            "[ 275/ 435]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
            "[ 276/ 435]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 277/ 435]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 278/ 435]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 279/ 435]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 280/ 435]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 281/ 435]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 282/ 435]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.087 0.109 0.237 0.109 0.087 0.065 0.045 0.030 0.019 0.027 \n",
            "[ 283/ 435]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.018 0.029 0.045 0.064 0.087 0.109 0.244 0.109 0.087 0.064 0.045 0.029 0.018 0.026 \n",
            "[ 284/ 435]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 285/ 435]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 286/ 435]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 287/ 435]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 288/ 435]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 289/ 435]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 290/ 435]                 blk.32.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 291/ 435]                 blk.32.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 292/ 435]                 blk.32.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.064 0.087 0.109 0.243 0.109 0.086 0.064 0.045 0.029 0.018 0.027 \n",
            "[ 293/ 435]            blk.32.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 294/ 435]               blk.32.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 295/ 435]                 blk.32.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 296/ 435]               blk.32.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 297/ 435]              blk.32.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 298/ 435]               blk.32.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 299/ 435]                 blk.33.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.108 0.232 0.107 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 300/ 435]                 blk.33.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.088 0.108 0.235 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 301/ 435]                 blk.33.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.018 0.029 0.044 0.064 0.087 0.109 0.243 0.109 0.087 0.064 0.044 0.029 0.018 0.026 \n",
            "[ 302/ 435]            blk.33.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
            "[ 303/ 435]               blk.33.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 304/ 435]                 blk.33.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 305/ 435]               blk.33.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 306/ 435]              blk.33.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 307/ 435]               blk.33.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 308/ 435]                 blk.34.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 309/ 435]                 blk.34.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 310/ 435]                 blk.34.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.063 0.087 0.109 0.246 0.109 0.086 0.064 0.044 0.029 0.018 0.026 \n",
            "[ 311/ 435]            blk.34.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
            "[ 312/ 435]               blk.34.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 313/ 435]                 blk.34.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 314/ 435]               blk.34.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 315/ 435]              blk.34.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 316/ 435]               blk.34.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 317/ 435]                 blk.35.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 318/ 435]                 blk.35.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 319/ 435]                 blk.35.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.064 0.087 0.108 0.242 0.108 0.087 0.065 0.045 0.030 0.019 0.027 \n",
            "[ 320/ 435]            blk.35.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 321/ 435]               blk.35.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 322/ 435]                 blk.35.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 323/ 435]               blk.35.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 324/ 435]              blk.35.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 325/ 435]               blk.35.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 326/ 435]                 blk.36.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 327/ 435]                 blk.36.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.234 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 328/ 435]                 blk.36.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.087 0.108 0.240 0.108 0.087 0.065 0.045 0.030 0.019 0.027 \n",
            "[ 329/ 435]            blk.36.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 330/ 435]               blk.36.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 331/ 435]                 blk.36.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
            "[ 332/ 435]               blk.36.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 333/ 435]              blk.36.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 334/ 435]               blk.36.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 335/ 435]                 blk.37.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 336/ 435]                 blk.37.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 337/ 435]                 blk.37.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.064 0.087 0.108 0.241 0.108 0.087 0.064 0.045 0.030 0.019 0.027 \n",
            "[ 338/ 435]            blk.37.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 339/ 435]               blk.37.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 340/ 435]                 blk.37.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.228 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 341/ 435]               blk.37.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 342/ 435]              blk.37.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 343/ 435]               blk.37.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 344/ 435]                 blk.38.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.108 0.231 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 345/ 435]                 blk.38.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 346/ 435]                 blk.38.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.087 0.107 0.235 0.107 0.087 0.065 0.046 0.031 0.019 0.027 \n",
            "[ 347/ 435]            blk.38.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 348/ 435]               blk.38.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 349/ 435]                 blk.38.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 350/ 435]               blk.38.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 351/ 435]              blk.38.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 352/ 435]               blk.38.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 353/ 435]                 blk.39.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.232 0.108 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 354/ 435]                 blk.39.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.236 0.108 0.088 0.065 0.046 0.030 0.019 0.026 \n",
            "[ 355/ 435]                 blk.39.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.107 0.237 0.107 0.087 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 356/ 435]            blk.39.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 357/ 435]               blk.39.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.031 0.020 0.027 \n",
            "[ 358/ 435]                 blk.39.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 359/ 435]               blk.39.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 360/ 435]              blk.39.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 361/ 435]               blk.39.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 362/ 435]                 blk.40.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 363/ 435]                 blk.40.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 364/ 435]                 blk.40.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.065 0.087 0.107 0.236 0.107 0.087 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 365/ 435]            blk.40.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 366/ 435]               blk.40.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 367/ 435]                 blk.40.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 368/ 435]               blk.40.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 369/ 435]              blk.40.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 370/ 435]               blk.40.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 371/ 435]                 blk.41.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 372/ 435]                 blk.41.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.236 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 373/ 435]                 blk.41.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.108 0.237 0.107 0.087 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 374/ 435]            blk.41.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 375/ 435]               blk.41.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 376/ 435]                 blk.41.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 377/ 435]               blk.41.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 378/ 435]              blk.41.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 379/ 435]               blk.41.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 380/ 435]                 blk.42.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 381/ 435]                 blk.42.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.109 0.236 0.108 0.087 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 382/ 435]                 blk.42.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.108 0.237 0.108 0.087 0.065 0.046 0.030 0.019 0.027 \n",
            "[ 383/ 435]            blk.42.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 384/ 435]               blk.42.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 385/ 435]                 blk.42.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.228 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 386/ 435]               blk.42.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 387/ 435]              blk.42.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 388/ 435]               blk.42.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 389/ 435]                 blk.43.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 390/ 435]                 blk.43.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.087 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 391/ 435]                 blk.43.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.087 0.107 0.233 0.107 0.087 0.066 0.046 0.031 0.019 0.027 \n",
            "[ 392/ 435]            blk.43.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 393/ 435]               blk.43.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 394/ 435]                 blk.43.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.228 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 395/ 435]               blk.43.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 396/ 435]              blk.43.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 397/ 435]               blk.43.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 398/ 435]                 blk.44.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 399/ 435]                 blk.44.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.064 0.087 0.110 0.243 0.110 0.087 0.064 0.044 0.029 0.018 0.026 \n",
            "[ 400/ 435]                 blk.44.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.087 0.108 0.241 0.108 0.087 0.064 0.045 0.029 0.019 0.027 \n",
            "[ 401/ 435]            blk.44.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 402/ 435]               blk.44.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 403/ 435]                 blk.44.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.228 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 404/ 435]               blk.44.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 405/ 435]              blk.44.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 406/ 435]               blk.44.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 407/ 435]                 blk.45.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.108 0.236 0.108 0.088 0.065 0.045 0.030 0.019 0.027 \n",
            "[ 408/ 435]                 blk.45.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.064 0.087 0.110 0.242 0.110 0.087 0.064 0.044 0.029 0.018 0.026 \n",
            "[ 409/ 435]                 blk.45.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.064 0.086 0.109 0.248 0.108 0.086 0.064 0.044 0.029 0.018 0.027 \n",
            "[ 410/ 435]            blk.45.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 411/ 435]               blk.45.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 412/ 435]                 blk.45.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 413/ 435]               blk.45.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 414/ 435]              blk.45.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 415/ 435]               blk.45.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 416/ 435]                 blk.46.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 417/ 435]                 blk.46.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.064 0.087 0.110 0.241 0.109 0.087 0.064 0.045 0.029 0.018 0.026 \n",
            "[ 418/ 435]                 blk.46.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.063 0.086 0.110 0.250 0.110 0.086 0.063 0.044 0.028 0.018 0.026 \n",
            "[ 419/ 435]            blk.46.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
            "[ 420/ 435]               blk.46.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 421/ 435]                 blk.46.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 422/ 435]               blk.46.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.088 0.109 0.235 0.109 0.088 0.065 0.045 0.030 0.019 0.027 \n",
            "[ 423/ 435]              blk.46.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 424/ 435]               blk.46.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 425/ 435]                 blk.47.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
            "[ 426/ 435]                 blk.47.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.064 0.087 0.110 0.242 0.110 0.087 0.064 0.044 0.029 0.018 0.026 \n",
            "[ 427/ 435]                 blk.47.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.028 0.043 0.063 0.086 0.111 0.250 0.111 0.087 0.063 0.043 0.028 0.018 0.026 \n",
            "[ 428/ 435]            blk.47.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 429/ 435]               blk.47.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 430/ 435]                 blk.47.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 431/ 435]               blk.47.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =   112.00 MiB ->    59.50 MiB | hist: 0.000 0.026 0.017 0.028 0.043 0.063 0.087 0.112 0.249 0.112 0.087 0.063 0.043 0.028 0.017 0.026 \n",
            "[ 432/ 435]              blk.47.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 433/ 435]               blk.47.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 434/ 435]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 435/ 435]                        output.weight - [ 4096, 40960,     1,     1], type =    f16, quantizing to q8_0 .. size =   320.00 MiB ->   170.00 MiB | hist: 0.000 0.028 0.022 0.035 0.053 0.072 0.089 0.101 0.198 0.102 0.089 0.072 0.053 0.035 0.022 0.028 \n",
            "llama_model_quantize_internal: model size  = 20609.52 MB\n",
            "llama_model_quantize_internal: quant size  = 10949.52 MB\n",
            "llama_model_quantize_internal: hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "\n",
            "main: quantize time = 179759.41 ms\n",
            "main:    total time = 179759.41 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = \"/content/drive/MyDrive/DACON/Hansol_QA/model\"\n",
        "model_name = \"yanolja-axolotl-aug-noise-sysprompt\"\n",
        "quant_opt = \"q8_0\"\n",
        "\n",
        "!llama.cpp/main -m {model_dir}/{model_name}/ggml-model-{quant_opt}.gguf -n 1000 --prompt \"제시된 각 질문에 대해 상세하고 명확한 설명을 제공하시오. 방청 페인트의 종류에는 어떤 것들이 있는지 알고 계신가요? 또한, 원목사이딩을 사용하는 것에 어떤 단점이 있을까요?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RP3ANS07GIK",
        "outputId": "c022adc1-0c06-4e14-b0c4-9ea2f8437ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2360 (6cdabe65)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1709884504\n",
            "llama_model_loader: loaded meta data with 23 key-value pairs and 435 tensors from /content/drive/MyDrive/DACON/Hansol_QA/model/yanolja-axolotl-aug-noise-sysprompt/ggml-model-q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 48\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,40960]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,40960]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,40960]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   97 tensors\n",
            "llama_model_loader: - type q8_0:  338 tensors\n",
            "llm_load_vocab: mismatch in special tokens definition ( 261/40960 vs 260/40960 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 40960\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 48\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 34B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 10.80 B\n",
            "llm_load_print_meta: model size       = 10.69 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 2 '</s>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.17 MiB\n",
            "llm_load_tensors:        CPU buffer size = 10949.52 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
            "llama_new_context_with_model:        CPU input buffer size   =    10.01 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    88.00 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 1\n",
            "\n",
            "system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 1000, n_keep = 1\n",
            "\n",
            "\n",
            " 제시된 각 질문에 대해 상세하고 명확한 설명을 제공하시오. 방청 페인트의 종류에는 어떤 것들이 있는지 알고 계신가요? 또한, 원목사이딩을 사용하는 것에 어떤 단점이 있을까요? 방청 페인트의 종류로는 광명단페인트, 방청산화철페인트, 알미늄페인트, 역청질페인트, 워시프라이머, 크롬산아연페인트 등이 있습니다. 이 외에도 다양한 다른 종류의 방청 페인트가 있으며, 각각의 특성과 용도에 따라 선택하여 사용할 수 있습니다. \n",
            " 원목사이딩의 단점은 주로 습기에 노출될 때 발생할 수 있는 변형과 관련된 문제입니다. 특히 수분 흡수 및 건조 과정에서 변형이 발생할 수 있으며, 이로 인해 형태가 변하거나 부식 현상이 나타날 가능성이 있습니다. 따라서 적절한 방법으로 보수 및 관리가 이루어지지 않을 경우 이러한 문제가 발생할 가능성이 있습니다. [end of text]\n",
            "\n",
            "llama_print_timings:        load time =   54322.31 ms\n",
            "llama_print_timings:      sample time =     129.08 ms /   150 runs   (    0.86 ms per token,  1162.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =   53890.24 ms /    44 tokens ( 1224.78 ms per token,     0.82 tokens per second)\n",
            "llama_print_timings:        eval time =   79009.48 ms /   149 runs   (  530.26 ms per token,     1.89 tokens per second)\n",
            "llama_print_timings:       total time =  133120.29 ms /   193 tokens\n",
            "Log end\n"
          ]
        }
      ]
    }
  ]
}